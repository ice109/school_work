#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsart
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Expectation Maximization: From the Horse's Mouth
\end_layout

\begin_layout Standard
Expectation
\begin_inset Foot
status open

\begin_layout Plain Layout
These notes are basically a transcription of a 2-3 lecture notes (taken
 dutifully by Chris Gianelli) from lectures given by Dr.
 Kshitij Khare at UF in Spring 15.
 He's not a horse - it's just an expression.
\end_layout

\end_inset

maximization (EM) is a way to iteratively approximate the maximum likelihood
 estimators (MLEs) for a parametric family when solving the MLE equations
 analytically is intractable.
 
\end_layout

\begin_layout Standard
Recall that finding the MLEs for a parametric family 
\begin_inset Formula 
\[
\mathbf{Y}\sim f_{\mathbf{Y}}\left(\cdot;\boldsymbol{\theta}\right)\,\boldsymbol{\theta}\in\Theta
\]

\end_inset

is tantamount to maximizing the likelihood function
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta};\mathbf{y}\right)=f_{\mathbf{Y}}\left(\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

as a function of 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 The maximum likelihood estimators are estimators of/for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

, usually denoted 
\begin_inset Formula $\hat{\boldsymbol{\theta}}$
\end_inset

.
 The actual maximization is effected by finding critical points of 
\begin_inset Formula $L$
\end_inset

 wrt 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 and testing concavity, i.e.
 solving 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta_{i}}L\left(\boldsymbol{\theta};\mathbf{y}\right)=0
\]

\end_inset

for all 
\begin_inset Formula $i$
\end_inset

 and checking the negative definiteness of Hessian of 
\begin_inset Formula $L$
\end_inset

.
 In general 
\begin_inset Formula $\partial_{\theta_{i}}L\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 might be highly non-linear in 
\begin_inset Formula $\theta_{i}$
\end_inset

 and hence finding each might be very difficult.
 What to do?
\end_layout

\begin_layout Section
Expectation Maximization Beta
\end_layout

\begin_layout Standard
Suppose
\begin_inset Foot
status open

\begin_layout Plain Layout
Suppose!
\end_layout

\end_inset

 there exists another random, unobserved, vector 
\begin_inset Formula $\mathbf{X}$
\end_inset

 such that were 
\begin_inset Formula $\mathbf{X}$
\end_inset

 in fact observed, maximizing the joint likelihood for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)=f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

is easier than the original problem.
 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is called a latent or hidden variable.
 Now a priori there's absolutely no reason to assume that having more informatio
n in hand and complicating the model should make things easier.
 On the contrary, for example, a good physical model makes simplifying assumptio
ns and thereby becomes tractable.
 But indeed for some very useful models such as Hidden Markov Models and
 Gaussian Mixture Models this ansatz does simplify computing the MLEs.
\end_layout

\begin_layout Standard
All is going swimmingly except 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is unobserved - the only observed data are 
\begin_inset Formula $\mathbf{Y}$
\end_inset

.
 What to do? Estimate 
\begin_inset Formula $\mathbf{X}$
\end_inset

 of course.
 How? Using the best estimator of 
\begin_inset Formula $\mathbf{x}$
\end_inset

, based on observed
\series bold
 
\begin_inset Formula $\mathbf{y}$
\end_inset


\series default
, that minimizes the risk for the quadratic loss function
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $l\left(\mathbf{X},\hat{\mathbf{X}}\right)=\left(\mathbf{X}-\hat{\mathbf{X}}\right)^{2}$
\end_inset


\end_layout

\end_inset

, i.e.
 minimizes the mean square error, i.e.
 , 
\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right]$
\end_inset

.
 Note two things.
 Firstly it's important that you can actually compute 
\emph on
this
\emph default
 in closed form, otherwise you've traded one intractable problem for another.
 Secondly, since 
\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right]$
\end_inset

 is implicitly a function of 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

, which is unknown to begin with
\begin_inset Foot
status open

\begin_layout Plain Layout
Don't forget the point of all this is actually to estimate 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
\end_layout

\end_inset

 you need some initial guess for it too, otherwise you can't compute the
 expectation.
 Hence the expectation computed is actually 
\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}'\right]$
\end_inset

 where 
\begin_inset Formula $\boldsymbol{\theta}'$
\end_inset

 is the current guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 Then once you have this estimate for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 just maximize 
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta};E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}'\right],\mathbf{y}\right)=f_{\mathbf{X},\mathbf{Y}}\left(E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}'\right],\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

The procedure alternates between estimating 
\series bold
x 
\series default
using 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}'\right]$
\end_inset

 and maximizing 
\begin_inset Formula $L\left(\boldsymbol{\theta};E\left[\mathbf{x}|\mathbf{y};\boldsymbol{\theta}'\right],\mathbf{y}\right)$
\end_inset

 .
 Compute expectation, then perform maxization, compute expectation, then
 perform maximization, compute expectation,...
 hence 
\family default
\series bold
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
Expectation Maximization
\series default
 algorithm.
 Just to be clear 
\end_layout

\begin_layout Definition

\series bold
Expectation Algorithm Beta.
 
\series default
Given some observed data 
\begin_inset Formula $\mathbf{y}$
\end_inset

, in order to perform the intractable maximization of 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

, posit the existence of some latent 
\begin_inset Formula $\mathbf{x}$
\end_inset

 such that 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)$
\end_inset

 is easier.
 Set 
\begin_inset Formula $\boldsymbol{\theta}^{\left(0\right)}$
\end_inset

 to be some initial guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 then
\series bold
 
\end_layout

\begin_deeper
\begin_layout Enumerate
E-step: Set 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{x}^{\left(r\right)}=E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset


\end_layout

\begin_layout Enumerate
M-step: Set 
\begin_inset Formula $\boldsymbol{\theta}^{\left(r+1\right)}=\arg\max_{\boldsymbol{\theta}}L\left(\boldsymbol{\theta};\mathbf{x}^{\left(r\right)},\mathbf{y}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Go to step 1 unless 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\left|\boldsymbol{\theta}^{\left(r+1\right)}-\boldsymbol{\theta}^{\left(r\right)}\right|<\varepsilon$
\end_inset

 for some 
\begin_inset Formula $\varepsilon$
\end_inset

 of your choosing.
\end_layout

\end_deeper
\begin_layout Standard
The equations in step 1 and 2 are called update equations because specify
 how to update the current estimate for 
\begin_inset Formula $\mathbf{x}^{\left(i\right)}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{\theta}^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Example
Here's an example: let 
\begin_inset Formula $Y_{i}$
\end_inset

 be iid such that 
\begin_inset Formula 
\[
f_{Y_{i}}\left(y_{i};\theta\right)=\theta g_{1}\left(y_{i}\right)+\left(1-\theta\right)g_{0}\left(y_{i}\right)
\]

\end_inset

where 
\begin_inset Formula $\theta\in\left(0,1\right)$
\end_inset

 and 
\begin_inset Formula $g_{0}$
\end_inset

 and 
\begin_inset Formula $g_{1}$
\end_inset

 are Gaussians with known means 
\begin_inset Formula $\left(\mu_{0},\mu_{1}\right)$
\end_inset

and variances 
\begin_inset Formula $\left(\sigma_{0}^{2},\sigma_{1}^{2}\right)$
\end_inset

.
 We want the MLE for 
\begin_inset Formula $\theta$
\end_inset

.
 Note that 
\begin_inset Formula 
\[
L\left(\theta;\mathbf{y}\right)=\prod_{i=1}^{n}\left(\theta g_{1}\left(y_{i}\right)+\left(1-\theta\right)g_{0}\left(y_{i}\right)\right)
\]

\end_inset

Quite messy
\begin_inset Foot
status open

\begin_layout Plain Layout
Even if you think you're clever and try to maximize log-likelihood you're
 still going to have a rough go at it.
\end_layout

\end_inset

.
 What to do? EM to the rescue! Suppose the 
\begin_inset Formula $Y_{i}$
\end_inset

 are drawn by a process where a 
\begin_inset Formula $\theta$
\end_inset

-biased coin is flipped and either 
\begin_inset Formula $g_{0}$
\end_inset

 or 
\begin_inset Formula $g_{1}$
\end_inset

 generates the 
\begin_inset Formula $y_{i}$
\end_inset

 depending on whether the coin lands heads up or down.
 The latent variable here then is which Gaussian generated 
\begin_inset Formula $y_{i}$
\end_inset

.
 Hence let 
\begin_inset Formula $X_{i}$
\end_inset

 be Bernoulli random variables where 
\begin_inset Formula 
\begin{eqnarray*}
P\left(X_{i}=x_{i}\right) & = & \begin{cases}
\theta & \text{ if }x_{i}=1\\
1-\theta & \text{ if }x_{i}=0
\end{cases}\\
 & = & \theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}
\end{eqnarray*}

\end_inset

and 
\begin_inset Formula $f_{Y_{i}|X_{i}}\left(y_{i}|x_{i}=1;\theta\right)=g_{1}\left(y_{i}\right)$
\end_inset

 and 
\begin_inset Formula $f_{Y_{i}|X_{i}}\left(y_{i}|x_{i}=0;\theta\right)=g_{0}\left(y_{i}\right)$
\end_inset

.
 Then 
\begin_inset Formula 
\[
f_{X_{i},Y_{i}}\left(x_{i},y_{i};\theta\right)=f_{Y_{i}|X_{i}}\left(y_{i}|x_{i};\theta\right)P\left(X_{i}=x_{i}\right)
\]

\end_inset

and so
\begin_inset Formula 
\begin{eqnarray*}
L\left(\theta;\mathbf{x},\mathbf{y}\right) & = & \prod_{i=1}^{n}f_{Y_{i}|X_{i}}\left(y_{i}|x_{i};\theta\right)P\left(X_{i}=x_{i}\right)\\
 & = & \prod_{i=1}^{n}\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}\\
 & = & \theta^{\sum_{i}x_{i}}\left(1-\theta\right)^{n-\sum_{i}x_{i}}\prod_{i=1}^{n}\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}
\end{eqnarray*}

\end_inset

Note the trick in writing 
\begin_inset Formula $f_{Y_{i}|X_{i}}\left(y_{i}|x_{i};\theta\right)=\left(g_{1}\right)^{x_{i}}\left(g_{0}\right)^{1-x_{i}}$
\end_inset

 - it comes up a lot for a class of models called Mixture models.
 For the E-step, to compute 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\theta'\right]$
\end_inset

 we use iid-ness and compute 
\begin_inset Formula $E\left[X_{i}|y_{i};\theta'\right]$
\end_inset

 instead, which will hold for each 
\begin_inset Formula $i$
\end_inset

.
 To compute the conditional expectation 
\begin_inset Formula $E\left[X_{i}|y_{i};\theta\right]$
\end_inset

 we need the conditional distribution 
\begin_inset Formula $f_{X_{i}|Y_{i}}\left(x_{i}|y_{i}\right)$
\end_inset

.
 By Bayes' Theorem
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\begin_inset Formula 
\begin{eqnarray*}
f_{X_{i}|Y_{i}}\left(x_{i}|y_{i};\theta\right) & = & \frac{f_{X_{i},Y_{i}}\left(x_{i},y_{i};\theta\right)}{f_{Y_{i}}\left(y_{i};\theta\right)}\\
 & = & \frac{f_{X_{i},Y_{i}}\left(x_{i},y_{i};\theta\right)}{\sum_{x_{i}}f_{X_{i},Y_{i}}\left(x_{i},y_{i};\theta\right)}\\
 & = & \frac{\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}}{\sum_{x_{i}}\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}}\\
 & = & \frac{\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}}{g_{1}\left(y_{i}\right)\theta+g_{0}\left(y_{i}\right)\left(1-\theta\right)}\\
 & = & \left(\frac{g_{1}\left(y_{i}\right)\theta}{g_{1}\left(y_{i}\right)\theta+g_{0}\left(y_{i}\right)\left(1-\theta\right)}\right)^{x_{i}}\left(\frac{g_{0}\left(y_{i}\right)\left(1-\theta\right)}{g_{1}\left(y_{i}\right)\theta+g_{0}\left(y_{i}\right)\left(1-\theta\right)}\right)^{1-x_{i}}
\end{eqnarray*}

\end_inset

So 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $X_{i}|Y_{i}$
\end_inset

 is still Bernoulli just renormalized.
 Hence 
\begin_inset Formula 
\[
E\left[X_{i}|y_{i};\theta\right]=\frac{g_{1}\left(y_{i}\right)\theta}{g_{1}\left(y_{i}\right)\theta+g_{0}\left(y_{i}\right)\left(1-\theta\right)}
\]

\end_inset

For the M-step since 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula 
\begin{eqnarray*}
L\left(\theta;\mathbf{x},\mathbf{y}\right) & = & \left[\theta^{\sum_{i}x_{i}}\left(1-\theta\right)^{n-\sum_{i}x_{i}}\right]\left[\prod_{i=1}^{n}\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\right]
\end{eqnarray*}

\end_inset

and the second term is independent of 
\begin_inset Formula $\theta$
\end_inset

 we can just maximize the first term.
 But this is just the the joint distribution for 
\begin_inset Formula $n$
\end_inset

 iid Bernoulli random variables and the MLE 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{\theta}=\frac{\sum_{i}x_{i}}{n}
\]

\end_inset

Therefore, finally, the update equations are 
\begin_inset Formula 
\begin{eqnarray*}
x_{i}^{\left(r\right)} & = & \frac{g_{1}\left(y_{i}\right)\theta^{\left(r\right)}}{g_{1}\left(y_{i}\right)\theta^{\left(r\right)}+g_{0}\left(y_{i}\right)\left(1-\theta^{\left(r\right)}\right)}\\
\theta^{\left(r+1\right)} & = & \frac{1}{n}\sum_{i=1}^{n}x_{i}^{\left(r\right)}
\end{eqnarray*}

\end_inset


\begin_inset Formula $\blacksquare$
\end_inset


\end_layout

\begin_layout Standard
So why did dub it 
\series bold
Expectation Maximization Beta
\series default
 instead of just 
\series bold
Expectation Maximization
\series default
? Because this isn't the standard EM algorithm.
 But why alter this algorithm at all? What's wrong with it as is? Well there
 are no convergence guarantees.
 Indeed the iterates in Example 1 don't converge to the right answer: figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "embetaiterates"

\end_inset

 shows the first 100 iterates for 
\begin_inset Formula $g_{1}\sim\text{n}\left(1,2\right),g_{0}\sim\text{n}\left(3,4\right),\theta=2/3$
\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename embetaiterates.jpeg
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "embetaiterates"

\end_inset

EM Beta
\end_layout

\end_inset


\end_layout

\end_inset

So they converge but to 
\begin_inset Formula $1\neq2/3$
\end_inset

.
 Why did I present this algorithm first? Standard EM is slightly more complicate
d and much less intuitive but legend has it that it was in fact conceived
 in this way first and then manipulated to get converge.
 
\end_layout

\begin_layout Section
Expectation Maximization for Real
\end_layout

\begin_layout Standard
Recall that the whole point of this procedure is to actually maximize the
 likelihood for 
\begin_inset Formula $\mathbf{Y}$
\end_inset

.
 This is equivalent to maximizing the log-likelihood for 
\begin_inset Formula $\mathbf{Y}$
\end_inset


\begin_inset Formula 
\[
l\left(\boldsymbol{\theta};\mathbf{y}\right)=\log L\left(\boldsymbol{\theta};\mathbf{y}\right)=\log f_{\mathbf{Y}}\left(\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

The ansatz here is the same: suppose there exists another random, unobserved,
 vector 
\begin_inset Formula $\mathbf{X}$
\end_inset

 such that were 
\begin_inset Formula $\mathbf{X}$
\end_inset

 in fact observed, maximizing the joint log-likelihood for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 
\begin_inset Formula 
\[
l\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)=\log L\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)=\log f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

is easier than the original problem.
 Where the ease of maximizing 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)$
\end_inset

 over 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 isn't immediately apparent but first using Bayes' theorem we see that
\begin_inset Formula 
\begin{eqnarray}
l\left(\boldsymbol{\theta};\mathbf{y}\right) & = & \log f_{\mathbf{Y}}\left(\mathbf{y};\boldsymbol{\theta}\right)\\
 & = & \log\left(\frac{f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)}\right)\\
 & = & \log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)\right)-\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)\right)\label{eq:2.3}
\end{eqnarray}

\end_inset

It's now that it becomes obvious that if we can easily maximize the first
 term in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.3"

\end_inset

, with respect to 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 and the second term in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.3"

\end_inset

 doesn't spoil things somehow, then we'll maximize 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

.
 This seems like a workflow different from that of the beta algorithm (here
 we're explicitly maximizing a function of the likelihood for 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 by maximizing a lowerbound) but the analogy follows.
 Since 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is unobserved we face the same difficulty as in the beta algorithm: the
 terms in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.3"

\end_inset

 can't be computed and must be estimated.
 We use the same estimator as before 
\begin_inset Formula $E\left[\cdot|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset


\begin_inset Formula 
\begin{equation}
l\left(\boldsymbol{\theta};\mathbf{y}\right)=E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]-E\left[\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]\label{eq:2.4}
\end{equation}

\end_inset

Note some subtle things: 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathbf{x}\rightarrow\mathbf{X}$
\end_inset

 because each of the terms on the right-hand side of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.4"

\end_inset

 are estimates of the random variable 
\series bold
X 
\series default
as a function of observed data 
\series bold
y.
\end_layout

\begin_layout Itemize
The expectations are again computed with respect to the conditional distribution
 of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 given 
\series bold
Y
\series default
 for fixed (iterate) values of 
\begin_inset Formula $\boldsymbol{\theta}^{\left(r\right)}$
\end_inset

, i.e.
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)$
\end_inset

.

\family default
\series bold
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\end_layout

\begin_layout Itemize
The estimators, for which the expectations are computed, are functions of
 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
\end_layout

\begin_layout Itemize
Equality is maintained because since 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 is independent of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 
\begin_inset Formula 
\[
E\left[l\left(\boldsymbol{\theta};\mathbf{y}\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]=l\left(\boldsymbol{\theta};\mathbf{y}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The algorithm then is
\end_layout

\begin_layout Definition

\series bold
Expectation Algorithm.
 
\series default
Given some observed data 
\begin_inset Formula $\mathbf{y}$
\end_inset

, in order to perform the intractable maximization of 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

, posit the existence of some latent 
\begin_inset Formula $\mathbf{x}$
\end_inset

 such that 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)$
\end_inset

 is easier.
 Set 
\begin_inset Formula $\boldsymbol{\theta}^{\left(0\right)}$
\end_inset

 to be some initial guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 then
\series bold
 
\end_layout

\begin_deeper
\begin_layout Enumerate
E-step: Compute 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset


\end_layout

\begin_layout Enumerate
M-step: Set 
\begin_inset Formula $\boldsymbol{\theta}^{\left(r+1\right)}=\arg\max_{\boldsymbol{\theta}}E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset


\end_layout

\begin_layout Enumerate
Go to step 1 unless 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\left|\boldsymbol{\theta}^{\left(r+1\right)}-\boldsymbol{\theta}^{\left(r\right)}\right|<\varepsilon$
\end_inset

 for some 
\begin_inset Formula $\varepsilon$
\end_inset

 of your choosing.
\end_layout

\end_deeper
\begin_layout Standard
The only thing to remains is to prove that maximizing 
\begin_inset Formula $E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

 is sufficient to maximize 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

.
 I won't but I'll prove a thing on the way to that result, namely that 
\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)\geq l\left(\boldsymbol{\theta}^{\left(r-1\right)};\mathbf{y}\right)$
\end_inset

 and since 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 is bounded above (it's a density) the sequence of 
\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)$
\end_inset

 must converge.
 The last hurdle would be proving that convergence of 
\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)$
\end_inset

 implies the convergence of the iterates 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\boldsymbol{\theta}^{\left(r\right)}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 themselves.
 I leave this to the paper
\begin_inset Foot
status open

\begin_layout Plain Layout
http://web.stanford.edu/class/ee378b/papers/wu-em.pdf
\end_layout

\end_inset

.
\end_layout

\begin_layout Theorem
(Monotonic EM Sequence) The sequence 
\begin_inset Formula $\left\{ \boldsymbol{\theta}^{\left(r\right)}\right\} $
\end_inset

 satisfies 
\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r+1\right)};\mathbf{y}\right)\geq l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)$
\end_inset

.
\end_layout

\begin_layout Proof
Start with
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\[
l\left(\boldsymbol{\theta};\mathbf{y}\right)=\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)\right)-\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)\right)
\]

\end_inset

Take conditional expectation with respect to 
\begin_inset Formula $\mathbf{X}|\mathbf{Y};\boldsymbol{\theta}^{\left(r\right)}$
\end_inset

 of both sides
\begin_inset Formula 
\begin{eqnarray*}
\int l\left(\boldsymbol{\theta};\mathbf{y}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x} & = & \int\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 &  & -\int\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}
\end{eqnarray*}

\end_inset

Since 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 is independent of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
\int l\left(\boldsymbol{\theta};\mathbf{y}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x} & = & l\left(\boldsymbol{\theta};\mathbf{y}\right)\int f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 & = & l\left(\boldsymbol{\theta};\mathbf{y}\right)
\end{eqnarray*}

\end_inset

and so 
\begin_inset Formula 
\begin{eqnarray*}
l\left(\boldsymbol{\theta};\mathbf{y}\right) & = & \int\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 &  & -\int\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 & = & E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]-E\left[\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]
\end{eqnarray*}

\end_inset

Let 
\begin_inset Formula 
\[
Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)=E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]
\]

\end_inset

and 
\begin_inset Formula $K\left(\boldsymbol{\theta},\boldsymbol{\theta}^{\left(r\right)}\right)=E\left[\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

.
 Then
\begin_inset Formula 
\[
l\left(\boldsymbol{\theta};\mathbf{y}\right)=Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)-K\left(\boldsymbol{\theta},\boldsymbol{\theta}^{\left(r\right)}\right)
\]

\end_inset

If we can show that 
\begin_inset Formula 
\begin{eqnarray*}
Q\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right) & \leq & Q\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)\\
 & \text{and}\\
K\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right) & \geq & K\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)
\end{eqnarray*}

\end_inset

then it will follow that 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r+1\right)};\mathbf{y}\right)\geq l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)$
\end_inset

.
 Well by definition of 
\begin_inset Formula 
\[
\boldsymbol{\theta}^{\left(r+1\right)}=\arg\max_{\boldsymbol{\theta}}E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]=\arg\max_{\boldsymbol{\theta}}Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)
\]

\end_inset

so 
\begin_inset Formula 
\[
Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)\leq Q\left(\mathbf{\boldsymbol{\theta}},\arg\max_{\boldsymbol{\theta}}Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)\right)=Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r+1\right)}\right)
\]

\end_inset

To see that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $K\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right)\geq K\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)$
\end_inset

 inspect 
\begin_inset Formula 
\[
\int\log\left(\frac{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}
\]

\end_inset

On the one hand 
\begin_inset Formula 
\begin{eqnarray*}
\int\log\left(\frac{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x} & = & \int\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 &  & -\int\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 & = & K\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)-K\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right)
\end{eqnarray*}

\end_inset

on the other hand, by Jensen's
\begin_inset Foot
status open

\begin_layout Plain Layout
Pronounce Yen-sen you uncultured boor!
\end_layout

\end_inset

 inequality
\begin_inset Foot
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
For convex 
\begin_inset Formula $g\left(X\right)$
\end_inset

 it's the case that 
\begin_inset Formula $g\left(E\left(X\right)\right)\leq Eg\left(X\right)$
\end_inset

 and for concave 
\begin_inset Formula $g\left(X\right)$
\end_inset

 the inequality is reversed.
\end_layout

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\int\log\left(\frac{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x} & \leq & \log\left[\int\left(\frac{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\right]\\
 & = & \log\left[\int f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)d\mathbf{x}\right]\\
 & = & 0
\end{eqnarray*}

\end_inset

Hence 
\begin_inset Formula 
\[
K\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)-K\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right)\leq0
\]

\end_inset

which completes the proof.
\end_layout

\begin_layout Section
Applications
\end_layout

\begin_layout Subsection
Gaussian Mixture Models
\end_layout

\begin_layout Standard
EM works really well for mixture models, e.g.
 
\begin_inset Formula $\mathbf{Y}_{i}$
\end_inset

 is distributed iid such that 
\begin_inset Formula 
\[
f_{\mathbf{Y}_{1}}\left(\mathbf{y}_{1}\right)=\sum_{j=1}^{m}\alpha_{j}g_{j}\left(\mathbf{y}_{1};\boldsymbol{\theta}_{j}\right)
\]

\end_inset

where 
\begin_inset Formula $g_{j}$
\end_inset

 are (in general multivariate) densities with parameters 
\begin_inset Formula $\boldsymbol{\theta}_{j}$
\end_inset

 (in general distinct for distinct 
\begin_inset Formula $j$
\end_inset

) and 
\begin_inset Formula $\sum_{j}\alpha_{j}=1$
\end_inset

.
 If 
\begin_inset Formula $g_{j}\sim\text{n}\left(\mu_{j},\sigma_{j}^{2}\right)$
\end_inset

 then the mixture model is called a Gaussian mixture model (GMM).
 Naively if you wanted to find the MLEs you would maximize
\begin_inset Formula 
\[
L\left(\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)=\prod_{i=1}^{n}f_{\mathbf{Y}_{i}}\left(\mathbf{y}_{i};\boldsymbol{\theta}\right)=\prod_{i=1}^{n}\sum_{j=1}^{m}\alpha_{j}g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)
\]

\end_inset

Even taking the log won't help you because of the interior sum.
 EM to the rescue again! Let 
\begin_inset Formula $\mathbf{X}=\left(X_{1},\dots,X_{n}\right)$
\end_inset

 be the mixture components, i.e.
 each 
\begin_inset Formula $X_{i}$
\end_inset

 is a categorical random variable that indicates which component of the
 mixture density produced the correspondent 
\begin_inset Formula $\mathbf{Y}_{i}$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
P\left(X_{i}=j\right) & = & \begin{cases}
\alpha_{0} & \text{ if }x_{i}=0\\
\alpha_{1} & \text{ if }x_{i}=1\\
\vdots\\
\alpha_{m} & \text{ if }x_{i}=m
\end{cases}
\end{eqnarray*}

\end_inset

Hence 
\begin_inset Formula 
\begin{eqnarray*}
P\left(X_{i}=j\right) & = & a_{j}
\end{eqnarray*}

\end_inset

And the 
\begin_inset Formula $\mathbf{Y}_{i}|X_{i}$
\end_inset

 takes the form 
\begin_inset Formula 
\[
f_{\mathbf{Y}_{i}|X_{i}}\left(\mathbf{y}_{i}|x_{i}=j;\boldsymbol{\theta}\right)=g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)
\]

\end_inset

The 
\begin_inset Formula $x_{i}$
\end_inset

 in 
\begin_inset Formula $g_{x_{i}}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{x_{i}}\right)$
\end_inset

 
\begin_inset Quotes eld
\end_inset

picks
\begin_inset Quotes erd
\end_inset

 which mixture component the 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

 is generated from.
 Then the log-likelihood becomes
\begin_inset Formula 
\begin{eqnarray*}
l\left(\left(x_{1},\dots,x_{n}\right),\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right) & = & \log\left[\prod_{i=1}^{n}f_{X_{i},\mathbf{Y}_{i}}\left(x_{i},\mathbf{y}_{i};\boldsymbol{\theta}\right)\right]\\
 & = & \log\left[\prod_{i=1}^{n}\left[f_{\mathbf{Y}_{i}|X_{i}}\left(\mathbf{y}_{i}|x_{i};\boldsymbol{\theta}\right)P\left(X_{i}=j\right)\right]\right]\\
 & = & \log\left[\prod_{i=1}^{n}\left[g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)\alpha_{j}\right]\right]\\
 & = & \sum_{i=1}^{n}\log\left(g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)\alpha_{j}\right)\\
 & = & \sum_{i=1}^{n}\left[\log\alpha_{j}+\log\left(g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)\right)\right]
\end{eqnarray*}

\end_inset

There are a lot of indices and subscripts and superscripts to keep track
 of - 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is all of the parameters, 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

 are observed samples (
\begin_inset Formula $n$
\end_inset

 of them), the 
\begin_inset Formula $X_{i}$
\end_inset

, which have realizations 
\begin_inset Formula $j$
\end_inset

, i.e.
 
\begin_inset Formula $X_{i}$
\end_inset

 comes up 
\begin_inset Formula $j$
\end_inset

, are the unobserved data (the mixture components), the 
\begin_inset Formula $\boldsymbol{\theta}_{j}$
\end_inset

 are the parameters of the 
\begin_inset Formula $j$
\end_inset

th mixture density, and the 
\begin_inset Formula $\alpha_{j}$
\end_inset

 are the mixture coefficients (i.e.
 in what proportion the 
\begin_inset Formula $j$
\end_inset

th density contributes).
 Much as we did for Example 1 we need to compute 
\begin_inset Formula $f_{X_{i}|\mathbf{Y}_{i}}\left(x_{i}|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)=P\left(X_{i}=j|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)$
\end_inset

 in order to compute the conditional expectations 
\begin_inset Formula 
\begin{eqnarray*}
P\left(X_{i}=j|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right) & = & \frac{f_{X_{i},\mathbf{Y}_{i}}\left(j,\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)}{f_{\mathbf{Y}_{i}}\left(\mathbf{y}_{i};\theta\right)}\\
 & = & \frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}
\end{eqnarray*}

\end_inset

So again 
\begin_inset Formula $X_{i}|\mathbf{Y}_{i}$
\end_inset

 is categorical random variable.
 By i.i.d 
\begin_inset Formula 
\[
P\left(\left(X_{1}=j_{1},\dots,X_{n}=j_{n}\right)|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right)=\prod_{i=1}^{n}\frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j_{i}}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}
\]

\end_inset

Now we just need to take the expectation of the log-likelihood against this
 conditional density
\begin_inset Formula 
\begin{eqnarray*}
E\left[l\left(\left(X_{1},\dots,X_{n}\right),\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]=\\
\sum_{j_{1}=1}^{m}\sum_{j_{2}=1}^{m}\cdots\sum_{j_{n}=1}^{m}\left(\left[\sum_{k=1}^{n}\left\{ \log\alpha_{j_{k}}+\log\left[g_{j_{k}}\left(\mathbf{y}_{k};\boldsymbol{\theta}_{j_{k}}\right)\right]\right\} \right]\prod_{i=1}^{n}\frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}\right)
\end{eqnarray*}

\end_inset

Pretty ugly right? Suffice it to say you're not actually taking this expectation.
 So let 
\begin_inset Formula 
\[
\gamma_{ij}^{\left(r\right)}\coloneqq P\left(X_{i}=j|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)=\frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}
\]

\end_inset

and a helpful lemma:
\end_layout

\begin_layout Lemma
For i.i.d incomples samples 
\begin_inset Formula $Y_{i}$
\end_inset

, with completions 
\begin_inset Formula $X_{i}$
\end_inset


\begin_inset Formula 
\[
E\left[l\left(\left(X_{1},\dots,X_{n}\right),\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]=\sum_{i=1}^{n}E\left[l\left(X_{i}|\mathbf{y}_{i};\boldsymbol{\theta}\right)|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right]
\]

\end_inset

where 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

 is the ith sample.
\end_layout

\begin_deeper
\begin_layout Proof
By i.i.d
\begin_inset Formula 
\begin{align*}
E\left[l\left(\left(X_{1},\dots,X_{n}\right),\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
E\left[\log\left[\prod_{i=1}^{n}f\left(X_{i}|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)\right]|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
E\left[\sum_{i=1}^{n}\log\left[f\left(X_{i}|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)\right]|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\left(\text{ since }X_{i}\perp X_{j}\text{ for }i\neq j\right)\\
\sum_{i=1}^{n}E\left[\log\left[f\left(X_{i}|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)\right]|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\left(\text{ since }P\left(X_{i}=x_{i}|\left(\mathbf{Y}_{1},\dots,\mathbf{Y}_{n}\right)\right)=P\left(X_{i}=x_{i}|\mathbf{Y}_{i}\right)\right)\\
\sum_{i=1}^{n}E\left[\log\left[f\left(X_{i}|\mathbf{y}_{i};\boldsymbol{\theta}\right)\right]|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
\sum_{i=1}^{n}E\left[l\left(X_{i}|\mathbf{y}_{i};\boldsymbol{\theta}\right)|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right]
\end{align*}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Therefore we only need to compute 
\begin_inset Formula $E\left[l\left(X_{i}|\mathbf{y}_{i};\boldsymbol{\theta}\right)|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

 to perform the E-step.
 So letting 
\begin_inset Formula $j_{i}\rightarrow j$
\end_inset

 for convenience and 
\begin_inset Formula $\left(\mu_{j},\Sigma_{j}\right)$
\end_inset

 be the parameters for the 
\begin_inset Formula $j$
\end_inset

th Gaussian.
 Actually let's take the 1 dimensional Gaussian case
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
key "chen2010demystified"

\end_inset

 is a good place for the general 
\begin_inset Formula $n$
\end_inset

 case.
\end_layout

\end_inset

 so 
\begin_inset Formula $\left(\mu_{j},\Sigma_{j}\right)=\left(\mu_{j},\sigma_{j}^{2}\right)$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}_{i}\rightarrow y_{i}$
\end_inset

.
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\begin{eqnarray*}
E\left[l\left(X_{i}|y_{i};\boldsymbol{\theta}\right)|y_{i};\boldsymbol{\theta}^{\left(r\right)}\right] & = & \sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\log\left(g_{j}\left(y_{i};\boldsymbol{\theta}_{j}\right)\right)\right]\gamma_{ij}^{\left(r\right)}\right)\\
 & = & \sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\left(-\frac{1}{2}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2\sigma_{j}^{2}}\left(y_{i}-\mu_{j}\right)^{2}\right)\right]\gamma_{ij}^{\left(r\right)}\right)
\end{eqnarray*}

\end_inset

Then 
\begin_inset Formula 
\begin{eqnarray*}
E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
\sum_{i=1}^{n}\sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\log\left(g_{j}\left(y_{i};\boldsymbol{\theta}_{j}\right)\right)\right]\gamma_{ij}^{\left(r\right)}\right) & =\\
\sum_{i=1}^{n}\sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\left(-\frac{1}{2}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2\sigma_{j}^{2}}\left(y_{i}-\mu_{j}\right)^{2}\right)\right]\gamma_{ij}^{\left(r\right)}\right)
\end{eqnarray*}

\end_inset

Define 
\begin_inset Formula 
\[
n_{j}^{\left(r\right)}=\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}=\sum_{i=1}^{n}P\left(X_{i}=j|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)
\]

\end_inset

This is something like the portion of the samples that were generated by
 the 
\begin_inset Formula $j$
\end_inset

th component of the density
\begin_inset Foot
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Why? Because 
\begin_inset Formula $\sum_{j=1}^{m}n_{j}=\sum_{j=1}^{m}\sum_{i=1}^{n}\gamma_{ij}=\sum_{i=1}^{n}1=n$
\end_inset

.
\end_layout

\end_inset

.
 The expectation becomes 
\begin_inset Formula 
\begin{eqnarray*}
E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
\sum_{i=1}^{n}\sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\left(-\frac{1}{2}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2\sigma_{j}^{2}}\left(y_{i}-\mu_{j}\right)^{2}\right)\right]\gamma_{ij}^{\left(r\right)}\right) & =\\
\sum_{i=1}^{n}\sum_{j=1}^{m}\log\alpha_{j}\gamma_{ij}^{\left(r\right)}-\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{1}{2}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)\gamma_{ij}^{\left(r\right)}-\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{1}{2\sigma_{j}^{2}}\left(y_{i}-\mu_{j}\right)^{2}\gamma_{ij}^{\left(r\right)} & =\\
\sum_{j=1}^{m}n_{j}^{\left(r\right)}\log\alpha_{j}-\frac{1}{2}\sum_{j=1}^{m}n_{j}^{\left(r\right)}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{\left(y_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{2}}\gamma_{ij}^{\left(r\right)}
\end{eqnarray*}

\end_inset

This is the expression that needs to be maximized with respect to 
\begin_inset Formula $\alpha_{j},\mu_{j},\sigma_{j}^{2}$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Pay attention to the difference between the arguments that come from the
 log 
\begin_inset Formula $\alpha_{j},\mu_{j},\sigma_{j}^{2}$
\end_inset

 and the current estimates of the parameters, those with iterate scripts
 
\begin_inset Formula $\left(r\right)$
\end_inset

 that come from conditional density, with respect to which the expectation
 was taken.
\end_layout

\end_inset

 .
 The fully specified maximization problem is 
\begin_inset Formula 
\begin{align*}
\max_{\alpha_{j},\mu_{j},\sigma_{j}^{2}} & E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]\\
 & \iff\\
\max_{\alpha_{j},\mu_{j},\sigma_{j}^{2}} & \sum_{j=1}^{m}n_{j}^{\left(r\right)}\log\alpha_{j}-\frac{1}{2}\sum_{j=1}^{m}n_{j}^{\left(r\right)}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{\left(y_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{2}}\gamma_{ij}^{\left(r\right)}\\
\text{subject to} & \sum_{j=1}^{m}\alpha_{j}=1,\:\alpha_{j}\geq0
\end{align*}

\end_inset

This looks tough but because it's a bunch of uncoupled (somewhat) terms
 summed together so it's easier than the original log-likelihood 
\begin_inset Formula 
\[
\log\left[\sum_{j=1}^{m}\alpha_{j}g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)\right]
\]

\end_inset

The local maxima for 
\begin_inset Formula $\alpha_{j},\mu_{j},\sigma_{j}^{2}$
\end_inset

 become the new iterates, ie.
 
\begin_inset Formula $\alpha_{j}^{\left(r+1\right)},\mu_{j}^{\left(r+1\right)},\left(\sigma_{j}^{2}\right)^{\left(r+1\right)}$
\end_inset

.
 To perform the constrained maximization (only the 
\begin_inset Formula $\alpha_{j}$
\end_inset

) we use Lagrange multipliers; form the Lagrangian
\begin_inset Formula 
\[
\mathcal{L}\left(\boldsymbol{\alpha},\lambda\right)=\sum_{j=1}^{m}n_{j}^{\left(r\right)}\log\alpha_{j}+\lambda\left(\sum_{j=1}^{m}\alpha_{j}-1\right)
\]

\end_inset

Then computing derivatives and setting to zero to find 
\begin_inset Formula $\alpha_{j}$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\mathcal{L}}{\partial\alpha_{j}} & = & \frac{n_{j}^{\left(r\right)}}{\alpha_{j}}+\lambda
\end{eqnarray*}

\end_inset

Therefore the critical
\begin_inset Foot
status open

\begin_layout Plain Layout
You need to do the convex analysis (second-deriative test) to determine
 whether 
\begin_inset Formula $\alpha_{j}$
\end_inset

 are maxima but they are and I'm not going to.
\end_layout

\end_inset

 
\begin_inset Formula $\alpha_{j}$
\end_inset

 satisfy 
\begin_inset Formula 
\[
\alpha_{j}=-\frac{n_{j}^{\left(r\right)}}{\lambda}
\]

\end_inset

To eliminate 
\begin_inset Formula $\lambda$
\end_inset

 use the equality constraint, i.e.
 
\begin_inset Formula $\sum_{j=1}^{m}\alpha_{j}=1$
\end_inset

 implies 
\begin_inset Formula $\lambda=-\sum_{j=1}^{m}n_{j}^{\left(r\right)}$
\end_inset

 and hence
\begin_inset Formula 
\[
\alpha_{j}^{\left(r+1\right)}=\frac{n_{j}^{\left(r\right)}}{\sum_{j=1}^{m}n_{j}^{\left(r\right)}}=\frac{n_{j}^{\left(r\right)}}{n}
\]

\end_inset

So 
\begin_inset Formula $\alpha_{j}^{\left(r+1\right)}$
\end_inset

 is just the current best estimate for how many of the samples were generated
 by the 
\begin_inset Formula $j$
\end_inset

th component of the mixture density
\begin_inset Foot
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Pretty much the epitome of the MLE for a parameter.
\end_layout

\end_inset

.
 To compute the updates 
\begin_inset Formula $\mu_{j}^{\left(r+1\right)}$
\end_inset

 we just need to do the standard maximization of 
\begin_inset Formula $E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

 since the 
\begin_inset Formula $\mu_{j}$
\end_inset

 are unconstrained.
 Therefore
\begin_inset Formula 
\[
\frac{\partial}{\partial\mu_{j}}E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]=\sum_{i=1}^{n}\frac{\left(y_{i}-\mu_{j}\right)}{\sigma_{j}^{2}}\gamma_{ij}^{\left(r\right)}=0
\]

\end_inset

Note the sum of 
\begin_inset Formula $j$
\end_inset

 disappears because only the 
\begin_inset Formula $j$
\end_inset

th term is non-zero.
 Then simplifying further
\begin_inset Formula 
\begin{eqnarray*}
\sum_{i=1}^{n}\frac{\left(y_{i}-\mu_{j}\right)}{\sigma_{j}^{2}}\gamma_{ij}^{\left(r\right)} & = & \frac{1}{\sigma_{j}^{2}}\left(\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}y_{i}-\mu_{j}\sum_{i=1}\gamma_{ij}^{\left(r\right)}\right)\\
 & = & \frac{1}{\sigma_{j}^{2}}\left(\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}y_{i}-\mu_{j}n_{j}^{\left(r\right)}\right)=0\\
 & \Rightarrow\\
\mu_{j}^{\left(r+1\right)} & = & \frac{1}{n_{j}^{\left(r\right)}}\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}y_{i}
\end{eqnarray*}

\end_inset

Finally doing the same kind of thing for 
\begin_inset Formula $\left(\sigma_{j}^{2}\right)^{\left(r+1\right)}$
\end_inset


\begin_inset Formula 
\[
\frac{\partial}{\partial\sigma_{j}^{2}}E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]=-\frac{1}{2}\frac{n_{j}^{\left(r\right)}}{\sigma_{j}^{2}}+\frac{1}{2}\sum_{i=1}^{n}\frac{\left(y_{i}-\mu_{j}^{\left(r+1\right)}\right)^{2}}{\left(\sigma_{j}^{2}\right)^{2}}\gamma_{ij}^{\left(r\right)}=0
\]

\end_inset

where 
\begin_inset Formula $\mu_{j}$
\end_inset

 now gets a superscript because it's already been updated.
 Hence 
\begin_inset Formula 
\[
\left(\sigma_{j}^{2}\right)^{\left(r+1\right)}=\frac{1}{n_{j}^{\left(r+1\right)}}\sum_{i=1}^{n}\left(y_{i}-\mu_{j}^{\left(r+1\right)}\right)^{2}\gamma_{ij}^{\left(r\right)}
\]

\end_inset

In summary the update equations for a univariate GMM 
\begin_inset Formula 
\begin{align*}
\alpha_{j}^{\left(r+1\right)} & =\frac{n_{j}^{\left(r\right)}}{n}\\
\mu_{j}^{\left(r+1\right)} & =\frac{1}{n_{j}^{\left(r\right)}}\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}y_{i}\\
\left(\sigma_{j}^{2}\right)^{\left(r+1\right)} & =\frac{1}{n_{j}^{\left(r+1\right)}}\sum_{i=1}^{n}\left(y_{i}-\mu_{j}^{\left(r+1\right)}\right)^{2}\gamma_{ij}^{\left(r\right)}
\end{align*}

\end_inset

where 
\begin_inset Formula 
\begin{align*}
\gamma_{ij}^{\left(r\right)} & =\frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}\\
n_{j}^{\left(r\right)} & =\sum_{i}\gamma_{ij}^{\left(r\right)}
\end{align*}

\end_inset

For multivariate GMM the only difference is 
\begin_inset Formula $\sigma_{j}^{2}\rightarrow\Sigma_{j}$
\end_inset

 and so you need to use matrix derivatives.
 Consult 
\begin_inset CommandInset citation
LatexCommand cite
key "chen2010demystified"

\end_inset

 for the full derivation 
\begin_inset Formula 
\[
\Sigma_{j}^{\left(r+1\right)}=\frac{1}{n_{j}^{\left(r+1\right)}}\sum_{i=1}^{n}\left(\mathbf{y}_{i}-\boldsymbol{\mu}_{j}^{\left(r+1\right)}\right)\left(\mathbf{y}_{i}-\boldsymbol{\mu}_{j}^{\left(r+1\right)}\right)^{T}\gamma_{ij}^{\left(r\right)}
\]

\end_inset


\end_layout

\begin_layout Subsection
Hidden Markov Model
\end_layout

\begin_layout Standard
A stochastic process is a sequence of random variables 
\begin_inset Formula $X_{i}$
\end_inset

 drawn-from/distribited-according-to a common distribution 
\begin_inset Formula $f_{X}$
\end_inset

.
 A Hidden Markov model (HMM) is a model driven by a 
\begin_inset Quotes eld
\end_inset

hidden
\begin_inset Quotes erd
\end_inset

 stochastic process 
\begin_inset Formula $\left(X_{1},X_{2}\dots\right)$
\end_inset

, with observations 
\begin_inset Formula $\left(Y_{1},Y_{2},\dots\right)$
\end_inset

.
 The subscripts can be interpreted to mean time.
 So at each unit time step 
\begin_inset Formula $i$
\end_inset

 a draw 
\begin_inset Formula $X_{i}$
\end_inset

 is made from the distribution 
\begin_inset Formula $f_{X}$
\end_inset

.
 Hidden is in the sense that at any given time the result of the draw is
 unknown.
 The Markov qualifier indicates that the process obeys the Markov property,
 specifically the conditional independence of the next draw given the far
 past, i.e.
\begin_inset Formula 
\[
P\left(X_{i+1}|X_{i},X_{i-1},\dots,X_{1}\right)=P\left(X_{i+1}|X_{i}\right)
\]

\end_inset

If 
\begin_inset Formula $f_{X}$
\end_inset

 is discrete then the process is called a 
\series bold
chain
\series default
 and the value from 
\begin_inset Formula $\left\{ x_{1},\dots,x_{n}\right\} $
\end_inset

 that 
\begin_inset Formula $X_{i}$
\end_inset

 takes on at time step 
\begin_inset Formula $i$
\end_inset

 is called the 
\emph on
state
\emph default
 of the process at time 
\begin_inset Formula $i$
\end_inset

.
 The set of observables 
\begin_inset Formula $\left\{ y_{1},y_{2},\dots,y_{m}\right\} $
\end_inset

 is such that at each time step 
\begin_inset Formula $i$
\end_inset

 another draw is made from a 
\series bold
different
\series default
 distribution that's dependent on which state the Markov chain is in.
 That is to say, at each time step 
\begin_inset Formula $i$
\end_inset

 a draw 
\begin_inset Formula $Y_{i}$
\end_inset

 is made from a distribution on 
\begin_inset Formula $\left\{ y_{1},y_{2},\dots,y_{m}\right\} $
\end_inset

 which is a function of 
\begin_inset Formula $x_{i}$
\end_inset

 where 
\begin_inset Formula $X_{i}=x_{i}$
\end_inset

, i.e.
 
\begin_inset Formula $Y_{i}\sim f_{x_{i}}$
\end_inset

.
 The set of observables is common to all states of the Markov chain but
 the probabilities with which each Markov chain state generates observables
 differ.
 
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename HiddenMarkovModel.svg
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
3 state - 4 observable HMM
\begin_inset CommandInset label
LatexCommand label
name "fig:3s4ohmm"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

So the distribution of 
\begin_inset Formula $Y_{i}$
\end_inset

 is implicitly a function of time step 
\begin_inset Formula $i$
\end_inset

, but only insofar as the hidden Markov process changes state at each time
 step.
 For example figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3s4ohmm"

\end_inset

 is a state transition diagram for an HMM with 3 hidden states and 4 observable
 states.
 The Markov process iterates through, for example, 
\begin_inset Formula $\left(X_{1}=x_{2},X_{2}=x_{1},X_{3}=x_{2},\dots,X_{n}=x_{3}\right)$
\end_inset

 and generates observations 
\begin_inset Formula $\left(Y_{1}=y_{4},Y_{2}=y_{3},\dots,Y_{n}=y_{4}\right)$
\end_inset

.
 
\begin_inset Foot
status open

\begin_layout Plain Layout
This is a rather uninteresting Markov chain because there are no self-transition
s and no transition from 
\begin_inset Formula $X_{3}$
\end_inset

 to 
\begin_inset Formula $X_{2}$
\end_inset

 is possible.
 In general all transitions could be allowed.
 
\end_layout

\end_inset

The parameters 
\begin_inset Formula $a_{ij}$
\end_inset

 are the probabilities for the Markov chain transitions, i.e.
 
\begin_inset Formula 
\[
P\left(X_{k+1}=x_{j}|X_{k}=x_{i}\right)=a_{i,j}
\]

\end_inset

and the parameters 
\begin_inset Formula $b_{q}\left(y_{r}\right)$
\end_inset

 are the probabilities of each state 
\begin_inset Formula $x_{q}$
\end_inset

 emitting a 
\begin_inset Formula $Y_{k}=y_{r}$
\end_inset

 observable, i.e.
\begin_inset Formula 
\[
P\left(Y_{k}=y_{r}|X_{k}=x_{q}\right)=b_{q}\left(y_{r}\right)
\]

\end_inset

The fundamental assumption of the HMM
\begin_inset Foot
status open

\begin_layout Plain Layout
Aside from the Markov property.
\end_layout

\end_inset

 is the conditional independence of the observable given the far past history
 of the process states and observables, i.e.
 
\begin_inset Formula 
\[
P\left(Y_{i+1}|Y_{i},X_{i},Y_{i-1},X_{i-1},\dots,Y_{1},X_{1}\right)=P\left(Y_{i+1}|X_{i}\right)
\]

\end_inset

Suppose 
\begin_inset Formula $X_{i}$
\end_inset

 can take 
\begin_inset Formula $n$
\end_inset

 distinct values and 
\begin_inset Formula $Y_{i}$
\end_inset

 each take 
\begin_inset Formula $m$
\end_inset

 distinct values.
 Then let 
\begin_inset Formula $A=\left\{ a_{ij}\right\} $
\end_inset

 be the 
\begin_inset Formula $n\times n$
\end_inset

 matrix of transition probabilities, 
\begin_inset Formula $B=\left\{ b_{q}\right\} $
\end_inset

 where 
\begin_inset Formula $b_{q}=\left\{ b_{q}\left(y_{0}\right),b_{q}\left(y_{1}\right),\dots,b_{q}\left(y_{m}\right)\right\} $
\end_inset

 be the discrete distribution on 
\begin_inset Formula $\left\{ y_{1},y_{2},\dots,y_{m}\right\} $
\end_inset

 if the chain is in state 
\begin_inset Formula $x_{q}$
\end_inset

, and 
\begin_inset Formula $\pi$
\end_inset

 be the initial distribution
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $A$
\end_inset

 specifies what states the chain is likely to iterate through but it doesn't
 say anything about where the chain starts; 
\begin_inset Formula $\pi$
\end_inset

 is that set of initial probabilities.
\end_layout

\end_inset

on 
\begin_inset Formula $\left\{ x_{1},\dots,x_{n}\right\} $
\end_inset

.
 where 
\begin_inset Formula $\boldsymbol{\theta}=\left(A,B,\pi\right)$
\end_inset

.
 And so, tellingly, we see a likelihood
\begin_inset Foot
status open

\begin_layout Plain Layout
Hint hint!
\end_layout

\end_inset

 written as an explicit function of parameters.
\end_layout

\begin_layout Standard
There are three possible problems one can solve in the context of an HMM
\end_layout

\begin_layout Enumerate
Compute the probabilities for some sequence of observations 
\begin_inset Formula $\left(Y_{1}=y_{4},Y_{2}=y_{3},\dots,Y_{k}=y_{4}\right)$
\end_inset

 given that the 
\begin_inset Formula $\left(A,B,\pi\right)$
\end_inset

 are known.
\end_layout

\begin_layout Enumerate
Compute the most likely sequence of hidden states 
\begin_inset Formula $\left(X_{1}=x_{2},X_{2}=x_{1},X_{3}=x_{2},\dots,X_{k}=x_{3}\right)$
\end_inset

 given some sequence of observations 
\begin_inset Formula $\left(Y_{1}=y_{4},Y_{2}=y_{3},\dots,Y_{k}=y_{4}\right)$
\end_inset

 and some known 
\begin_inset Formula $\left(A,B,\pi\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Find the MLE for 
\begin_inset Formula $\left(A,B,\pi\right)$
\end_inset

 given some sequence of 
\begin_inset Formula $\left(Y_{1}=y_{4},Y_{2}=y_{3},\dots,Y_{k}=y_{4}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Problem 1 is a matter of direct evaluate (just lots of products and sums),
 problem 2 is solved using the Viterbi algorithm, and problem 3 is solved
 using the Baum-Welch algorithm, also known as the forward backward algorithm,
 or EM for HMMs.
 This last one is what I'll cover.
\end_layout

\begin_layout Standard
The likelihood 
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta};\mathbf{y}\right)=P\left(\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

is complicated because 
\begin_inset Formula $Y_{i}$
\end_inset

 are not i.i.d; for example if we have a 2 state 2 observable HMM and 
\begin_inset Formula $P\left(Y=y_{0}|X=X_{1}\right)=0$
\end_inset

 and 
\begin_inset Formula $P\left(Y=y_{1}|X=X_{0}\right)=0$
\end_inset

 and 
\begin_inset Formula $P\left(X_{1}=x_{0}|X_{0}=x_{0}\right)=0$
\end_inset

 and the first observation is 
\begin_inset Formula $y_{0}$
\end_inset

 then we know the second must be 
\begin_inset Formula $y_{1}$
\end_inset

.
 So 
\begin_inset Formula $Y_{i}$
\end_inset

 are dependent through 
\begin_inset Formula $X_{i}$
\end_inset

, and the 
\begin_inset Formula $X_{i}$
\end_inset

, in general, are dependent.
 Maximization of 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 essentially has to be done over all 
\begin_inset Formula $k$
\end_inset

-length sequences 
\begin_inset Formula $\left(X_{1},\dots,X_{k}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
In general the probability of seeing any sequence of 
\begin_inset Formula $\left(X_{1},X_{2}\dots,X_{k}\right)$
\end_inset

, with observations 
\begin_inset Formula $\left(Y_{1},Y_{2},\dots,Y_{k}\right)$
\end_inset

 is 
\begin_inset Formula 
\[
P\left(\mathbf{X},\mathbf{Y};\boldsymbol{\theta}\right)=\pi_{x_{1}}\prod_{i=2}^{k-1}a_{x_{i},x_{i+1}}\prod_{i=1}^{k}b_{x_{i}}\left(y_{i}\right)
\]

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "gentle"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
