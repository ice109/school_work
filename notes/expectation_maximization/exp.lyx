#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsart
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Expectation Maximization: From the Horse's Mouth
\end_layout

\begin_layout Author
Maksim Levental
\end_layout

\begin_layout Standard
Expectation
\begin_inset Foot
status open

\begin_layout Plain Layout
These notes are basically a transcription of notes (taken dutifully by Chris
 Gianelli) from lectures given by Dr.
 Kshitij Khare at UF in Spring 15.
 He's not a horse - it's just an expression.
\end_layout

\end_inset

 maximization (EM) is a way to iteratively approximate the maximum likelihood
 estimators (MLEs) for a parametric family when solving the MLE equations
 analytically is intractable.
 Recall that finding the MLEs for a parametric family 
\begin_inset Formula 
\[
\mathbf{Y}\sim f_{\mathbf{Y}}\left(\cdot;\boldsymbol{\theta}\right)\,\boldsymbol{\theta}\in\Theta
\]

\end_inset

is tantamount to maximizing the likelihood function
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta};\mathbf{y}\right)=f_{\mathbf{Y}}\left(\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

as a function of 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 The maximum likelihood estimators are estimators of/for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

, usually denoted 
\begin_inset Formula $\hat{\boldsymbol{\theta}}$
\end_inset

.
 The actual maximization is effected by finding critical points of 
\begin_inset Formula $L$
\end_inset

 with respect to 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 and testing concavity, i.e.
 solving 
\begin_inset Formula 
\[
\nabla L\left(\boldsymbol{\theta};\mathbf{y}\right)=0
\]

\end_inset

and checking the negative definiteness of Hessian of 
\begin_inset Formula $L$
\end_inset

.
 In general 
\begin_inset Formula $\partial_{\theta_{i}}L\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 might be highly non-linear in 
\begin_inset Formula $\theta_{i}$
\end_inset

 and hence finding each might be very difficult.
 What to do?
\end_layout

\begin_layout Section
Expectation Maximization Beta
\end_layout

\begin_layout Standard
Suppose
\begin_inset Foot
status open

\begin_layout Plain Layout
Suppose!
\end_layout

\end_inset

 there exists another random, unobserved, vector 
\begin_inset Formula $\mathbf{X}$
\end_inset

 such that were 
\begin_inset Formula $\mathbf{X}$
\end_inset

 in fact observed, maximizing the joint likelihood for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)=f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

is easier than the original problem.
 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is called a latent or hidden variable.
 Now a priori there's absolutely no reason to assume that having more informatio
n in hand and complicating the model should make things easier.
 On the contrary, for example, a good physical model makes simplifying assumptio
ns and thereby becomes tractable.
 But indeed for some very useful models such as Hidden Markov Models and
 Gaussian Mixture Models this ansatz does simplify computing the MLEs.
\end_layout

\begin_layout Standard
All is going swimmingly except 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is unobserved - the only observed data are 
\begin_inset Formula $\mathbf{Y}$
\end_inset

.
 What to do? Estimate 
\begin_inset Formula $\mathbf{X}$
\end_inset

 of course.
 How? Using the best estimator 
\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right]$
\end_inset

 of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, based on observed
\series bold
 
\begin_inset Formula $\mathbf{y}$
\end_inset


\series default
, that minimizes the risk for the quadratic loss function
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $l\left(\mathbf{X},\hat{\mathbf{X}}\right)=\left(\mathbf{X}-\hat{\mathbf{X}}\right)^{2}$
\end_inset


\end_layout

\end_inset

, i.e.
 minimizes the mean square error.
 Note two things.
 Firstly, it's important that you can actually compute 
\emph on
this
\emph default
 in closed form, otherwise you've traded one intractable problem for another.
 Secondly, since 
\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right]$
\end_inset

 is implicitly a function of 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

, which is unknown to begin with
\begin_inset Foot
status open

\begin_layout Plain Layout
Don't forget the point of all this is actually to estimate 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
\end_layout

\end_inset

 you need some initial guess for it too, otherwise you can't compute the
 expectation.
 Hence the expectation computed is actually 
\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

 where 
\begin_inset Formula $\boldsymbol{\theta}^{\left(r\right)}$
\end_inset

 is the current guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 Then once you have this estimate for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 just maximize 
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta};E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right],\mathbf{y}\right)=f_{\mathbf{X},\mathbf{Y}}\left(E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right],\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

The procedure alternates between estimating 
\series bold
x 
\series default
using 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

 and maximizing 
\begin_inset Formula $L\left(\boldsymbol{\theta};E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right],\mathbf{y}\right)$
\end_inset

.
 Compute expectation, then perform maxization, compute expectation, then
 perform maximization, compute expectation,...
 hence 
\family default
\series bold
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
Expectation Maximization
\series default
 algorithm.
 Just to be clear 
\end_layout

\begin_layout Definition

\series bold
Expectation Algorithm Beta.
 
\series default
Given some observed data 
\begin_inset Formula $\mathbf{y}$
\end_inset

, in order to perform the intractable maximization of 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

, posit the existence of some latent 
\begin_inset Formula $\mathbf{x}$
\end_inset

 such that 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)$
\end_inset

 is easier.
 Set 
\begin_inset Formula $\boldsymbol{\theta}^{\left(0\right)}$
\end_inset

 to be some initial guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 then
\series bold
 
\end_layout

\begin_deeper
\begin_layout Enumerate
E-step: Set 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{x}^{\left(r\right)}=E\left[\mathbf{X}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset


\end_layout

\begin_layout Enumerate
M-step: Set 
\begin_inset Formula $\boldsymbol{\theta}^{\left(r+1\right)}=\arg\max_{\boldsymbol{\theta}}L\left(\boldsymbol{\theta};\mathbf{x}^{\left(r\right)},\mathbf{y}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Go to step 1 unless 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\left|\boldsymbol{\theta}^{\left(r+1\right)}-\boldsymbol{\theta}^{\left(r\right)}\right|<\varepsilon$
\end_inset

 for some 
\begin_inset Formula $\varepsilon$
\end_inset

 of your choosing.
\end_layout

\end_deeper
\begin_layout Standard
The equations in step 1 and 2 are called update equations because specify
 how to update the current estimate for 
\begin_inset Formula $\mathbf{x}^{\left(i\right)}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{\theta}^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Example
Here's an example: let 
\begin_inset Formula $Y_{i}$
\end_inset

 be iid such that 
\begin_inset Formula 
\[
f_{Y_{i}}\left(y_{i};\theta\right)=\theta g_{1}\left(y_{i}\right)+\left(1-\theta\right)g_{0}\left(y_{i}\right)
\]

\end_inset

where 
\begin_inset Formula $\theta\in\left(0,1\right)$
\end_inset

 and 
\begin_inset Formula $g_{0}$
\end_inset

 and 
\begin_inset Formula $g_{1}$
\end_inset

 are Gaussians with known means 
\begin_inset Formula $\left(\mu_{0},\mu_{1}\right)$
\end_inset

and variances 
\begin_inset Formula $\left(\sigma_{0}^{2},\sigma_{1}^{2}\right)$
\end_inset

.
 We want the MLE for 
\begin_inset Formula $\theta$
\end_inset

.
 Note that 
\begin_inset Formula 
\[
L\left(\theta;\mathbf{y}\right)=\prod_{i=1}^{n}\left(\theta g_{1}\left(y_{i}\right)+\left(1-\theta\right)g_{0}\left(y_{i}\right)\right)
\]

\end_inset

Quite messy
\begin_inset Foot
status open

\begin_layout Plain Layout
Even if you think you're clever and try to maximize log-likelihood you're
 still going to have a rough go at it.
\end_layout

\end_inset

.
 What to do? EM to the rescue! Suppose the 
\begin_inset Formula $Y_{i}$
\end_inset

 are drawn by a process where a 
\begin_inset Formula $\theta$
\end_inset

-biased coin is flipped and either 
\begin_inset Formula $g_{0}$
\end_inset

 or 
\begin_inset Formula $g_{1}$
\end_inset

 generates the 
\begin_inset Formula $y_{i}$
\end_inset

 depending on whether the coin lands heads up or down.
 The latent variable here then is which Gaussian generated 
\begin_inset Formula $y_{i}$
\end_inset

.
 Hence let 
\begin_inset Formula $X_{i}$
\end_inset

 be Bernoulli random variables where 
\begin_inset Formula 
\begin{eqnarray*}
P\left(X_{i}=x_{i}\right) & = & \begin{cases}
\theta & \text{ if }x_{i}=1\\
1-\theta & \text{ if }x_{i}=0
\end{cases}\\
 & = & \theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}
\end{eqnarray*}

\end_inset

and 
\begin_inset Formula $f_{Y_{i}|X_{i}}\left(y_{i}|x_{i}=1;\theta\right)=g_{1}\left(y_{i}\right)$
\end_inset

 and 
\begin_inset Formula $f_{Y_{i}|X_{i}}\left(y_{i}|x_{i}=0;\theta\right)=g_{0}\left(y_{i}\right)$
\end_inset

.
 Then 
\begin_inset Formula 
\[
f_{X_{i},Y_{i}}\left(x_{i},y_{i};\theta\right)=f_{Y_{i}|X_{i}}\left(y_{i}|x_{i};\theta\right)P\left(X_{i}=x_{i}\right)
\]

\end_inset

and so
\begin_inset Formula 
\begin{eqnarray*}
L\left(\theta;\mathbf{x},\mathbf{y}\right) & = & \prod_{i=1}^{n}f_{Y_{i}|X_{i}}\left(y_{i}|x_{i};\theta\right)P\left(X_{i}=x_{i}\right)\\
 & = & \prod_{i=1}^{n}\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}\\
 & = & \theta^{\sum_{i}x_{i}}\left(1-\theta\right)^{n-\sum_{i}x_{i}}\prod_{i=1}^{n}\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}
\end{eqnarray*}

\end_inset

Note the trick in writing 
\begin_inset Formula $f_{Y_{i}|X_{i}}\left(y_{i}|x_{i};\theta\right)=\left(g_{1}\right)^{x_{i}}\left(g_{0}\right)^{1-x_{i}}$
\end_inset

 - it comes up a lot for a class of models called mixture models.
 For the E-step, to compute 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $E\left[\mathbf{X}|\mathbf{y};\theta'\right]$
\end_inset

 we use i.i.d-ness and compute 
\begin_inset Formula $E\left[X_{i}|y_{i};\theta'\right]$
\end_inset

 instead, which will hold for each 
\begin_inset Formula $i$
\end_inset

.
 To compute the conditional expectation 
\begin_inset Formula $E\left[X_{i}|y_{i};\theta\right]$
\end_inset

 we need the conditional distribution 
\begin_inset Formula $f_{X_{i}|Y_{i}}\left(x_{i}|y_{i}\right)$
\end_inset

.
 By Bayes' Theorem
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\begin_inset Formula 
\begin{eqnarray*}
f_{X_{i}|Y_{i}}\left(x_{i}|y_{i};\theta\right) & = & \frac{f_{X_{i},Y_{i}}\left(x_{i},y_{i};\theta\right)}{f_{Y_{i}}\left(y_{i};\theta\right)}\\
 & = & \frac{f_{X_{i},Y_{i}}\left(x_{i},y_{i};\theta\right)}{\sum_{x_{i}}f_{X_{i},Y_{i}}\left(x_{i},y_{i};\theta\right)}\\
 & = & \frac{\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}}{\sum_{x_{i}}\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}}\\
 & = & \frac{\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}}{g_{1}\left(y_{i}\right)\theta+g_{0}\left(y_{i}\right)\left(1-\theta\right)}\\
 & = & \left(\frac{g_{1}\left(y_{i}\right)\theta}{g_{1}\left(y_{i}\right)\theta+g_{0}\left(y_{i}\right)\left(1-\theta\right)}\right)^{x_{i}}\left(\frac{g_{0}\left(y_{i}\right)\left(1-\theta\right)}{g_{1}\left(y_{i}\right)\theta+g_{0}\left(y_{i}\right)\left(1-\theta\right)}\right)^{1-x_{i}}
\end{eqnarray*}

\end_inset

So 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $X_{i}|Y_{i}$
\end_inset

 is still Bernoulli just renormalized.
 Hence 
\begin_inset Formula 
\[
E\left[X_{i}|y_{i};\theta\right]=\frac{g_{1}\left(y_{i}\right)\theta}{g_{1}\left(y_{i}\right)\theta+g_{0}\left(y_{i}\right)\left(1-\theta\right)}
\]

\end_inset

For the M-step, since 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula 
\begin{eqnarray*}
L\left(\theta;\mathbf{x},\mathbf{y}\right) & = & \left[\theta^{\sum_{i}x_{i}}\left(1-\theta\right)^{n-\sum_{i}x_{i}}\right]\left[\prod_{i=1}^{n}\left(g_{1}\left(y_{i}\right)\right)^{x_{i}}\left(g_{0}\left(y_{i}\right)\right)^{1-x_{i}}\right]
\end{eqnarray*}

\end_inset

and the second term is independent of 
\begin_inset Formula $\theta$
\end_inset

 we can just maximize the first term.
 But this is just the the joint distribution for 
\begin_inset Formula $n$
\end_inset

 i.i.d Bernoulli random variables and the MLE 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{\theta}=\frac{\sum_{i}x_{i}}{n}
\]

\end_inset

Therefore, finally, the update equations are 
\begin_inset Formula 
\begin{eqnarray*}
x_{i}^{\left(r\right)} & = & \frac{g_{1}\left(y_{i}\right)\theta^{\left(r\right)}}{g_{1}\left(y_{i}\right)\theta^{\left(r\right)}+g_{0}\left(y_{i}\right)\left(1-\theta^{\left(r\right)}\right)}\\
\theta^{\left(r+1\right)} & = & \frac{1}{n}\sum_{i=1}^{n}x_{i}^{\left(r\right)}
\end{eqnarray*}

\end_inset


\begin_inset Formula $\blacksquare$
\end_inset


\end_layout

\begin_layout Standard
So why did I dub it EM
\series bold
 Beta
\series default
 instead of just EM? Because this isn't the standard EM algorithm.
 But why alter this algorithm at all? What's wrong with it as is? Well there
 are no convergence guarantees.
 Indeed the iterates in Example 1 don't converge to the right answer: figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "embetaiterates"

\end_inset

 shows the first 100 iterates for 
\begin_inset Formula $g_{1}\sim\text{n}\left(1,2\right),g_{0}\sim\text{n}\left(3,4\right),\theta=2/3$
\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename embetaiterates.jpeg
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "embetaiterates"

\end_inset

EM Beta
\end_layout

\end_inset


\end_layout

\end_inset

So they converge but to 
\begin_inset Formula $1\neq2/3$
\end_inset

.
 Why did I present this algorithm first? Standard EM is slightly more complicate
d and much less intuitive but legend has it that it was in fact conceived
 in this way first and then manipulated to get convergence.
 
\end_layout

\begin_layout Section
Expectation Maximization for Real
\end_layout

\begin_layout Standard
Recall that the whole point of this procedure is to actually maximize the
 likelihood for 
\begin_inset Formula $\mathbf{Y}$
\end_inset

.
 This is equivalent to maximizing the log-likelihood for 
\begin_inset Formula $\mathbf{Y}$
\end_inset


\begin_inset Formula 
\[
l\left(\boldsymbol{\theta};\mathbf{y}\right)=\log L\left(\boldsymbol{\theta};\mathbf{y}\right)=\log f_{\mathbf{Y}}\left(\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

The ansatz here is the same: suppose there exists another random, unobserved,
 vector 
\begin_inset Formula $\mathbf{X}$
\end_inset

 such that were 
\begin_inset Formula $\mathbf{X}$
\end_inset

 in fact observed, maximizing the joint log-likelihood for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 
\begin_inset Formula 
\[
l\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)=\log L\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)=\log f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)
\]

\end_inset

is easier than the original problem.
 The ease of maximizing 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)$
\end_inset

 over 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 isn't immediately apparent but first using Bayes' theorem we see that
\begin_inset Formula 
\begin{eqnarray}
l\left(\boldsymbol{\theta};\mathbf{y}\right) & = & \log f_{\mathbf{Y}}\left(\mathbf{y};\boldsymbol{\theta}\right)\\
 & = & \log\left(\frac{f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)}\right)\\
 & = & \log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)\right)-\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)\right)\label{eq:2.3}
\end{eqnarray}

\end_inset

If we can easily maximize the first term in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.3"

\end_inset

, with respect to 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 and the second term in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.3"

\end_inset

 doesn't spoil things somehow, then we'll indeed maximize 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

.
 This seems like a workflow different from that of the beta algorithm
\begin_inset Foot
status open

\begin_layout Plain Layout
Here we're explicitly maximizing a function of the likelihood for 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 by maximizing a lowerbound.
\end_layout

\end_inset

 but the analogy follows.
 Since 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is unobserved we face the same difficulty as in the beta algorithm: the
 terms in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.3"

\end_inset

 can't be computed and must be estimated.
 We use the same estimator as before 
\begin_inset Formula $E\left[\cdot|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset


\begin_inset Formula 
\begin{equation}
l\left(\boldsymbol{\theta};\mathbf{y}\right)=E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]-E\left[\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]\label{eq:2.4}
\end{equation}

\end_inset

Note some subtle things: 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathbf{x}\rightarrow\mathbf{X}$
\end_inset

 because each of the terms on the right-hand side of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.4"

\end_inset

 are estimates of the random variable 
\series bold
X 
\series default
as a function of observed data 
\series bold
y.
\end_layout

\begin_layout Itemize
The expectations are again computed with respect to the conditional distribution
 of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 given 
\series bold
Y
\series default
 for fixed (iterate) values of 
\begin_inset Formula $\boldsymbol{\theta}^{\left(r\right)}$
\end_inset

, i.e.
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)$
\end_inset

.

\family default
\series bold
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\end_layout

\begin_layout Itemize
The estimators, for which the expectations are computed, are functions of
 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
\end_layout

\begin_layout Itemize
Equality is maintained because since 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 is independent of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 
\begin_inset Formula 
\[
E\left[l\left(\boldsymbol{\theta};\mathbf{y}\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]=l\left(\boldsymbol{\theta};\mathbf{y}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The algorithm then is
\end_layout

\begin_layout Definition

\series bold
Expectation Algorithm.
 
\series default
Given some observed data 
\begin_inset Formula $\mathbf{y}$
\end_inset

, in order to perform the intractable maximization of 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

, posit the existence of some latent 
\begin_inset Formula $\mathbf{X}$
\end_inset

 such that 
\begin_inset Formula $L\left(\boldsymbol{\theta};\mathbf{x},\mathbf{y}\right)$
\end_inset

 is easier.
 Set 
\begin_inset Formula $\boldsymbol{\theta}^{\left(0\right)}$
\end_inset

 to be some initial guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 then
\series bold
 
\end_layout

\begin_deeper
\begin_layout Enumerate
E-step: Compute 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset


\end_layout

\begin_layout Enumerate
M-step: Set 
\begin_inset Formula $\boldsymbol{\theta}^{\left(r+1\right)}=\arg\max_{\boldsymbol{\theta}}E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset


\end_layout

\begin_layout Enumerate
Go to step 1 unless 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\left|\boldsymbol{\theta}^{\left(r+1\right)}-\boldsymbol{\theta}^{\left(r\right)}\right|<\varepsilon$
\end_inset

 for some 
\begin_inset Formula $\varepsilon$
\end_inset

 of your choosing.
\end_layout

\end_deeper
\begin_layout Standard
The only thing to remains is to prove that maximizing 
\begin_inset Formula $E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

 is sufficient to maximize 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

.
 I won't but I'll prove a thing on the way to that result, namely that 
\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)\geq l\left(\boldsymbol{\theta}^{\left(r-1\right)};\mathbf{y}\right)$
\end_inset

 and since 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 is bounded above (it's a density) the sequence of 
\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)$
\end_inset

 must converge.
 The last hurdle would be proving that convergence of 
\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)$
\end_inset

 implies the convergence of the iterates 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\boldsymbol{\theta}^{\left(r\right)}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 themselves
\begin_inset CommandInset citation
LatexCommand cite
key "wu1983convergence"

\end_inset

.
\end_layout

\begin_layout Theorem
(Monotonic EM Sequence) The sequence 
\begin_inset Formula $\left\{ \boldsymbol{\theta}^{\left(r\right)}\right\} $
\end_inset

 satisfies 
\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r+1\right)};\mathbf{y}\right)\geq l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)$
\end_inset

.
\end_layout

\begin_layout Proof
Start with
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\[
l\left(\boldsymbol{\theta};\mathbf{y}\right)=\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)\right)-\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)\right)
\]

\end_inset

Take conditional expectation with respect to 
\begin_inset Formula $\mathbf{X}|\mathbf{Y};\boldsymbol{\theta}^{\left(r\right)}$
\end_inset

 of both sides
\begin_inset Formula 
\begin{eqnarray*}
\int l\left(\boldsymbol{\theta};\mathbf{y}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x} & = & \int\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 &  & -\int\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}
\end{eqnarray*}

\end_inset

Since 
\begin_inset Formula $l\left(\boldsymbol{\theta};\mathbf{y}\right)$
\end_inset

 is independent of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
\int l\left(\boldsymbol{\theta};\mathbf{y}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x} & = & l\left(\boldsymbol{\theta};\mathbf{y}\right)\int f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 & = & l\left(\boldsymbol{\theta};\mathbf{y}\right)
\end{eqnarray*}

\end_inset

and so 
\begin_inset Formula 
\begin{eqnarray*}
l\left(\boldsymbol{\theta};\mathbf{y}\right) & = & \int\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{x},\mathbf{y};\boldsymbol{\theta}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 &  & -\int\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 & = & E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]-E\left[\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]
\end{eqnarray*}

\end_inset

Let 
\begin_inset Formula 
\[
Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)=E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]
\]

\end_inset

and 
\begin_inset Formula $K\left(\boldsymbol{\theta},\boldsymbol{\theta}^{\left(r\right)}\right)=E\left[\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{X}|\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

.
 Then
\begin_inset Formula 
\[
l\left(\boldsymbol{\theta};\mathbf{y}\right)=Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)-K\left(\boldsymbol{\theta},\boldsymbol{\theta}^{\left(r\right)}\right)
\]

\end_inset

If we can show that 
\begin_inset Formula 
\begin{eqnarray*}
Q\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right) & \leq & Q\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)\\
 & \text{and}\\
K\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right) & \geq & K\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)
\end{eqnarray*}

\end_inset

then it will follow that 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $l\left(\boldsymbol{\theta}^{\left(r+1\right)};\mathbf{y}\right)\geq l\left(\boldsymbol{\theta}^{\left(r\right)};\mathbf{y}\right)$
\end_inset

.
 Well by definition of 
\begin_inset Formula 
\[
\boldsymbol{\theta}^{\left(r+1\right)}=\arg\max_{\boldsymbol{\theta}}E\left[\log\left(f_{\mathbf{X},\mathbf{Y}}\left(\mathbf{X},\mathbf{y};\boldsymbol{\theta}\right)\right)|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right]=\arg\max_{\boldsymbol{\theta}}Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)
\]

\end_inset

so 
\begin_inset Formula 
\[
Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)\leq Q\left(\mathbf{\boldsymbol{\theta}},\arg\max_{\boldsymbol{\theta}}Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r\right)}\right)\right)=Q\left(\mathbf{\boldsymbol{\theta}},\boldsymbol{\theta}^{\left(r+1\right)}\right)
\]

\end_inset

To see that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $K\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right)\geq K\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)$
\end_inset

 inspect 
\begin_inset Formula 
\[
\int\log\left(\frac{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}
\]

\end_inset

On the one hand 
\begin_inset Formula 
\begin{eqnarray*}
\int\log\left(\frac{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x} & = & \int\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 &  & -\int\log\left(f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\\
 & = & K\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)-K\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right)
\end{eqnarray*}

\end_inset

on the other hand, by Jensen's
\begin_inset Foot
status open

\begin_layout Plain Layout
Pronounce Yen-sen you uncultured boor!
\end_layout

\end_inset

 inequality
\begin_inset Foot
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
For convex 
\begin_inset Formula $g\left(X\right)$
\end_inset

 it's the case that 
\begin_inset Formula $g\left(E\left(X\right)\right)\leq Eg\left(X\right)$
\end_inset

 and for concave 
\begin_inset Formula $g\left(X\right)$
\end_inset

 the inequality is reversed.
\end_layout

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\int\log\left(\frac{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x} & \leq & \log\left[\int\left(\frac{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)}{f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)}\right)f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r\right)}\right)d\mathbf{x}\right]\\
 & = & \log\left[\int f_{\mathbf{X}|\mathbf{Y}}\left(\mathbf{x}|\mathbf{y};\boldsymbol{\theta}^{\left(r+1\right)}\right)d\mathbf{x}\right]\\
 & = & 0
\end{eqnarray*}

\end_inset

Hence 
\begin_inset Formula 
\[
K\left(\boldsymbol{\theta}^{\left(r+1\right)},\boldsymbol{\theta}^{\left(r\right)}\right)-K\left(\boldsymbol{\theta}^{\left(r\right)},\boldsymbol{\theta}^{\left(r\right)}\right)\leq0
\]

\end_inset

which completes the proof.
\end_layout

\begin_layout Section
Applications
\end_layout

\begin_layout Subsection
Gaussian Mixture Models
\end_layout

\begin_layout Standard
EM works really well for mixture models, e.g.
 
\begin_inset Formula $\mathbf{Y}_{i}$
\end_inset

 is distributed iid such that 
\begin_inset Formula 
\[
f_{\mathbf{Y}_{1}}\left(\mathbf{y}_{1}\right)=\sum_{j=1}^{m}\alpha_{j}g_{j}\left(\mathbf{y}_{1};\boldsymbol{\theta}_{j}\right)
\]

\end_inset

where 
\begin_inset Formula $g_{j}$
\end_inset

 are (in general multivariate) densities with parameter vectors 
\begin_inset Formula $\boldsymbol{\theta}_{j}$
\end_inset

 (in general, distinct for distinct 
\begin_inset Formula $j$
\end_inset

) and 
\begin_inset Formula $\sum_{j}\alpha_{j}=1$
\end_inset

.
 If 
\begin_inset Formula $g_{j}\sim\text{n}\left(\mu_{j},\sigma_{j}^{2}\right)$
\end_inset

 then the mixture model is called a Gaussian mixture model (GMM).
 Naively if you wanted to find the MLEs you would maximize
\begin_inset Formula 
\[
L\left(\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)=\prod_{i=1}^{n}f_{\mathbf{Y}_{i}}\left(\mathbf{y}_{i};\boldsymbol{\theta}\right)=\prod_{i=1}^{n}\sum_{j=1}^{m}\alpha_{j}g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)
\]

\end_inset

Even taking the log won't help you because of the interior sum.
 EM to the rescue! Let 
\begin_inset Formula $\mathbf{X}=\left(X_{1},\dots,X_{n}\right)$
\end_inset

 be the mixture components, i.e.
 each 
\begin_inset Formula $X_{i}$
\end_inset

 is a categorical random variable that indicates which component of the
 mixture density produced the correspondent 
\begin_inset Formula $\mathbf{Y}_{i}$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
P\left(X_{i}=j\right) & = & \begin{cases}
\alpha_{0} & \text{ if }x_{i}=0\\
\alpha_{1} & \text{ if }x_{i}=1\\
\vdots\\
\alpha_{m} & \text{ if }x_{i}=m
\end{cases}
\end{eqnarray*}

\end_inset

Hence 
\begin_inset Formula 
\begin{eqnarray*}
P\left(X_{i}=j\right) & = & a_{j}
\end{eqnarray*}

\end_inset

And the 
\begin_inset Formula $\mathbf{Y}_{i}|X_{i}$
\end_inset

 takes the form 
\begin_inset Formula 
\[
f_{\mathbf{Y}_{i}|X_{i}}\left(\mathbf{y}_{i}|x_{i}=j;\boldsymbol{\theta}\right)=g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)
\]

\end_inset

The 
\begin_inset Formula $x_{i}$
\end_inset

 in 
\begin_inset Formula $g_{x_{i}}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{x_{i}}\right)$
\end_inset

 
\begin_inset Quotes eld
\end_inset

picks
\begin_inset Quotes erd
\end_inset

 which mixture component the 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

 is generated by.
 Then the log-likelihood becomes
\begin_inset Formula 
\begin{eqnarray*}
l\left(\left(x_{1},\dots,x_{n}\right),\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right) & = & \log\left[\prod_{i=1}^{n}f_{X_{i},\mathbf{Y}_{i}}\left(x_{i},\mathbf{y}_{i};\boldsymbol{\theta}\right)\right]\\
 & = & \log\left[\prod_{i=1}^{n}\left[f_{\mathbf{Y}_{i}|X_{i}}\left(\mathbf{y}_{i}|x_{i};\boldsymbol{\theta}\right)P\left(X_{i}=j\right)\right]\right]\\
 & = & \log\left[\prod_{i=1}^{n}\left[g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)\alpha_{j}\right]\right]\\
 & = & \sum_{i=1}^{n}\log\left(g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)\alpha_{j}\right)\\
 & = & \sum_{i=1}^{n}\left[\log\alpha_{j}+\log\left(g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)\right)\right]
\end{eqnarray*}

\end_inset

There are a lot of indices and subscripts and superscripts to keep track
 of: 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is all of the parameters of all of the mixture components, 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

 are observed samples (
\begin_inset Formula $n$
\end_inset

 of them), the 
\begin_inset Formula $X_{i}$
\end_inset

 are the unobserved data (the mixture components), the 
\begin_inset Formula $\boldsymbol{\theta}_{j}$
\end_inset

 are the parameters of the 
\begin_inset Formula $j$
\end_inset

th mixture density, and the 
\begin_inset Formula $\alpha_{j}$
\end_inset

 are the mixture coefficients (i.e.
 in what proportion the 
\begin_inset Formula $j$
\end_inset

th density contributes).
 Much as we did for Example 1 we need to compute 
\begin_inset Formula $f_{X_{i}|\mathbf{Y}_{i}}\left(x_{i}|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)=P\left(X_{i}=j|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)$
\end_inset

 in order to compute the conditional expectations 
\begin_inset Formula 
\begin{eqnarray*}
P\left(X_{i}=j|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right) & = & \frac{f_{X_{i},\mathbf{Y}_{i}}\left(j,\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)}{f_{\mathbf{Y}_{i}}\left(\mathbf{y}_{i};\theta\right)}\\
 & = & \frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}
\end{eqnarray*}

\end_inset

So again 
\begin_inset Formula $X_{i}|\mathbf{Y}_{i}$
\end_inset

 is categorical random variable.
 By i.i.d 
\begin_inset Formula 
\[
P\left(\left(X_{1}=j_{1},\dots,X_{n}=j_{n}\right)|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right)=\prod_{i=1}^{n}\frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j_{i}}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}
\]

\end_inset

Now we just need to take the expectation of the log-likelihood against this
 conditional density
\begin_inset Formula 
\begin{eqnarray*}
E\left[l\left(\left(X_{1},\dots,X_{n}\right),\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]=\\
\sum_{j_{1}=1}^{m}\sum_{j_{2}=1}^{m}\cdots\sum_{j_{n}=1}^{m}\left(\left[\sum_{k=1}^{n}\left\{ \log\alpha_{j_{k}}+\log\left[g_{j_{k}}\left(\mathbf{y}_{k};\boldsymbol{\theta}_{j_{k}}\right)\right]\right\} \right]\prod_{i=1}^{n}\frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}\right)
\end{eqnarray*}

\end_inset

Pretty ugly right? Suffice it to say you're not actually taking this expectation.
 So let 
\begin_inset Formula 
\[
\gamma_{ij}^{\left(r\right)}\coloneqq P\left(X_{i}=j|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)=\frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}
\]

\end_inset

and a helpful lemma:
\end_layout

\begin_layout Lemma
For i.i.d incomplete samples 
\begin_inset Formula $Y_{i}$
\end_inset

, with completions 
\begin_inset Formula $X_{i}$
\end_inset


\begin_inset Formula 
\[
E\left[l\left(\left(X_{1},\dots,X_{n}\right),\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]=\sum_{i=1}^{n}E\left[l\left(X_{i}|\mathbf{y}_{i};\boldsymbol{\theta}\right)|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right]
\]

\end_inset

where 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

 is the ith sample.
\end_layout

\begin_deeper
\begin_layout Proof
By i.i.d
\begin_inset Formula 
\begin{align*}
E\left[l\left(\left(X_{1},\dots,X_{n}\right),\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
E\left[\log\left[\prod_{i=1}^{n}f\left(X_{i}|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)\right]|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
E\left[\sum_{i=1}^{n}\log\left[f\left(X_{i}|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)\right]|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\left(\text{ since }X_{i}\perp X_{j}\text{ for }i\neq j\right)\\
\sum_{i=1}^{n}E\left[\log\left[f\left(X_{i}|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}\right)\right]|\left(\mathbf{y}_{1},\dots,\mathbf{y}_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\left(\text{ since }P\left(X_{i}=x_{i}|\left(\mathbf{Y}_{1},\dots,\mathbf{Y}_{n}\right)\right)=P\left(X_{i}=x_{i}|\mathbf{Y}_{i}\right)\right)\\
\sum_{i=1}^{n}E\left[\log\left[f\left(X_{i}|\mathbf{y}_{i};\boldsymbol{\theta}\right)\right]|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
\sum_{i=1}^{n}E\left[l\left(X_{i}|\mathbf{y}_{i};\boldsymbol{\theta}\right)|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right]
\end{align*}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Therefore we only need to compute 
\begin_inset Formula $E\left[l\left(X_{i}|\mathbf{y}_{i};\boldsymbol{\theta}\right)|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

 to perform the E-step.
 Let 
\begin_inset Formula $j_{i}\rightarrow j$
\end_inset

 for convenience and 
\begin_inset Formula $\left(\mu_{j},\Sigma_{j}\right)$
\end_inset

 be the parameters for the 
\begin_inset Formula $j$
\end_inset

th Gaussian.
 Actually let's take the 1 dimensional Gaussian case
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
key "chen2010demystified"

\end_inset

 is a good place for the general case.
\end_layout

\end_inset

 so 
\begin_inset Formula $\left(\mu_{j},\Sigma_{j}\right)=\left(\mu_{j},\sigma_{j}^{2}\right)$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}_{i}\rightarrow y_{i}$
\end_inset

.
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\begin{eqnarray*}
E\left[l\left(X_{i}|y_{i};\boldsymbol{\theta}\right)|y_{i};\boldsymbol{\theta}^{\left(r\right)}\right] & = & \sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\log\left(g_{j}\left(y_{i};\boldsymbol{\theta}_{j}\right)\right)\right]\gamma_{ij}^{\left(r\right)}\right)\\
 & = & \sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\left(-\frac{1}{2}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2\sigma_{j}^{2}}\left(y_{i}-\mu_{j}\right)^{2}\right)\right]\gamma_{ij}^{\left(r\right)}\right)
\end{eqnarray*}

\end_inset

Then 
\begin_inset Formula 
\begin{eqnarray*}
E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
\sum_{i=1}^{n}\sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\log\left(g_{j}\left(y_{i};\boldsymbol{\theta}_{j}\right)\right)\right]\gamma_{ij}^{\left(r\right)}\right) & =\\
\sum_{i=1}^{n}\sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\left(-\frac{1}{2}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2\sigma_{j}^{2}}\left(y_{i}-\mu_{j}\right)^{2}\right)\right]\gamma_{ij}^{\left(r\right)}\right)
\end{eqnarray*}

\end_inset

Define 
\begin_inset Formula 
\[
n_{j}^{\left(r\right)}=\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}=\sum_{i=1}^{n}P\left(X_{i}=j|\mathbf{y}_{i};\boldsymbol{\theta}^{\left(r\right)}\right)
\]

\end_inset

This is something like the portion of the samples that were generated by
 the 
\begin_inset Formula $j$
\end_inset

th component of the density
\begin_inset Foot
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Why? Because 
\begin_inset Formula $\sum_{j=1}^{m}n_{j}=\sum_{j=1}^{m}\sum_{i=1}^{n}\gamma_{ij}=\sum_{i=1}^{n}1=n$
\end_inset

.
\end_layout

\end_inset

.
 The expectation becomes 
\begin_inset Formula 
\begin{eqnarray*}
E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right] & =\\
\sum_{i=1}^{n}\sum_{j=1}^{m}\left(\left[\log\alpha_{j}+\left(-\frac{1}{2}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2\sigma_{j}^{2}}\left(y_{i}-\mu_{j}\right)^{2}\right)\right]\gamma_{ij}^{\left(r\right)}\right) & =\\
\sum_{i=1}^{n}\sum_{j=1}^{m}\log\alpha_{j}\gamma_{ij}^{\left(r\right)}-\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{1}{2}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)\gamma_{ij}^{\left(r\right)}-\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{1}{2\sigma_{j}^{2}}\left(y_{i}-\mu_{j}\right)^{2}\gamma_{ij}^{\left(r\right)} & =\\
\sum_{j=1}^{m}n_{j}^{\left(r\right)}\log\alpha_{j}-\frac{1}{2}\sum_{j=1}^{m}n_{j}^{\left(r\right)}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{\left(y_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{2}}\gamma_{ij}^{\left(r\right)}
\end{eqnarray*}

\end_inset

This is the expression that needs to be maximized with respect to 
\begin_inset Formula $\alpha_{j},\mu_{j},\sigma_{j}^{2}$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Pay attention to the difference between the arguments that come from the
 log 
\begin_inset Formula $\alpha_{j},\mu_{j},\sigma_{j}^{2}$
\end_inset

 and the current estimates of the parameters, those with iterate superscripts
 
\begin_inset Formula $\left(r\right)$
\end_inset

 that come from conditional density, with respect to which the expectation
 was taken.
\end_layout

\end_inset

.
 The fully specified maximization problem is 
\begin_inset Formula 
\begin{align*}
\max_{\alpha_{j},\mu_{j},\sigma_{j}^{2}} & E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]\\
 & \iff\\
\max_{\alpha_{j},\mu_{j},\sigma_{j}^{2}} & \sum_{j=1}^{m}n_{j}^{\left(r\right)}\log\alpha_{j}-\frac{1}{2}\sum_{j=1}^{m}n_{j}^{\left(r\right)}\left(\log\left(\sigma_{j}^{2}\right)+\log\left(2\pi\right)\right)-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{\left(y_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{2}}\gamma_{ij}^{\left(r\right)}\\
\text{subject to} & \sum_{j=1}^{m}\alpha_{j}=1,\:\alpha_{j}\geq0
\end{align*}

\end_inset

This looks tough but because it's a bunch of uncoupled (somewhat) terms
 summed together so it's easier than the original log-likelihood 
\begin_inset Formula 
\[
\log\left[\sum_{j=1}^{m}\alpha_{j}g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}\right)\right]
\]

\end_inset

The local maxima for 
\begin_inset Formula $\alpha_{j},\mu_{j},\sigma_{j}^{2}$
\end_inset

 become the new iterates, ie.
 
\begin_inset Formula $\alpha_{j}^{\left(r+1\right)},\mu_{j}^{\left(r+1\right)},\left(\sigma_{j}^{2}\right)^{\left(r+1\right)}$
\end_inset

.
 To perform the constrained maximization (only the 
\begin_inset Formula $\alpha_{j}$
\end_inset

) we use Lagrange multipliers; form the Lagrangian
\begin_inset Formula 
\[
\mathcal{L}\left(\boldsymbol{\alpha},\lambda\right)=\sum_{j=1}^{m}n_{j}^{\left(r\right)}\log\alpha_{j}+\lambda\left(\sum_{j=1}^{m}\alpha_{j}-1\right)
\]

\end_inset

Then computing derivatives and setting to zero to find 
\begin_inset Formula $\alpha_{j}$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\mathcal{L}}{\partial\alpha_{j}} & = & \frac{n_{j}^{\left(r\right)}}{\alpha_{j}}+\lambda
\end{eqnarray*}

\end_inset

Therefore the critical
\begin_inset Foot
status open

\begin_layout Plain Layout
You need to do the convex analysis (second-deriative test) to determine
 whether 
\begin_inset Formula $\alpha_{j}$
\end_inset

 are maxima but they are and I'm not going to.
\end_layout

\end_inset

 
\begin_inset Formula $\alpha_{j}$
\end_inset

 satisfy 
\begin_inset Formula 
\[
\alpha_{j}=-\frac{n_{j}^{\left(r\right)}}{\lambda}
\]

\end_inset

To eliminate 
\begin_inset Formula $\lambda$
\end_inset

 use the equality constraint, i.e.
 
\begin_inset Formula $\sum_{j=1}^{m}\alpha_{j}=1$
\end_inset

 implies 
\begin_inset Formula $\lambda=-\sum_{j=1}^{m}n_{j}^{\left(r\right)}$
\end_inset

 and hence
\begin_inset Formula 
\[
\alpha_{j}^{\left(r+1\right)}=\frac{n_{j}^{\left(r\right)}}{\sum_{j=1}^{m}n_{j}^{\left(r\right)}}=\frac{n_{j}^{\left(r\right)}}{n}
\]

\end_inset

So 
\begin_inset Formula $\alpha_{j}^{\left(r+1\right)}$
\end_inset

 is just the current best estimate for how many of the samples were generated
 by the 
\begin_inset Formula $j$
\end_inset

th component of the mixture density
\begin_inset Foot
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Pretty much the epitome of the MLE for a parameter.
\end_layout

\end_inset

.
 To compute the updates 
\begin_inset Formula $\mu_{j}^{\left(r+1\right)}$
\end_inset

 we just need to do the standard maximization of 
\begin_inset Formula $E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]$
\end_inset

 since the 
\begin_inset Formula $\mu_{j}$
\end_inset

 are unconstrained.
 Therefore
\begin_inset Formula 
\[
\frac{\partial}{\partial\mu_{j}}E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]=\sum_{i=1}^{n}\frac{\left(y_{i}-\mu_{j}\right)}{\sigma_{j}^{2}}\gamma_{ij}^{\left(r\right)}=0
\]

\end_inset

Note the sum of 
\begin_inset Formula $j$
\end_inset

 disappears because only the 
\begin_inset Formula $j$
\end_inset

th term is non-zero.
 Then simplifying further
\begin_inset Formula 
\begin{eqnarray*}
\sum_{i=1}^{n}\frac{\left(y_{i}-\mu_{j}\right)}{\sigma_{j}^{2}}\gamma_{ij}^{\left(r\right)} & = & \frac{1}{\sigma_{j}^{2}}\left(\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}y_{i}-\mu_{j}\sum_{i=1}\gamma_{ij}^{\left(r\right)}\right)\\
 & = & \frac{1}{\sigma_{j}^{2}}\left(\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}y_{i}-\mu_{j}n_{j}^{\left(r\right)}\right)=0\\
 & \Rightarrow\\
\mu_{j}^{\left(r+1\right)} & = & \frac{1}{n_{j}^{\left(r\right)}}\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}y_{i}
\end{eqnarray*}

\end_inset

Finally doing the same kind of thing for 
\begin_inset Formula $\left(\sigma_{j}^{2}\right)^{\left(r+1\right)}$
\end_inset


\begin_inset Formula 
\[
\frac{\partial}{\partial\sigma_{j}^{2}}E\left[l\left(\left(X_{1},\dots,X_{n}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}\right)|\left(y_{1},\dots,y_{n}\right);\boldsymbol{\theta}^{\left(r\right)}\right]=-\frac{1}{2}\frac{n_{j}^{\left(r\right)}}{\sigma_{j}^{2}}+\frac{1}{2}\sum_{i=1}^{n}\frac{\left(y_{i}-\mu_{j}^{\left(r+1\right)}\right)^{2}}{\left(\sigma_{j}^{2}\right)^{2}}\gamma_{ij}^{\left(r\right)}=0
\]

\end_inset

where 
\begin_inset Formula $\mu_{j}$
\end_inset

 now gets a superscript because it's already been updated.
 Hence 
\begin_inset Formula 
\[
\left(\sigma_{j}^{2}\right)^{\left(r+1\right)}=\frac{1}{n_{j}^{\left(r+1\right)}}\sum_{i=1}^{n}\left(y_{i}-\mu_{j}^{\left(r+1\right)}\right)^{2}\gamma_{ij}^{\left(r\right)}
\]

\end_inset

In summary the update equations for a univariate GMM 
\begin_inset Formula 
\begin{align*}
\alpha_{j}^{\left(r+1\right)} & =\frac{n_{j}^{\left(r\right)}}{n}\\
\mu_{j}^{\left(r+1\right)} & =\frac{1}{n_{j}^{\left(r\right)}}\sum_{i=1}^{n}\gamma_{ij}^{\left(r\right)}y_{i}\\
\left(\sigma_{j}^{2}\right)^{\left(r+1\right)} & =\frac{1}{n_{j}^{\left(r+1\right)}}\sum_{i=1}^{n}\left(y_{i}-\mu_{j}^{\left(r+1\right)}\right)^{2}\gamma_{ij}^{\left(r\right)}
\end{align*}

\end_inset

where 
\begin_inset Formula 
\begin{align*}
\gamma_{ij}^{\left(r\right)} & =\frac{g_{j}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{j}^{\left(r\right)}\right)\alpha_{j}^{\left(r\right)}}{\sum_{k=1}^{m}\left(g_{k}\left(\mathbf{y}_{i};\boldsymbol{\theta}_{k}^{\left(r\right)}\right)\alpha_{k}^{\left(r\right)}\right)}\\
n_{j}^{\left(r\right)} & =\sum_{i}\gamma_{ij}^{\left(r\right)}
\end{align*}

\end_inset

For multivariate GMM the only difference is 
\begin_inset Formula $\sigma_{j}^{2}\rightarrow\Sigma_{j}$
\end_inset

 and so you need to use matrix derivatives.
 Consult 
\begin_inset CommandInset citation
LatexCommand cite
key "chen2010demystified"

\end_inset

 for the full derivation 
\begin_inset Formula 
\[
\Sigma_{j}^{\left(r+1\right)}=\frac{1}{n_{j}^{\left(r+1\right)}}\sum_{i=1}^{n}\left(\mathbf{y}_{i}-\boldsymbol{\mu}_{j}^{\left(r+1\right)}\right)\left(\mathbf{y}_{i}-\boldsymbol{\mu}_{j}^{\left(r+1\right)}\right)^{T}\gamma_{ij}^{\left(r\right)}
\]

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "gentle"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
