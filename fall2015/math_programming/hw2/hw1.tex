%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside]{amsart}
\usepackage[latin9]{inputenc}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsbsy}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{stmaryrd}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
  \theoremstyle{plain}
  \newtheorem*{cor*}{\protect\corollaryname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

%
\usepackage{amsfonts}
%\usepackage{mathabx}
\usepackage{nopageno}%%%  The following few lines affect the margin sizes. 
\usepackage{bm}
\addtolength{\topmargin}{-.5in}
\setlength{\textwidth}{6in}       
\setlength{\oddsidemargin}{.25in}              
\setlength{\evensidemargin}{.25in}         
  
\setlength{\textheight}{9in}
\renewcommand{\baselinestretch}{1}
\reversemarginpar   
%
%

\makeatother

  \providecommand{\corollaryname}{Corollary}

\begin{document}

\title{ESI 6420 Homework 2 Solutions}


\author{Maksim Levental}


\date{\today}

\maketitle
Time spent: too much but time well spent is never wasted.
\begin{enumerate}
\item [2.1]Claim: For $A\in\mathbb{R}^{m\times n}$, $B\in\mathbb{R}^{k\times n}$
, if for all $x$ it's the case that $Ax=0$ implies $Bx=0$ then
$B=CA$ for some $C\in\mathbb{R}^{k\times m}$.

\begin{proof}
Assume without loss of generality that and $k\leq m$. Suppose $Ax=0\Rightarrow Bx=0$.
This is equivalent to $\text{null}\left(A\right)\subset\text{null}\left(B\right)$.
By the fundamental theorem of Linear Algebra and the fact that $\text{row}\left(A\right)\subset\mathbb{R}^{n}$
and $\text{row}\left(B\right)\subset\mathbb{R}^{n}$ we have that
$\text{null}\left(A\right)\cup\text{row}\left(A\right)=\text{null}\left(B\right)\cup\text{row}\left(B\right)=\mathbb{R}^{n}$
and hence 
\begin{align*}
\text{null}\left(A\right) & \subset\text{null}\left(B\right)\\
 & \iff\\
\overline{\text{null}\left(A\right)} & \subset\overline{\text{null}\left(B\right)}\\
 & \iff\\
\text{row}\left(B\right) & \subset\text{row}\left(A\right)
\end{align*}
Therefore for all $i=1,\dots,k$ there exist $c_{i}\in\mathbb{R}^{m}$
such that
\[
A^{T}c_{i}=b_{i}^{T}
\]
where $b_{i}^{T}=\left(B^{T}\right)_{i}$ the $i$th row of $B$.
Collecting all of the $c_{i}$ into a matrix $C^{T}$ we get that
\[
A^{T}C^{T}=B^{T}
\]
or 
\[
CA=B
\]

\end{proof}
\item [2.2]Claim: For $\left\{ v_{1},\dots,v_{m}\right\} \subset\mathbb{R}^{n}$
and $m\geq n+2$ there exist $\boldsymbol{\alpha}=\left(\alpha_{1},\dots,\alpha_{m}\right)\neq\mathbf{0}$
such that 
\[
\sum_{i=1}^{m}\alpha_{i}v_{i}=0
\]
and
\[
\sum_{i=1}^{m}\alpha_{i}=0
\]


\begin{proof}
Assume $m\geq n+2$. Let 
\[
w_{i}=\begin{pmatrix}v_{i}\\
1
\end{pmatrix}
\]
Note that $w_{i}\in\mathbb{R}^{\left(n+1\right)}$. Since $m\geq n+2$
it's the case that $m>n+1$ and hence $\left\{ w_{1},\dots,w_{m}\right\} $
is linearly dependent (since the maximal number of linearly independent
vectors in $\mathbb{R}^{\left(n+1\right)}$ is $n+1$). Therefore,
with $W=\left(w_{1},w_{2},\dots,w_{m}\right)$ it must be case that
there exists $\boldsymbol{\alpha}\in\mathbb{R}^{m}$ such that 
\begin{align*}
W\boldsymbol{\alpha} & =\begin{pmatrix}v_{1} & \cdots & v_{m}\\
1 & \cdots & 1
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{m}
\end{pmatrix}\\
 & =\begin{pmatrix}\sum_{i=1}^{m}\alpha_{i}v_{i}\\
\sum_{i=1}^{m}\alpha_{i}
\end{pmatrix}\\
 & =\mathbf{0}
\end{align*}

\end{proof}
\item [2.3]Claim: For $A:U\rightarrow V$ and $B:V\rightarrow W$ linear
maps
\[
\dim\left(R\left[A\right]\cap\ker\left[B\right]\right)=\dim\left(R\left[A\right]\right)-\dim\left(R\left[BA\right]\right)=\dim\left(\ker\left[BA\right]\right)-\dim\left(\ker\left[A\right]\right)
\]
\begin{align*}
\dim\left(R\left[BA\right]\right) & =\dim\left(R\left[A\right]\right)-\dim\left(R\left[A\right]\cap\ker\left[B\right]\right)\\
\dim\left(R\left[A\right]\right)+\dim\left(\ker\left[A\right]\right) & =\dim\left(\ker\left[BA\right]\right)+\dim\left(R\left[BA\right]\right)
\end{align*}
where $R\left[X\right]$ is the column-space of $X$.

\begin{proof}
Let $u,v,w$ be the dimensions of $U,V,W$ respectively. Since $BA:U\rightarrow W$
the rank-nullity theorem states that 
\[
\text{rank}\left(A\right)+\text{nullity}\left(A\right)=u=\text{rank}\left(BA\right)+\text{nullity}\left(BA\right)
\]
and since $\text{rank}\left(X\right)=\dim\left(R\left[X\right]\right)$
and $\text{nullity}\left(X\right)=\dim\left(\ker\left[X\right]\right)$
it's the case that 
\[
\dim\left(R\left[A\right]\right)+\dim\left(\ker\left[A\right]\right)=\dim\left(\ker\left[BA\right]\right)+\dim\left(R\left[BA\right]\right)
\]
or
\[
\dim\left(R\left[A\right]\right)-\dim\left(R\left[BA\right]\right)=\dim\left(\ker\left[BA\right]\right)-\dim\left(\ker\left[A\right]\right)
\]
Now let $\left\{ u_{1},\dots,u_{k}\right\} $ be a basis for $R\left[A\right]\cap\ker\left[B\right]$.
The set $\left\{ u_{1},\dots,u_{k}\right\} $ is linearly independent
in $R\left[A\right]$ and hence can be extended to a basis: let $\left\{ u_{1},\dots,u_{k},v_{1},\dots,v_{n}\right\} $
be a basis for $R\left[A\right]$. Finally let $\left\{ y_{1},\dots,y_{m}\right\} $
be a basis for $R\left[BA\right]$ and pick an arbitrary $y_{i}$.
By definition there exists $x_{i}$ such that 
\[
BAx_{i}=y_{i}
\]
But $Ax_{i}$ is in $R\left[A\right]$ and therefore there exist $\left\{ \alpha_{1},\dots,\alpha_{k},\beta_{1},\dots,\beta_{n}\right\} $
such that 
\[
Ax_{i}=\sum_{i=1}^{k}\alpha_{i}u_{i}+\sum_{i=1}^{n}\beta_{i}v_{i}
\]
But 
\[
y_{i}=B\left(Ax_{i}\right)=B\left(\sum_{i=1}^{k}\alpha_{i}u_{i}+\sum_{i=1}^{n}\beta_{i}v_{i}\right)=\sum_{i=1}^{k}\alpha_{i}Bu_{i}+B\left(\sum_{i=1}^{n}\beta_{i}v_{i}\right)
\]
and by $\left\{ u_{i}\right\} $ being a basis for $\ker\left[B\right]$
\[
y_{i}=\cancel{\sum_{i=1}^{k}\alpha_{i}Bu_{i}}+B\left(\sum_{i=1}^{n}\beta_{i}v_{i}\right)=\sum_{i=1}^{n}\beta_{i}Bv_{i}=\sum_{i=1}^{n}\beta_{i}w_{i}
\]
where $w_{i}\coloneqq Bv_{i}$. Therefore 
\[
R\left[BA\right]=\text{span}\left(\left\{ y_{1},\dots,y_{m}\right\} \right)=\text{span}\left(\left\{ w_{1},\dots,w_{n}\right\} \right)
\]
but note that $\left\{ w_{1},\dots,w_{n}\right\} $ is linearly independent:
if there existed $\left\{ \gamma_{1},\dots,\gamma_{n}\right\} $ such
that $\sum_{i=1}^{n}\gamma_{i}w_{i}=0$ then 
\[
\sum_{i=1}^{n}\gamma_{i}w_{i}=\sum_{i=1}^{n}\gamma_{i}Bv_{i}=B\left(\sum_{i=1}^{n}\gamma_{i}v_{i}\right)=0
\]
and $\left\{ v_{1},\dots,v_{n}\right\} $ were defined to be linearly
independent of $\ker\left[B\right]$ (and hence no vector in their
span could be in $\ker\left[B\right]$). Hence (finally) 
\begin{align*}
\dim\left(R\left[BA\right]\right) & =\dim\left(\text{span}\left(\left\{ y_{1},\dots,y_{m}\right\} \right)\right)\\
 & =\dim\left(\text{span}\left(\left\{ w_{1},\dots,w_{n}\right\} \right)\right)\\
 & =\dim\left(\left\{ u_{1},\dots,u_{k},v_{1},\dots,v_{n}\right\} \backslash\left\{ u_{1},\dots,u_{k}\right\} \right)
\end{align*}
and $\left\{ u_{1},\dots,u_{k}\right\} $ and $\left\{ v_{1},\dots,v_{n}\right\} $
are linearly independent 
\begin{align*}
\dim\left(R\left[BA\right]\right) & =\dim\left(\left\{ u_{1},\dots,u_{k},v_{1},\dots,v_{n}\right\} \backslash\left\{ u_{1},\dots,u_{k}\right\} \right)\\
 & =\dim\left(\left\{ u_{1},\dots,u_{k},v_{1},\dots,v_{n}\right\} \right)-\dim\left(\left\{ u_{1},\dots,u_{k}\right\} \right)\\
 & =\dim\left(R\left[A\right]\right)-\dim\left(R\left[A\right]\cap\ker\left[B\right]\right)
\end{align*}

\begin{cor*}
For $D$ a linear operator
\[
\dim\left(\ker\left[D^{n+1}\right]\right)=\dim\left(\ker\left[D\right]\right)+\sum_{j=1}^{n}\dim\left(R\left[D^{k}\right]\cap\ker\left[D\right]\right)
\]
\end{cor*}
\begin{proof}
By induction on $n$. Let $n=1$ and by the (manipulated above) result
with $A=B=D$

\[
\dim\left(R\left[D\right]\cap\ker\left[D\right]\right)+\dim\left(\ker\left[D\right]\right)=\dim\left(\ker\left[DD\right]\right)
\]
or 
\[
\dim\left(\ker\left[D^{1+1}\right]\right)=\dim\left(\ker\left[D\right]\right)+\sum_{j=1}^{1}\dim\left(R\left[D^{k}\right]\cap\ker\left[D\right]\right)
\]
Assume $n>1$. Then by applying the above result again with $B=D^{n}$
and $A=D$, and applying the induction hypothesis 
\begin{align*}
\dim\left(\ker\left[D^{n+1}\right]\right) & =\dim\left(\ker\left[D^{1}D^{n}\right]\right)\\
 & =\dim\left(R\left[D^{n}\right]\cap\ker\left[D\right]\right)+\dim\left(\ker\left[D\right]\right)\\
 & =\dim\left(R\left[D^{n}\right]\cap\ker\left[D\right]\right)+\dim\left(\ker\left[D\right]\right)\\
 & +\sum_{j=1}^{n-1}\dim\left(R\left[D^{k}\right]\cap\ker\left[D\right]\right)\\
 & =\dim\left(\ker\left[D\right]\right)+\sum_{j=1}^{n}\dim\left(R\left[D^{k}\right]\cap\ker\left[D\right]\right)
\end{align*}

\end{proof}
\end{proof}
\item [2.5]Claim: For $V$ and $n$-dimensional real vector space and $U,W$
$m$-dimensional subspaces of $V$, if $u\perp W$ for all $u\in U\backslash\left\{ 0\right\} $
then there exists $w\in W\backslash\left\{ 0\right\} $ such that
$w\perp U\backslash\left\{ 0\right\} $.

\begin{proof}
Fix bases $\left\{ u_{1},\dots,u_{m}\right\} ,\left\{ w_{1},\dots,w_{m}\right\} $
for for $U,W$. The hypothesis $u\perp W$ for all $u\in U\backslash\left\{ 0\right\} $
implies that $u_{j}\cdot w_{i}=0$ for all $i,j$. Let 
\[
w=\sum_{i=1}^{m}w_{i}
\]
Then $w\perp U\backslash\left\{ 0\right\} $ since for arbitrary $u\in U\backslash\left\{ 0\right\} $
\[
u=\sum_{i=1}^{m}\alpha_{i}u_{i}
\]
and 
\[
w\cdot u=\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}u_{j}\cdot w_{i}=0
\]

\end{proof}
\item [3.1]Claim: $\left\langle \left\langle I\right\rangle \right\rangle \geq1$
for all matrix norms $\left\langle \left\langle \cdot\right\rangle \right\rangle $. 

\begin{proof}
Since $II=I$ we have that 
\[
\left\langle \left\langle I\right\rangle \right\rangle =\left\langle \left\langle II\right\rangle \right\rangle \leq\left\langle \left\langle I\right\rangle \right\rangle \left\langle \left\langle I\right\rangle \right\rangle 
\]
cancelling $\left\langle \left\langle I\right\rangle \right\rangle $
from both sides of the inequality we have that 
\[
1\leq\left\langle \left\langle I\right\rangle \right\rangle 
\]

\end{proof}
\item [3.2]Claim: The Frobenius norm 
\[
\left\langle \left\langle A\right\rangle \right\rangle _{F}=\sqrt{\text{tr}\left(A^{T}A\right)}=\sqrt{\sum_{i}\sum_{j}\left|a_{ij}\right|^{2}}
\]
is a matrix norm.

\begin{proof}
It is positively homogenous, subadditive, and positive definite because
it is equivalent to the Euclidean norm on $\mathbb{R}^{n^{2}}$. Further
it satisfies ``submultiplicativity'': first note that for $A,B$
\[
\left(AB\right)_{ij}=\sum_{k}a_{ik}b_{kj}
\]
Therefore 
\begin{align*}
\left\langle \left\langle AB\right\rangle \right\rangle _{F} & =\sqrt{\sum_{i}\sum_{j}\left(\left(AB\right)_{ij}\right)^{2}}\\
 & =\sqrt{\sum_{i}\sum_{j}\left(\left|\sum_{k}a_{ik}b_{kj}\right|\right)^{2}}
\end{align*}
now by Cauchy-Schwarz
\begin{align*}
\left\langle \left\langle AB\right\rangle \right\rangle _{F} & =\sqrt{\sum_{i}\sum_{j}\left(\left(AB\right)_{ij}\right)^{2}}\\
 & =\sqrt{\sum_{i}\sum_{j}\left(\left|\sum_{k}a_{ik}b_{kj}\right|\right)^{2}}\\
 & \leq\sqrt{\sum_{i}\sum_{j}\left(\sum_{k}\left|a_{ik}\right|^{2}\sum_{k}\left|b_{kj}\right|^{2}\right)}\\
 & =\sqrt{\left(\sum_{i}\sum_{k}\left|a_{ik}\right|^{2}\right)\left(\sum_{j}\sum_{k}\left|b_{kj}\right|^{2}\right)}\\
 & =\left\langle \left\langle A\right\rangle \right\rangle _{F}\left\langle \left\langle B\right\rangle \right\rangle _{F}
\end{align*}

\end{proof}
\item [3.3]Let the induced matrix $p$-norm be
\[
\left\langle \left\langle A\right\rangle \right\rangle _{p}=\sup_{x\in\mathbb{R}^{n}\backslash\left\{ 0\right\} }\frac{\left\Vert Ax\right\Vert _{p}}{\left\Vert x\right\Vert _{p}}
\]


\begin{enumerate}
\item Claim: The induced matrix $p$-norm is equivalent to
\[
\left\langle \left\langle A\right\rangle \right\rangle _{p}=\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert Ax\right\Vert _{p}
\]


\begin{proof}
Let $y$ be such that 
\[
y=\text{argmax}\left(\frac{\left\Vert Ax\right\Vert _{p}}{\left\Vert x\right\Vert _{p}}\right)
\]
Then 
\[
\left\langle \left\langle A\right\rangle \right\rangle _{p}=\frac{\left\Vert Ay\right\Vert _{p}}{\left\Vert y\right\Vert _{p}}
\]
Let $z\in\mathbb{R}^{n},c\in\mathbb{R}$ be such that $cz=y$ and
$\left\Vert z\right\Vert _{p}=1$. Then 
\begin{align*}
\frac{\left\Vert Ay\right\Vert _{p}}{\left\Vert y\right\Vert _{p}} & =\frac{\left\Vert A\left(cz\right)\right\Vert _{p}}{\left\Vert cz\right\Vert _{p}}
\end{align*}
Then by linearity of matrix multiplication and the $p$-norm
\begin{align*}
\frac{\left\Vert c\right\Vert _{p}\left\Vert Az\right\Vert _{p}}{\left\Vert c\right\Vert _{p}\left\Vert z\right\Vert _{p}} & =\frac{\left\Vert Az\right\Vert _{p}}{\left\Vert z\right\Vert _{p}}\\
 & =\left\Vert Az\right\Vert _{p}
\end{align*}
Now suppose that there were some $z'$ such that $\left\Vert z'\right\Vert _{p}=1$
and $\left\Vert Az'\right\Vert _{p}>\left\Vert Az\right\Vert _{p}$.
Then 
\begin{align*}
\left\Vert Az'\right\Vert _{p}>\left\Vert Az\right\Vert _{p} & =\frac{\left\Vert Az\right\Vert _{p}}{\left\Vert z\right\Vert _{p}}=\frac{\left\Vert A\frac{y}{c}\right\Vert _{p}}{\left\Vert \frac{y}{c}\right\Vert _{p}}=\frac{\left\Vert \frac{1}{c}\right\Vert _{p}\left\Vert Ay\right\Vert _{p}}{\left\Vert \frac{1}{c}\right\Vert _{p}\left\Vert y\right\Vert _{p}}=\frac{\left\Vert Ay\right\Vert _{p}}{\left\Vert y\right\Vert _{p}}
\end{align*}
which is a contradiction (since $y$ was defined to the argmax of
$\left\Vert Ax\right\Vert _{p}/\left\Vert x\right\Vert _{p}$). Hence
it sufficies to define 
\[
\left\langle \left\langle A\right\rangle \right\rangle _{p}=\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert Ax\right\Vert _{p}
\]

\end{proof}
\item Claim: the induced matrix $p$-norm is a legitimate matrix norm.

\begin{proof}
Using the equivalent definition from part (a) we prove
\begin{enumerate}
\item Positive homogenous: let $c\in\mathbb{R}$ and then by the homogeneity
of the $p$-norm 
\begin{align*}
\left\langle \left\langle cA\right\rangle \right\rangle _{p} & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert \left(cA\right)x\right\Vert _{p}\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left|c\right|\left\Vert Ax\right\Vert _{p}\\
 & =\left|c\right|\left(\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert Ax\right\Vert _{p}\right)\\
 & =\left|c\right|\left\langle \left\langle A\right\rangle \right\rangle _{p}
\end{align*}

\item Subadditivity: let $A,B\in M_{n}\left(\mathbb{R}\right)$ and then
by the subadditivity of the $p$-norm
\begin{align*}
\left\langle \left\langle A+B\right\rangle \right\rangle _{p} & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert \left(A+B\right)x\right\Vert _{p}\\
 & \leq\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left(\left\Vert Ax\right\Vert _{p}+\left\Vert Bx\right\Vert _{p}\right)\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert Ax\right\Vert _{p}+\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert Bx\right\Vert _{p}\\
 & =\left\langle \left\langle A\right\rangle \right\rangle _{p}+\left\langle \left\langle B\right\rangle \right\rangle _{p}
\end{align*}

\item Positive-definiteness: let $A\neq0$ and then firstly note that
\[
\left\langle \left\langle -A\right\rangle \right\rangle _{p}=\left|-1\right|\left\langle \left\langle A\right\rangle \right\rangle _{p}=\left\langle \left\langle A\right\rangle \right\rangle _{p}
\]
 and by the subadditivity of the $p$-norm 
\begin{align*}
0 & =\left\langle \left\langle A+\left(-A\right)\right\rangle \right\rangle _{p}\\
 & \leq\left\langle \left\langle A\right\rangle \right\rangle _{p}+\left\langle \left\langle -A\right\rangle \right\rangle _{p}\\
 & =2\left\langle \left\langle A\right\rangle \right\rangle _{p}
\end{align*}
hence $\left\langle \left\langle A\right\rangle \right\rangle _{p}\geq0$.
\item Submultiplicativity: first note that for arbitrary $y$, by definition
of $\left\langle \left\langle A\right\rangle \right\rangle _{p}$
\begin{align*}
\left\Vert Ay\right\Vert _{p} & \leq\left\langle \left\langle A\right\rangle \right\rangle _{p}\left\Vert y\right\Vert _{p}
\end{align*}
Then let $y=Bx$ for $\left\Vert x\right\Vert =1$ and
\begin{align*}
\left\langle \left\langle AB\right\rangle \right\rangle _{p} & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert ABx\right\Vert _{p}\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert Ay\right\Vert _{p}\\
 & \leq\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\langle \left\langle A\right\rangle \right\rangle _{p}\left\Vert y\right\Vert _{p}\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\langle \left\langle A\right\rangle \right\rangle _{p}\left\Vert Bx\right\Vert _{p}\\
 & =\left\langle \left\langle A\right\rangle \right\rangle _{p}\left\langle \left\langle B\right\rangle \right\rangle _{p}
\end{align*}

\end{enumerate}
\end{proof}
\item Claim: The supremum of a set $A$ is equal to the infimum of the set
$B$ of upperbounds on $A.$

\begin{proof}
By contradiction. Let $l=\sup\:A$. Then $l$ is an upperbound on
$A$ and hence $l\in B$. Consider the set $B'$ of lower bounds of
the set $B$. Note that $l$ is a lower bound on $B$, since if that
were not the case there would exist some element $b\in B$ such that
$b<l$, but $l$ is the least upper bound. Suppose $l\neq\inf\,B$.
Then there exists $b$ such that $b$ is a lower bound on $B$ and
$l<b$. But then $b$ is not a lower bound on $B$ since $l\in B$
and $l<b$.\end{proof}
\begin{cor*}
The induced matrix $p$-norm is equivalent to $\left\langle \left\langle A\right\rangle \right\rangle _{p}=\inf\,\Lambda$
where 
\[
\Lambda=\left\{ \lambda\Bigg|\frac{\left\Vert Ax\right\Vert _{p}}{\left\Vert x\right\Vert _{p}}\leq\lambda,x\in\mathbb{R}^{n},\lambda\in\mathbb{R}\right\} 
\]
\end{cor*}
\begin{proof}
By definition every $\lambda\in\Lambda$ is an upper bound on the
set $C=\left\{ c\big|c=\left\Vert Ax\right\Vert _{p}/\left\Vert x\right\Vert _{p},x\in\mathbb{R}^{n}\right\} $.
Since $\left\langle \left\langle A\right\rangle \right\rangle _{p}=\sup\left(C\right)$
by the above we have that 
\[
\left\langle \left\langle A\right\rangle \right\rangle _{p}=\sup\,C=\inf\,\Lambda
\]

\end{proof}
\item Claim: $\left\langle \left\langle I\right\rangle \right\rangle =1$.

\begin{proof}
To wit
\[
\left\langle \left\langle I\right\rangle \right\rangle _{p}=\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert Ix\right\Vert _{p}=\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert x\right\Vert _{p}=1
\]

\end{proof}
\item Claim: For $\left\langle \left\langle A\right\rangle \right\rangle _{2}$
it's the case that 
\[
\left\langle \left\langle A\right\rangle \right\rangle _{2}=\sqrt{\lambda_{\max}\left(A^{T}A\right)}
\]


\begin{proof}
Firstly 
\begin{align*}
\left\langle \left\langle A\right\rangle \right\rangle _{2} & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{2}=1}\left\Vert Ax\right\Vert _{2}\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{2}=1}\left(Ax\right)^{T}\left(Ax\right)\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{2}=1}x^{T}\left(A^{T}A\right)x
\end{align*}
Since $A^{T}A$ is symmetric there exist $P,\Sigma$ such that $A^{T}A=P\Sigma P^{T}$
and so
\begin{align*}
\left\langle \left\langle A\right\rangle \right\rangle _{2} & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{2}=1}x^{T}\left(P\Sigma P^{T}\right)x\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{2}=1}\left(P^{T}x\right)^{T}\Sigma\left(P^{T}x\right)
\end{align*}
Let $y=P^{T}x$ and note that $\left\Vert y\right\Vert _{2}=1$ since
$P^{T}$ is an isometry. Further note that $A^{T}A$ is positive semi-definite
since $x^{T}\left(A^{T}A\right)x=\left\Vert Ax\right\Vert \geq0$
and therefore $\Sigma_{ii}\geq0$. Let $\left(\sqrt{\Sigma}\right)_{ij}=\sqrt{\Sigma_{ij}}$
and then
\begin{align*}
\left\langle \left\langle A\right\rangle \right\rangle _{2} & =\sup_{y\in\mathbb{R}^{n},\left\Vert y\right\Vert _{2}=1}\left(y^{T}\sqrt{\Sigma}\right)\left(\sqrt{\Sigma}y\right)\\
 & =\sup_{y\in\mathbb{R}^{n},\left\Vert y\right\Vert _{2}=1}\left(\sqrt{\Sigma}y\right)^{T}\left(\sqrt{\Sigma}y\right)
\end{align*}
since $\sqrt{\Sigma}$ is symmetric. Finally 
\begin{align*}
\left\langle \left\langle A\right\rangle \right\rangle _{2} & =\sup_{y\in\mathbb{R}^{n},\left\Vert y\right\Vert _{2}=1}\left\Vert \sqrt{\Sigma}y\right\Vert _{2}
\end{align*}
and under the constraint $\left\Vert y\right\Vert _{2}=1$ it's the
case that $\left\Vert \sqrt{\Sigma}y\right\Vert _{2}$ is maximized
at $y=e_{j}$ where $j$ corresponds to the $\left(\sqrt{\Sigma}\right)_{jj}$
is maximum, i.e. the square root of the maximal eigenvalue of $A^{T}A$,
and furthermore $\left\Vert \sqrt{\Sigma}e_{j}\right\Vert _{2}=\sqrt{\lambda_{j}}$.
\end{proof}
\item Claim:$\left\langle \left\langle A\right\rangle \right\rangle _{F}$
is not an induced $p$-norm.

\begin{proof}
By counterexample: take 
\[
A=\begin{pmatrix}1 & 0\\
0 & -1
\end{pmatrix}
\]
Then
\[
A^{T}A=\begin{pmatrix}1 & 0\\
0 & 1
\end{pmatrix}
\]
and
\[
\left\langle \left\langle A\right\rangle \right\rangle _{F}=\sqrt{1+1}=\sqrt{2}
\]
but 
\begin{align*}
\left\langle \left\langle A\right\rangle \right\rangle _{p} & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert \begin{pmatrix}1 & 0\\
0 & -1
\end{pmatrix}x\right\Vert _{p}\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert \begin{pmatrix}x_{1}\\
-x_{2}
\end{pmatrix}\right\Vert _{p}\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left(\left|x_{1}\right|^{p}+\left|-x_{2}\right|^{p}\right)^{1/p}\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left(\left|x_{1}\right|^{p}+\left|x_{2}\right|^{p}\right)^{1/p}\\
 & =\sup_{x\in\mathbb{R}^{n},\left\Vert x\right\Vert _{p}=1}\left\Vert x\right\Vert _{p}\\
 & =1
\end{align*}

\end{proof}
\end{enumerate}
\item [4.1]The Laplacian is $L=D-A$ where $D$ is the degree matrix and
$A$ is the adjacency matrix, hence
\[
L=D-A=\begin{pmatrix}2 & 0 & 0 & 0 & 0 & 0\\
0 & 3 & 0 & 0 & 0 & 0\\
0 & 0 & 2 & 0 & 0 & 0\\
0 & 0 & 0 & 3 & 0 & 0\\
0 & 0 & 0 & 0 & 3 & 0\\
0 & 0 & 0 & 0 & 0 & 1
\end{pmatrix}-\begin{pmatrix}0 & 1 & 0 & 0 & 1 & 0\\
1 & 0 & 1 & 0 & 1 & 0\\
0 & 1 & 0 & 1 & 0 & 0\\
0 & 0 & 1 & 0 & 1 & 1\\
1 & 1 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0
\end{pmatrix}=\begin{pmatrix}2 & -1 & 0 & 0 & -1 & 0\\
-1 & 3 & -1 & 0 & -1 & 0\\
0 & -1 & 2 & -1 & 0 & 0\\
0 & 0 & -1 & 3 & -1 & -1\\
-1 & -1 & 0 & -1 & 3 & 0\\
0 & 0 & 0 & -1 & 0 & 1
\end{pmatrix}
\]
where
\item [4.2]Claim: $L^{T}=L$.

\begin{proof}
For an undirected graph the adjacency matrix $A$ is symmetric since
$A_{ij}=1$ iff $\left(i,j\right)\in E$ but by undirected $\left(i,j\right)\in E$
implies $\left(j,i\right)\in E$ and so $A_{ij}=A_{ji}$. $D$ is
symmetric since it is diagonal. Therefore 
\[
L^{T}=\left(D-A\right)^{T}=D^{T}-A^{T}=D-A=L
\]

\end{proof}
\item [4.3]Claim: $L$ is PSD.

\begin{proof}
If for $L$ we can write 
\[
u^{T}Lu=\frac{1}{2}\sum_{i,j\big|\left(i,j\right)\in E}\left(u_{i}-u_{j}\right)^{2}
\]
for arbitrary $u$ then $L$ is PSD, since 
\[
\sum_{i,j\big|\left(i,j\right)\in E}\left(u_{i}-u_{j}\right)^{2}\geq0
\]
Let $u=\left(u_{1},\dots,u_{n}\right)$ and using $L=D-A$ we have
\begin{align*}
u^{T}\left(D-A\right)u & =u^{T}Du-u^{T}Au\\
 & =\sum_{i=1}^{n}d\left(i\right)u_{i}^{2}-u^{T}Au\\
 & =\sum_{i=1}^{n}d\left(i\right)u_{i}^{2}-\sum_{i,j}u_{i}u_{j}A_{ij}
\end{align*}
Since $A_{ij}=\delta_{\left\{ \left(i,j\right)\in E\right\} }$ 
\begin{align*}
u^{T}\left(D-A\right)u & =\sum_{i=1}^{n}d\left(i\right)u_{i}^{2}-\sum_{i,j\big|\left(i,j\right)\in E}u_{i}u_{j}
\end{align*}
Now $d\left(i\right)=\sum_{j=1}^{n}\delta_{\left(i,j\right)\in E}$
(self edges are not in $E,$ i.e. there does not exist $i$ such that
$\left(i,i\right)\in E$) and since the graph is undirected for all
$i,j$ it's the case that $\delta_{\left\{ \left(i,j\right)\in E\right\} }=1\iff\delta_{\left\{ \left(j,i\right)\in E\right\} }=1$.
Essentially there is double counting in the $d\left(i\right)$. Therefore
\begin{align*}
u^{T}\left(D-A\right)u & =\sum_{i=1}^{n}d\left(i\right)u_{i}^{2}-\sum_{i,j\big|\left(i,j\right)\in E}u_{i}u_{j}\\
 & ==\sum_{i=1}^{n}\left(\sum_{j=1}^{n}\delta_{\left(i,j\right)\in E}\right)u_{i}^{2}-\sum_{i,j\big|\left(i,j\right)\in E}u_{i}u_{j}\\
 & =\sum_{i,j\big|\left(i,j\right)\in E}^{n}2\left(u_{i}^{2}+u_{j}^{2}\right)-\sum_{i,j\big|\left(i,j\right)\in E}u_{i}u_{j}\\
 & =\frac{1}{2}\left(\sum_{i,j\big|\left(i,j\right)\in E}^{n}\left(u_{i}^{2}+u_{j}^{2}\right)-2u_{i}u_{j}\right)\\
 & =\frac{1}{2}\left(\sum_{i,j\big|\left(i,j\right)\in E}^{n}u_{i}^{2}-2u_{i}u_{j}+u_{j}^{2}\right)\\
 & =\frac{1}{2}\sum_{i,j\big|\left(i,j\right)\in E}^{n}\left(u_{i}-u_{j}\right)^{2}
\end{align*}

\end{proof}
\item [4.4]Claim: $0$ is always an eigenvalue of $L$

\begin{proof}
If a matrix $A$ is diagonalizable then 0 is an eigenvalue iff $A$
is singular, since 
\begin{align*}
\det\left(A\right) & =\det\left(PDP^{T}\right)\\
 & =\det\left(P^{T}PD\right)\\
 & =\det\left(D\right)\\
 & =\prod_{i}\lambda_{i}
\end{align*}
Therefore if the nullspace of $A$ is non-trivial then $A$ has an
eigenvalue equal to zero. Note that summing across any row of a Laplacian
matrix $L$, for an undirected graph, we get zero. Therefore let $u=\left(1,\dots,1\right)$
implies $Lu=0$. Hence 0 is an eigenvalue.
\end{proof}
\item [4.5]Claim: If $G=\left(V,E\right)$ is path connected then the multiplicity
of the zero eigenvalue is $1$.

\begin{proof}
We prove a strong fact: the dimension of the eigenspace associated
with the eigenvalue 0 is the number of connected components of $G$.
Indeed the number of connected components of the graph corresponds
to the block structure of the Laplacian. If there exists more than
one block, e.g. 
\[
L=\begin{pmatrix}L_{1} & 0\\
0 & L_{2}
\end{pmatrix}
\]
that means there's no path from connected component $L_{1}$ to $L_{2}$
(since, up to relabeling\footnote{Simply write down the Laplacian for each connected component $L_{1},\dots,L_{k}$
(with distinct labels for vertices in distinct connected components)
and take the direct sum of the matrices $\varoplus_{i=1}^{k}L_{i}=\text{diag}\left(L_{1},\dots,L_{k}\right)$.}, there are no edges from any vertex in $L_{1}$ to any in $L_{2}$).
Furthermore this clearly corresponds to the dimension of the nullspace,
i.e. the degenerate eigenspace associated with eigenvalue $\lambda=0$.
For example for the $L$ above, if $L_{1}\in\mathbb{R}^{m\times m}$
and $L_{2}\in\mathbb{R}^{n\times n}$, then both 
\[
u_{1}=\left(\underbrace{1,\dots,1}_{m},\underbrace{0,\dots,0}_{n}\right)
\]
and 
\[
u_{2}=\left(\underbrace{0,\dots,0}_{m},\underbrace{1,\dots,1}_{n}\right)
\]


are eigenvectors corresponding to the eigenvalue $0$ and $u_{1}\cdot u_{2}=0$.
\end{proof}
\item [5.1]Claim: $A^{T}A$ is symmetric and PSD.

\begin{proof}
Firstly 
\[
\left(A^{T}A\right)^{T}=A^{T}\left(A^{T}\right)^{T}=A^{T}A
\]
Then consider
\[
x^{T}\left(A^{T}A\right)x=\left(Ax\right)^{T}\left(Ax\right)=\left|Ax\right|\geq0
\]

\end{proof}
\item [5.2]Claim: Let $v_{j}$ be orthonormal eigenbasis of $A^{T}A$,
$\lambda_{j}$ eigenvalue associated with $v_{j}$, $\sigma_{j}=\sqrt{\lambda_{j}}$,
$w_{j}\in\text{span}\left\{ u_{1},\dots,u_{j-1}\right\} ^{\perp}$
with $\left|w_{j}\right|=1$, and 
\[
u_{j}=\begin{cases}
\frac{1}{\sigma_{j}}Av_{j} & \text{if }\sigma_{j}\neq0\\
w_{j} & \text{otherwise}
\end{cases}
\]
Then $u_{j}^{T}\cdot u_{i}=\delta_{ij}$.

\begin{proof}
If $\sigma_{i}\neq0$ and $\sigma_{j}\neq0$ consider
\begin{align*}
u_{j}^{T}\cdot u_{i} & =\left(\frac{1}{\sigma_{j}}Av_{j}\right)^{T}\cdot\left(\frac{1}{\sigma_{i}}Av_{i}\right)\\
 & =\frac{1}{\sigma_{j}\sigma_{i}}v_{j}^{T}\left(A^{T}A\right)v_{i}\\
 & =\frac{1}{\sigma_{j}\sigma_{i}}v_{j}^{T}\lambda_{i}v_{i}\\
 & =\frac{\lambda_{i}}{\sigma_{j}\sigma_{i}}v_{j}^{T}\cdot v_{i}\\
 & =\frac{\lambda_{i}}{\sigma_{j}\sigma_{i}}\delta_{ij}
\end{align*}
since $v_{i},v_{j}$ are orthonormal eigenvectors of $A^{T}A$. Then
\begin{align*}
u_{j}^{T}\cdot u_{i} & =\frac{\lambda_{i}}{\sigma_{j}\sigma_{i}}\delta_{ij}\\
 & =\frac{\lambda_{i}}{\left(\sigma_{i}\right)^{2}}\delta_{ij}\\
 & =\frac{\lambda_{i}}{\left(\sqrt{\lambda_{i}}\right)^{2}}\delta_{ij}\\
 & =\delta_{ij}
\end{align*}
Without loss of generality assume $0=\sigma_{j}\neq\sigma_{i}$ and
$i<j$. Then $w_{j}\in\text{span}\left\{ u_{1},\dots,u_{j-1}\right\} ^{\perp}$.
Then immediately
\begin{align*}
u_{j}^{T}\cdot u_{i} & =\left(w_{j}\right)^{T}\cdot u_{i}=0
\end{align*}


Assume $0=\sigma_{j}=\sigma_{i}$ and $i<j$, then by $w_{j}\in\text{span}\left\{ u_{1},\dots,u_{j-1}\right\} ^{\perp}$
\begin{align*}
u_{j}^{T}\cdot u_{i} & =\left(w_{j}\right)^{T}\cdot w_{i}=0
\end{align*}

\end{proof}
\item [5.3]Reduced SVD

\begin{enumerate}
\item Claim: There exist $U,\Sigma,V$ such that $A=U\Sigma V^{T}$ and
$\Sigma$ is diagonal and 
\[
\Sigma=\begin{pmatrix}\sigma_{1} & 0 & \cdots & 0\\
0 & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & 0\\
0 & \cdots & 0 & \sigma_{n}
\end{pmatrix}
\]


\begin{proof}
Let $\sigma_{j}$ for $j=1,\dots,m$, with $m\leq n$ be the non-zero
square roots of the $A^{T}A$ and $v_{j}$ for $j=$. The by 5.2 we
have that 
\[
Av_{j}=u_{j}\sigma_{j}
\]
Succintly this is
\[
A\left[v_{1}\big|\cdots\big|v_{m}\right]=\left[u_{1}\big|\cdots\big|u_{m}\right]\begin{pmatrix}\sigma_{1} & 0 & \cdots & 0\\
0 & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & 0\\
0 & \cdots & 0 & \sigma_{n}
\end{pmatrix}
\]
i.e. $AV=U\Sigma$. Then since $V$ is an orthonormal matrix (since
the $v_{j}$ are othonormal eigenvectors of $A^{T}A$)
\[
AVV^{T}=A=U\Sigma V^{T}
\]

\end{proof}
\item Claim: $VV^{T}=V^{T}V=I$

\begin{proof}
As mentioned in part (a), since the $v_{j}$ are othonormal eigenvectors
of $A^{T}A$ we have that 
\[
v_{j}^{T}\cdot v_{i}=v_{j}^{T}\cdot v_{i}=\delta_{ij}
\]
and so $VV^{T}=V^{T}V=I$.
\end{proof}
\item Claim: $U^{T}U=I$

\begin{proof}
By 5.2 $u_{j}^{T}\cdot u_{i}=\delta_{ij}$ and hence $U^{T}U=I$
\end{proof}
\end{enumerate}
\item [5.4]For $r$ being the largest $i$ such that $\sigma_{i}\neq0$
(note that $\sigma_{i}\geq\sigma_{i+1}$)

\begin{enumerate}
\item Claim: $\left\{ u_{1},\dots,u_{r}\right\} $ is a basis for $R\left[A\right]$.

\begin{proof}
Consider for $x\in\mathbb{R}^{n}$ 
\begin{align*}
Ax & =\left[u_{1}\big|\cdots\big|u_{m}\right]\begin{pmatrix}\sigma_{1} & 0 & \cdots & 0\\
0 & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & 0\\
0 & \cdots & 0 & \sigma_{n}
\end{pmatrix}\begin{bmatrix}v_{1}^{T}\\
\vdots\\
v_{n}^{T}
\end{bmatrix}x\\
 & =\left[u_{1}\big|\cdots\big|u_{m}\right]\begin{bmatrix}\sigma_{1}v_{1}^{T}\\
\vdots\\
\sigma_{r}v_{r}^{T}\\
0\\
\vdots\\
0
\end{bmatrix}x\\
 & =\left[u_{1}\big|\cdots\big|u_{m}\right]\begin{pmatrix}\sigma_{1}v_{11} & \cdots & \sigma_{1}v_{1n}\\
\vdots & \ddots & \vdots\\
\sigma_{r}v_{r1} & \cdots & \sigma_{1}v_{1n}\\
0 & \cdots & 0\\
\vdots & \ddots & \vdots\\
0 & \cdots & 0
\end{pmatrix}x\\
 & =\left[\sum_{i=1}^{r}u_{i}\sigma_{i}v_{i1}\big|\cdots\big|\sum_{i=1}^{r}u_{i}\sigma_{i}v_{in}\right]\begin{pmatrix}x_{1}\\
\vdots\\
x_{n}
\end{pmatrix}\\
 & =\sum_{j=1}^{n}\sum_{i=1}^{r}u_{i}\sigma_{i}v_{ij}x_{j}\\
 & =\sum_{i=1}^{r}u_{i}\left(\sum_{j=1}^{n}\sigma_{i}v_{ij}x_{j}\right)
\end{align*}
Let $k_{i}=\left(\sum_{j=1}^{n}\sigma_{i}v_{ij}x_{j}\right)$ and
then $Ax=\sum_{i=1}^{r}u_{i}k_{i}$ and the image of $x$ is a linear
combination of $\left\{ u_{1},\dots,u_{r}\right\} $. Therefore the
range (column-space) of $A$ is spanned by $\left\{ u_{1},\dots,u_{r}\right\} $
and since by 5.2 $u_{j}^{T}\cdot u_{i}=\delta_{ij}$ it's in fact
a basis.
\end{proof}
\item Claim: $\left\{ u_{r+1},\dots,u_{m}\right\} $ is a basis for $\ker\left[A^{T}\right]$.

\begin{proof}
Firstly $\left\{ u_{1},\dots,u_{m}\right\} $ spans $\mathbb{R}^{m}$.
By the fundamental theorem of linear algebra $R\left[A\right]\perp\ker\left[A^{T}\right]$
and since by part (a) $\left\{ u_{1},\dots,u_{r}\right\} $ is a basis
for $R\left[A\right]$ we have that $\left\{ u_{1},\dots,u_{r}\right\} \perp\ker\left[A^{T}\right]$.
But by definition $\left\{ u_{r+1},\dots,u_{m}\right\} \perp\left\{ u_{1},\dots,u_{r}\right\} $
and furthermore for $j=r+2,\dots,m$ it's the case that $u_{j}\perp\left\{ u_{r+1},\dots,u_{j-1}\right\} $
and so $\left\{ u_{r+1},\dots,u_{m}\right\} $ are pairwise orthogonal.
Hence $\left\{ u_{r+1},\dots,u_{m}\right\} $ is a suitable basis
for $\ker\left[A^{T}\right]$.
\end{proof}
\item Claim: $\left\{ v_{1},\dots,v_{r}\right\} $ is a basis for $R\left[A^{T}\right]$.

\begin{proof}
Note that $A^{T}=\left(U\Sigma V^{T}\right)=V\Sigma U^{T}$, since
$\Sigma$ is diagonal. Then by similar reasoning as in (a) we have
that $\left\{ v_{1},\dots,v_{r}\right\} $ is a basis for $R\left[A^{T}\right]$.
\end{proof}
\item Claim: $\left\{ v_{r+1},\dots,v_{n}\right\} $ is a basis for $\ker\left[A\right]$.

\begin{proof}
By similar reasoning as in (b).
\end{proof}
\end{enumerate}
\item [5.5]Claim: For $1\leq k\leq\text{rank}\left(A\right)$
\[
\min_{X\big|\text{rank}\left(X\right)=k}\left\langle \left\langle A-X\right\rangle \right\rangle _{2}=\sigma_{k+1}
\]
and
\[
\text{argmin}\left\langle \left\langle A-X\right\rangle \right\rangle _{2}=X_{k}=\sum_{j=1}^{k}\sigma_{j}u_{j}v_{j}^{T}
\]
 

\begin{proof}
In 3 parts:\end{proof}
\begin{enumerate}
\item Lemma: For $X$ such that $\text{rank}\left(X\right)=k$ there exists
$z\in\ker\left[X\right]\cap\text{span}\left\{ v_{1},\dots,v_{k+1}\right\} $
such that $\left\Vert z\right\Vert _{2}=1$.

\begin{proof}
sdf
\end{proof}
\item Lemma: $\left\langle \left\langle A-X\right\rangle \right\rangle _{2}\geq\sigma_{k+1}$.

\begin{proof}
Using $z$ from part (a) 
\[
\left\langle \left\langle A-X\right\rangle \right\rangle _{2}=\sup_{x\in\mathbb{R}^{n}\big|\left\Vert x\right\Vert =1}
\]
\end{proof}
\end{enumerate}
\end{enumerate}

\end{document}
