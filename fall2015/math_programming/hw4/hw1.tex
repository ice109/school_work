%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside]{amsart}
\usepackage[latin9]{inputenc}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{mathtools}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{esint}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

%
\usepackage{amsfonts}
\usepackage{xfrac}
%\usepackage{mathabx}
\usepackage{nopageno}%%%  The following few lines affect the margin sizes. 
\usepackage{bm}
\addtolength{\topmargin}{-.5in}
\setlength{\textwidth}{6in}       
\setlength{\oddsidemargin}{.25in}              
\setlength{\evensidemargin}{.25in}         
  
\setlength{\textheight}{9in}
\renewcommand{\baselinestretch}{1}
\reversemarginpar   
%
%

\makeatother

\begin{document}

\title{ESI 6420 Homework 4 Solutions}


\author{Maksim Levental}


\date{\today}

\maketitle
Time spent: 15 hours

Collaborators: Chris Gianelli
\begin{enumerate}
\item [2.1]Claim: for a continuous $f\,:\,\mathbb{R}\rightarrow\mathbb{R}$
and $h>0$ 
\[
f_{h}\left(x\right)\coloneqq\frac{1}{2h}\int_{x-h}^{x+h}f\left(t\right)dt\geq f\left(x\right)
\]
iff $f$ is convex.

\begin{proof}
$\Leftarrow$ Suppose $f$ is convex. Proceed by contradiction: suppose
there exist $h_{0},x_{0}$ such that $f\left(x_{0}\right)>f_{h_{0}}\left(x_{0}\right)$.
Since $f$ is convex there exists $g\left(x\right)=f\left(x_{0}\right)+m\left(x-x_{0}\right)$
such that $g\leq f$. But then 
\[
f\left(x_{0}\right)=\frac{1}{2h}\int_{x_{0}-h_{0}}^{x_{0}+h_{0}}g\left(t\right)dt\leq\frac{1}{2h}\int_{x_{0}-h_{0}}^{x_{0}+h_{0}}f\left(t\right)dt=f_{h_{0}}\left(x_{0}\right)
\]
a contradiction. 

$\Rightarrow$ Suppose $f_{h}\left(x\right)\geq f\left(x\right)$
for all $h,x$. Towards a contradiction suppose $f$ is not convex.
Then there exist $\lambda_{0},x_{1},x_{2}$ such that 
\[
f\left(\lambda_{0}x_{1}+\left(1-\lambda_{0}\right)x_{2}\right)>\lambda_{0}f\left(x_{1}\right)+\left(1-\lambda_{0}\right)f\left(x_{2}\right)
\]
where $\lambda_{0}\in\left(0,1\right)$. Consider the function 
\[
F\left(\lambda\right)=f\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right)-\left(\lambda f\left(x_{1}\right)+\left(1-\lambda\right)f\left(x_{2}\right)\right)
\]
on $\left[0,1\right]$. Note that $F\left(0\right)=F\left(1\right)=0$
and $F\left(\lambda_{0}\right)>0$. Since $F$ is continuous, being
a linear function of $f$, it must achieve a maximum $F\left(\lambda^{*}\right)$
on $\left[0,1\right]$ (by the extreme value theorem) and $F\left(\lambda^{*}\right)>0$
(since $F\left(\lambda_{0}\right)>0$). Therefore there exists an
$h$-ball around $\lambda^{*}$ such that for $\lambda\in\left[\lambda^{*}-h,\lambda^{*}+h\right]$
it's the case that $F\left(\lambda\right)>0$ and without loss of
generality \footnote{Why? If $F$ is in fact constant on $\left[\lambda^{*}-h,\lambda^{*}+h\right]$
then we can take the minimum $h'>h$ such that either $\lambda^{*}-h'=0$
or $\lambda^{*}+h'=1$ and $F$ cannot be constant on $\left[\lambda^{*}-h',\lambda^{*}+h'\right]$.
This is since, depending on whether $h'$ is such that $\lambda^{*}-h'=0$
or $\lambda^{*}+h'=1$, either $F\left(\lambda^{*}-h'\right)=F\left(0\right)=0$
or $F\left(\lambda^{*}+h'\right)=F\left(1\right)=0$ (and $F$ cannot
equal zero on all $\left[\lambda^{*}-h,\lambda^{*}+h\right]$ since
$F\left(\lambda_{0}\right)>0$ and $F$ is continuous).}we can assume $F$ is not constant on $\left[\lambda^{*}-h,\lambda^{*}+h\right]$.
Then since $F\left(\lambda^{*}\right)\geq F\left(\lambda\right)$
for all $\lambda\in\left[\lambda^{*}-h,\lambda^{*}+h\right]$ and
$F\left(\lambda^{*}\right)>F\left(\lambda\right)$ for at least one
$\lambda\in\left[\lambda^{*}-h,\lambda^{*}+h\right]$ (otherwise $F$
would be constant on $\left[\lambda^{*}-h,\lambda^{*}+h\right]$)
we have that 
\[
2hF\left(\lambda^{*}\right)>\int_{\lambda^{*}-h}^{\lambda^{*}+h}F\left(\lambda\right)d\lambda
\]
which is equivalent to 
\begin{align*}
f\left(\lambda^{*}x_{1}+\left(1-\lambda^{*}\right)x_{2}\right)-\\
\left(\lambda^{*}f\left(x_{1}\right)+\left(1-\lambda^{*}\right)f\left(x_{2}\right)\right) & >\frac{1}{2h}\int_{\lambda^{*}-h}^{\lambda^{*}+h}\left[f\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right)-\left(\lambda f\left(x_{1}\right)+\left(1-\lambda\right)f\left(x_{2}\right)\right)\right]d\lambda\\
 & =\frac{1}{2h'}\int_{\lambda^{*}-h}^{\lambda^{*}+h}f\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right)d\lambda-\left(\lambda^{*}f\left(x_{1}\right)+\left(1-\lambda^{*}\right)f\left(x_{2}\right)\right)
\end{align*}
Cancelling $-\left(\lambda^{*}f\left(x_{1}\right)+\left(1-\lambda^{*}\right)f\left(x_{2}\right)\right)$
from both sides of the inequality we get that 
\begin{align*}
f\left(\lambda^{*}x_{1}+\left(1-\lambda^{*}\right)x_{2}\right) & >\frac{1}{2h}\int_{\lambda^{*}-h}^{\lambda^{*}+h}f\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right)d\lambda=f_{h'}\left(\lambda^{*}x_{1}+\left(1-\lambda^{*}\right)x_{2}\right)
\end{align*}
contradicting that $f\left(x\right)\leq f_{h}\left(x\right)$ for
all. Hence $f$ must be convex.
\end{proof}
\item [2.2]Let $f\left(X\right)=-\log\left(\det\left(X\right)\right)$. 

\begin{enumerate}
\item Claim: For $X,D\succeq0$ and $X\succ0$ and $g\left(t\right)=f\left(X+tD\right)$
it's the case that 
\[
g\left(t\right)=-\log\left(\det\left(\sqrt{X}\left(I+t\left(\sqrt{X}\right)^{-1}D\left(\sqrt{X}\right)^{-1}\right)\sqrt{X}\right)\right)
\]


\begin{proof}
Firstly since $X\succ0$ it's the case that $X$ is full rank (all
nonzero eigenvalues) and there exists a matrix $\sqrt{X}$ such that
$\sqrt{X}\sqrt{X}=X$ and $\sqrt{X}$ is full rank ($\sqrt{X}=Q\sqrt{\Sigma}Q^{T}$
where $Q$ is the set of eigenvectors corresponding to $X$ and $\sqrt{\Sigma}\succ0$
since $\Sigma\succ0$). Then $\left(\sqrt{X}\right)^{-1}$ exists
and hence 
\[
\sqrt{X}\left(\left(I+t\right)\left(\sqrt{X}\right)^{-1}D\left(\sqrt{X}\right)^{-1}\right)\sqrt{X}=X+tD
\]
and so $g\left(t\right)=-\log\left(\det\left(X+tD\right)\right)=f\left(X+tD\right)$.
\end{proof}
\item Claim: $f\left(X\right)$ is convex.

\begin{proof}
Using the representation of $g$ proven to be appropriate in part
(a) 
\begin{align*}
g\left(t\right) & =-\log\left(\det\left(\sqrt{X}\right)\det\left(I+t\left(\sqrt{X}\right)^{-1}D\left(\sqrt{X}\right)^{-1}\right)\det\left(\sqrt{X}\right)\right)\\
 & =-\log\left(\det\left(X\right)\right)-\log\left(\det\left(I+t\left(\sqrt{X}\right)^{-1}D\left(\sqrt{X}\right)^{-1}\right)\right)
\end{align*}
Let $Y=\left(\sqrt{X}\right)^{-1}D\left(\sqrt{X}\right)^{-1}$, which
is PSD since $D$ is PSD and $\left(\sqrt{X}\right)^{-1}$ is PD,
and 
\begin{align*}
g\left(t\right) & =-\log\left(\det\left(X\right)\right)-\log\left(\det\left(I+tY\right)\right)\\
 & =-\log\left(\det\left(X\right)\right)-\log\left(\prod_{i=1}^{n}\left(1+t\lambda_{i}\right)\right)\\
 & =-\log\left(\det\left(X\right)\right)-\sum_{i=1}^{n}\log\left(1+t\lambda_{i}\right)
\end{align*}
Note $g$ is convex in $t$ since it's the sum of a constant and convex
functions of linear transformations of $t$. Therefore $f\left(X\right)$
is convex since it is convex on every line.
\end{proof}
\end{enumerate}
\item Claim: Let $c\sim\mathcal{N}\left(\mu,\Sigma\right)$. Then assuming
there exists $x$ such that $P\left(c^{\intercal}x\geq\alpha\right)\geq\frac{1}{2}$
\begin{align*}
\max_{x\in\mathbb{R}^{n}} & P\left(c^{\intercal}x\geq\alpha\right)\\
\text{s.t.} & Fx\leq g\\
 & Ax=b
\end{align*}
can be reformulated as a quadratic convex optimization problem.

\begin{proof}
Firstly since $c\sim\mathcal{N}\left(\mu,\Sigma\right)$ it's the
case that $X=c^{\intercal}x\sim\mathcal{N}\left(\mu\cdot x,x^{\intercal}\Sigma x\right)$
and hence
\[
P\left(X\geq\alpha\right)=P\left(\frac{X-\mu\cdot x}{\sqrt{x^{\intercal}\Sigma x}}\geq\frac{\alpha-\mu\cdot x}{\sqrt{x^{\intercal}\Sigma x}}\right)=P\left(Z\geq\frac{\alpha-\mu\cdot x}{\sqrt{x^{\intercal}\Sigma x}}\right)
\]
 where $Z\sim\mathcal{N}\left(0,1\right)$. So the maximization problem
is 
\[
\max_{x}\left[P\left(Z\geq\frac{\alpha-\mu\cdot x}{\sqrt{x^{\intercal}\Sigma x}}\right)\right]
\]
Clearly maximizing this objective is equivalent to minimizing $\frac{\alpha-\mu\cdot x}{\sqrt{x^{\intercal}\Sigma x}}$.
So the problem now is 
\begin{align*}
\min_{x\in\mathbb{R}^{n}} & \frac{\alpha-\mu\cdot x}{\sqrt{x^{\intercal}\Sigma x}}\\
\text{s.t.} & Fx\preceq g\\
 & Ax=b
\end{align*}
Alternatively we can maximize the reciprocal of the objective and
hence solve the problem
\begin{align*}
\max_{x\in\mathbb{R}^{n}} & \frac{\sqrt{x^{\intercal}\Sigma x}}{\alpha-\mu\cdot x}\\
\text{s.t.} & Fx\preceq g\\
 & Ax=b
\end{align*}
Alternatively (flipping the sign) 
\begin{align*}
\min_{x\in\mathbb{R}^{n}} & \frac{\sqrt{x^{\intercal}\Sigma x}}{\mu\cdot x-\alpha}\\
\text{s.t.} & Fx\preceq g\\
 & Ax=b
\end{align*}
The fact that there exists $x_{0}$ such that 
\[
P\left(c^{\intercal}x_{0}\geq\alpha\right)\geq\frac{1}{2}
\]
or 
\[
P\left(Z\geq\frac{\alpha-\mu\cdot x_{0}}{\sqrt{x_{0}^{\intercal}\Sigma x_{0}}}\right)\geq\frac{1}{2}
\]
or 
\[
\frac{\alpha-\mu\cdot x_{0}}{\sqrt{x_{0}^{\intercal}\Sigma x_{0}}}\leq0
\]
or
\[
\alpha-\mu\cdot x_{0}\leq0
\]
implies 
\[
\left\{ x\big|Fx\preceq g,Ax=b,\mu\cdot x-\alpha\geq0\right\} \neq\emptyset
\]
Hence let $t=\frac{1}{\alpha-\mu\cdot x}$and $y=xt$. Then an equivalent
problem is
\begin{align*}
\min_{x\in\mathbb{R}^{n}} & \sqrt{y^{\intercal}\Sigma y}\\
\text{s.t.} & Fy\preceq gt\\
 & Ay=bt\\
 & \mu\cdot x-\alpha t=1\\
 & t\geq0
\end{align*}
Squaring the objective we get a convex program with a quadratic constraint.
\end{proof}
\item 

\begin{enumerate}
\item Let $x_{1}=\ln\left(r\right)$ and $x_{2}=\ln\left(h\right)$. Note
this transformation is a bijection since $r,h>0$ and $r\in\left(0,\infty\right)$
implies $x_{1}\in\left(-\infty,\infty\right)$ and similarly for $x_{2}$.
Further 
\begin{align*}
2\pi\left(r^{2}+rh\right) & =2\pi\left(e^{2x_{1}}+e^{x_{1}+x_{2}}\right)\\
\pi r^{2}h\geq V & \iff2x_{1}+x_{2}\geq\ln\left(\frac{V}{\pi}\right)
\end{align*}
Hence the two problems are equivalent.
\item The new problem is a convex optimization problem because the objective
is convex (being the sum of two convex functions of linear transformations)
and the constraints are linear.
\item Since everything is differentiable we can just use calculus: let $f\left(x_{1},x_{2}\right)=2\pi\left(e^{2x_{1}}+e^{x_{1}+x_{2}}\right)$.
Then 
\[
\nabla f=\left(2e^{2x_{1}}+e^{x_{1}+x_{2}},e^{x_{1}+x_{2}}\right)
\]
Since this is never zero the constraint must be active (or the problem
unbounded). On the constaint boundary 
\[
g\left(x_{1}\right)=f\left(\ln\left(\frac{V}{\pi}\right)-2x_{1},x_{1}\right)=2\pi\left(e^{2x_{1}}+e^{x_{1}+\ln\left(\frac{V}{\pi}\right)-2x_{1}}\right)=2\pi\left(e^{2x_{1}}+e^{\ln\left(\frac{V}{\pi}\right)-x_{1}}\right)=2\pi\left(e^{2x_{1}}+\frac{V}{\pi}e^{-x_{1}}\right)
\]
Then 
\[
g'\left(x_{1}\right)=2\pi\left(2e^{2x_{1}}-\frac{V}{\pi}e^{-x_{1}}\right)
\]
which is zero at 
\[
e^{2x_{1}}=\frac{V}{2\pi}e^{-x_{1}}\Rightarrow2x_{1}=\ln\left(\frac{V}{2\pi}\right)-x_{1}\Rightarrow x_{1}=\frac{1}{3}\ln\left(\frac{V}{2\pi}\right)
\]
Therefore 
\[
x_{2}=\ln\left(\frac{V}{\pi}\right)-\frac{2}{3}\ln\left(\frac{V}{2\pi}\right)=\ln\left(2\right)+\ln\left(\frac{V}{2\pi}\right)-\frac{2}{3}\ln\left(\frac{V}{2\pi}\right)=\ln\left(2\right)+\frac{1}{3}\ln\left(\frac{V}{2\pi}\right)
\]
Hence
\begin{align*}
r & =e^{\frac{1}{3}\ln\left(\frac{V}{2\pi}\right)}=\left(\frac{V}{2\pi}\right)^{\frac{1}{3}}\\
h & =e^{\ln\left(2\right)+\frac{2}{3}\ln\left(\frac{V}{2\pi}\right)}=2\left(\frac{V}{2\pi}\right)^{\frac{1}{3}}
\end{align*}

\end{enumerate}
\item Let $P$ be the program 
\begin{align*}
\min_{\left(x,y\right)} & c^{\intercal}x+d^{\intercal}y+x^{\intercal}Hy\\
\text{s.t.} & x\in X,y\in Y
\end{align*}
where $X,Y$ are bounded polyhedra. 

\begin{enumerate}
\item Claim: There exists an extreme point $\left(\bar{x},\bar{y}\right)$
that solves the bilinear program. 

\begin{proof}
\textbf{Full disclosure: }For this part I looked at Thieu's paper
here http://journals.math.ac.vn/acta/pdf/ 198002106.pdf. The problem
can be restated as 
\[
\min_{x\in X}\left(c^{\intercal}x+\min_{y\in Y}\left(\left(d^{\intercal}+x^{\intercal}H\right)y\right)\right)
\]
Note that $\left(\left(d^{\intercal}+x^{\intercal}H\right)y\right)$
is linear in $y$ over the polyhedron $Y$ hence the optimium is attained
at an extreme point. Let $V\left(Y\right)$ be the set of extreme
points $Y$. Then the problem can be restated again as 
\[
\min_{x\in X}\left(c^{\intercal}x+\min_{y\in V\left(Y\right)}\left(\left(d^{\intercal}+x^{\intercal}H\right)y\right)\right)
\]
For each $\bar{y}\in V\left(Y\right)$ the function 
\begin{align*}
g\left(x\right) & =\left(\bar{y}H+c^{\intercal}\right)x+d^{\intercal}\bar{y}
\end{align*}
is a linear function of $x$. Hence the problem 
\[
\min_{x\in X}g\left(x\right)
\]
is the minimization of a piecewise linear function of $x$ over a
polyhedron and therefore attains its minimum at an extreme point $\bar{x}$.
Therefore $\left(\bar{x},\bar{y}\right)$ is a solution of the original
bilinear problem and both $\bar{x},\bar{y}$ are extreme points of
$X,Y$ respectively.
\end{proof}
\item Claim: $\left(\hat{x},\hat{y}\right)$ is a local minimum iff $\left(c^{\intercal}+\hat{y}^{\intercal}H^{\intercal}\right)\left(x-\hat{x}\right)\geq0$
and $\left(d^{\intercal}+\hat{x}^{\intercal}H\right)\left(y-\hat{y}\right)\geq0$
for $x,y\in X,Y$ and $\left(c^{\intercal}+\hat{y}^{\intercal}H^{\intercal}\right)\left(x-\hat{x}\right)+\left(d^{\intercal}+\hat{x}^{\intercal}H\right)\left(y-\hat{y}\right)>0$
when $\left(x-\hat{x}\right)^{\intercal}H\left(y-\hat{y}\right)<0$.

\begin{proof}
$\Rightarrow$ Assume $\left(\hat{x},\hat{y}\right)$ is a local minimum.
Then there exists some $\mathcal{N}_{\varepsilon}\left(\left(\hat{x},\hat{y}\right)\right)$
such that for all $\left(x,y\right)\in\mathcal{N}_{\varepsilon}\left(\left(\hat{x},\hat{y}\right)\right)$
it's the case that 
\[
c^{\intercal}x+d^{\intercal}y+x^{\intercal}Hy\geq c^{\intercal}\hat{x}+d^{\intercal}\hat{y}+\hat{x}^{\intercal}H\hat{y}
\]
$\hat{x}$ being optimal implies that for any $\left(x,\hat{y}\right)\in\mathcal{N}_{\varepsilon}\left(\left(\hat{x},\hat{y}\right)\right)$
\begin{align*}
c^{\intercal}x+d^{\intercal}\hat{y}+x^{\intercal}H\hat{y} & \geq c^{\intercal}\hat{x}+d^{\intercal}\hat{y}+\hat{x}^{\intercal}H\hat{y}\\
 & \iff\\
c^{\intercal}\left(x-\hat{x}\right)+x^{\intercal}H\hat{y} & \geq\hat{x}^{\intercal}H\hat{y}\\
 & \iff\\
c^{\intercal}\left(x-\hat{x}\right)+\left(x-\hat{x}\right)^{\intercal}H\hat{y} & \geq0
\end{align*}
Since $u^{\intercal}Av=\left(u^{\intercal}Av\right)^{\intercal}=v^{\intercal}A^{\intercal}u$
for all $u,v,A$ 
\begin{align*}
c^{\intercal}\left(x-\hat{x}\right)+\left(x-\hat{x}\right)^{\intercal}H\hat{y} & \geq0\\
 & \iff\\
c^{\intercal}\left(x-\hat{x}\right)+\hat{y}^{\intercal}H^{\intercal}\left(x-\hat{x}\right) & \geq0\\
 & \iff\\
\left(c^{\intercal}+\hat{y}^{\intercal}H^{\intercal}\right)\left(x-\hat{x}\right) & \geq0
\end{align*}
Similarly for $\left(\hat{x},y\right)\in\mathcal{N}_{\varepsilon}\left(\left(\hat{x},\hat{y}\right)\right)$
\begin{align*}
d^{\intercal}y+\hat{x}^{\intercal}Hy & \geq d^{\intercal}\hat{y}+\hat{x}^{\intercal}H\hat{y}\\
 & \iff\\
d^{\intercal}\left(y-\hat{y}\right)+\hat{x}^{\intercal}Hy-\hat{x}^{\intercal}H\hat{y} & \geq0\\
 & \iff\\
d^{\intercal}\left(y-\hat{y}\right)+\hat{x}^{\intercal}H\left(y-\hat{y}\right) & \geq0\\
 & \iff\\
\left(d^{\intercal}+\hat{x}^{\intercal}H\right)\left(y-\hat{y}\right) & \geq0
\end{align*}
Further if $\left(\hat{x},\hat{y}\right)$ is a local minimum
\begin{align*}
c^{\intercal}x+d^{\intercal}y+x^{\intercal}Hy & \geq c^{\intercal}\hat{x}+d^{\intercal}\hat{y}+\hat{x}^{\intercal}H\hat{y}\\
 & \iff\\
c^{\intercal}\left(x-\hat{x}\right)+d^{\intercal}\left(y-\hat{y}\right)+x^{\intercal}Hy-\hat{x}^{\intercal}H\hat{y} & \geq0\\
 & \iff\\
c^{\intercal}\left(x-\hat{x}\right)+d^{\intercal}\left(y-\hat{y}\right)+\left(x^{\intercal}-\hat{x}^{\intercal}\right)H\left(y-\hat{y}\right)+\left(-2\hat{x}^{\intercal}H\hat{y}+x^{\intercal}H\hat{y}+\hat{x}^{\intercal}Hy\right) & \geq0
\end{align*}
Now since $\left(x-\hat{x}\right)^{\intercal}H\left(y-\hat{y}\right)<0$
\begin{align*}
c^{\intercal}\left(x-\hat{x}\right)+d^{\intercal}\left(y-\hat{y}\right)+\left(-2\hat{x}^{\intercal}H\hat{y}+x^{\intercal}H\hat{y}+\hat{x}^{\intercal}Hy\right) & >0\\
 & \iff\\
c^{\intercal}\left(x-\hat{x}\right)+d^{\intercal}\left(y-\hat{y}\right)+\left(x^{\intercal}-\hat{x}^{\intercal}\right)H\hat{y}+\hat{x}^{\intercal}H\left(y-\hat{y}\right) & >0\\
 & \iff\\
\left(c^{\intercal}+\hat{y}^{\intercal}H^{\intercal}\right)\left(x-\hat{x}\right)+\left(d^{\intercal}+\hat{x}^{\intercal}H\right)\left(y-\hat{y}\right) & >0
\end{align*}
Finally since, by part (1), since a solution exists at an extreme
point, these inequalities hold for all $\left(x,y\right)\in X\times Y$.

$\Leftarrow$ Assume the inequalities hold for $\left(x,y\right)\in X\times Y$
and $\left(x-\hat{x}\right)^{\intercal}H\left(y-\hat{y}\right)\geq0$.
Then 
\begin{align*}
\left(c^{\intercal}+\hat{y}^{\intercal}H^{\intercal}\right)\left(x-\hat{x}\right)+\left(d^{\intercal}+\hat{x}^{\intercal}H\right)\left(y-\hat{y}\right)+\left(x-\hat{x}\right)^{\intercal}H\left(y-\hat{y}\right) & \geq0\\
 & \iff\\
c^{\intercal}\left(x-\hat{x}\right)+d^{\intercal}\left(y-\hat{y}\right)\cancel{+\hat{y}^{\intercal}H^{\intercal}x}-\hat{y}^{\intercal}H^{\intercal}\hat{x}\cancel{+\hat{x}^{\intercal}Hy}\cancel{-\hat{x}^{\intercal}H\hat{y}}+x^{\intercal}Hy\cancel{-\hat{x}^{\intercal}Hy}\cancel{-x^{\intercal}H\hat{y}}\cancel{+\hat{x}^{\intercal}H\hat{y}} & \geq0\\
 & \iff\\
c^{\intercal}\left(x-\hat{x}\right)+d^{\intercal}\left(y-\hat{y}\right)+x^{\intercal}Hy & \geq\hat{y}^{\intercal}H^{\intercal}\hat{x}\\
 & \iff\\
c^{\intercal}x+d^{\intercal}y+x^{\intercal}Hy & \geq c^{\intercal}\hat{x}+d^{\intercal}\hat{y}+\hat{y}^{\intercal}H^{\intercal}\hat{x}
\end{align*}
If for $\left(x,y\right)$ it's the case that $\left(x-\hat{x}\right)^{\intercal}H\left(y-\hat{y}\right)<0$
then 
\begin{align*}
\left(c^{\intercal}+\hat{y}^{\intercal}H^{\intercal}\right)\left(x-\hat{x}\right)+\left(d^{\intercal}+\hat{x}^{\intercal}H\right)\left(y-\hat{y}\right) & >0\\
 & \iff\\
\left(c^{\intercal}+\hat{y}^{\intercal}H^{\intercal}\right)\left(x-\hat{x}\right)+\left(d^{\intercal}+\hat{x}^{\intercal}H\right)\left(y-\hat{y}\right)+\left(x^{\intercal}-\hat{x}^{\intercal}\right)H\left(y-\hat{y}\right) & \geq0\\
 & \iff\\
c^{\intercal}x+d^{\intercal}y+x^{\intercal}Hy & \geq c^{\intercal}\hat{x}+d^{\intercal}\hat{y}+\hat{x}^{\intercal}H\hat{y}
\end{align*}
\end{proof}
\end{enumerate}
\end{enumerate}

\end{document}
