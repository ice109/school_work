#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsart
\begin_preamble

%
\usepackage{amsfonts}
%\usepackage{mathabx}
\usepackage{nopageno}%%%  The following few lines affect the margin sizes. 
\usepackage{bm}
\addtolength{\topmargin}{-.5in}
\setlength{\textwidth}{6in}       
\setlength{\oddsidemargin}{.25in}              
\setlength{\evensidemargin}{.25in}         
  
\setlength{\textheight}{9in}
\renewcommand{\baselinestretch}{1}
\reversemarginpar   
%
%
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 2
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Convex Sets
\end_layout

\begin_layout Enumerate
The set of optima for a min LP is a convex set (just use linearity of the
 matrix defining the constraints, i.e.
 since 
\begin_inset Formula $Ax\preceq b$
\end_inset

 defines the polyhedron).
\end_layout

\begin_layout Enumerate
Intersection, Minkowski sum, Minkowski difference, preserve convexity of
 sets.
\end_layout

\begin_layout Enumerate
Caratheodory's theorem: for a convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 any point can be expressed as a convex combination of at most 
\begin_inset Formula $n+1$
\end_inset

 points (prove using linear independence of differences between points).
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $S$
\end_inset

 a convex set with non-empty interior, 
\begin_inset Formula $x_{1}$
\end_inset

 in the closure of 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

 in the interior then the 
\begin_inset Quotes eld
\end_inset

weak
\begin_inset Quotes erd
\end_inset

 convex combination, i.e.
 (
\begin_inset Formula $\lambda\in\left(0,1\right)$
\end_inset

) is always in the interior.
\end_layout

\begin_layout Enumerate
The interior of a convex set is convex and if the interior is non-empty
 then so is the closure.
\end_layout

\begin_layout Enumerate
For a convex set with non-empty interior 
\begin_inset Formula $cl\left(int\left(S\right)\right)=cl\left(S\right)$
\end_inset

 and 
\begin_inset Formula $int\left(cl\left(S\right)\right)=int\left(S\right)$
\end_inset

.
\end_layout

\begin_layout Section
Hyperplanes
\end_layout

\begin_layout Subsection
Projection theorem
\end_layout

\begin_layout Standard
Projection theorem: for any closed convex set 
\begin_inset Formula $S$
\end_inset

 and for any point not in the convex set 
\begin_inset Formula $y$
\end_inset

 there exists a projection 
\begin_inset Formula $\bar{x}$
\end_inset

 of 
\begin_inset Formula $y$
\end_inset

 onto 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $\bar{x}$
\end_inset

 is such that 
\begin_inset Formula $\left(y-\bar{x}\right)^{\intercal}\left(x-\bar{x}\right)\leq0$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

, i.e.
 the plane defined by 
\begin_inset Formula $\left(y-\bar{x}\right)$
\end_inset

 separates 
\begin_inset Formula $S$
\end_inset

 from 
\begin_inset Formula $y$
\end_inset

.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename projection_hyperplane.jpg
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Projection theorem
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Proof
We seek to minimize 
\begin_inset Formula $\left\Vert x-y\right\Vert $
\end_inset

 over 
\begin_inset Formula $x\in S$
\end_inset

.
 This is equivalent to minimizing over the set
\begin_inset Formula 
\[
\left\{ x\big|\left\Vert x-y\right\Vert \leq\left\Vert x-w\right\Vert \right\} 
\]

\end_inset

for some 
\begin_inset Formula $w\in S$
\end_inset

.
 Why? Because 
\begin_inset Formula $\left\{ x\big|\left\Vert x-y\right\Vert \leq\left\Vert x-w\right\Vert \right\} \subset S$
\end_inset

 and the 
\begin_inset Formula $\bar{x}$
\end_inset

 that minimizes 
\begin_inset Formula $\left\Vert x-y\right\Vert $
\end_inset

 is definitely in it (since it's smaller than all 
\begin_inset Formula $\left\Vert x-y\right\Vert $
\end_inset

).
 This is a bounded closed set (since 
\begin_inset Formula $S$
\end_inset

 is closed) and norms are continuous and so by extreme value theorem there
 exists a minimum.
 
\begin_inset Formula $\bar{x}$
\end_inset

 is unique: suppose there exist two such points 
\begin_inset Formula $\bar{x}_{1},\bar{x}_{2}$
\end_inset

.
 Then
\begin_inset Formula 
\begin{align*}
0 & <\left\Vert \left(\bar{x}_{1}-y\right)-\left(\bar{x}_{2}-y\right)\right\Vert ^{2}\\
 & =2\left\Vert \bar{x}_{1}-y\right\Vert ^{2}+2\left\Vert \bar{x}_{2}-y\right\Vert ^{2}-4\left\Vert \frac{1}{2}\left[\left(\bar{x}_{1}-y\right)-\left(\bar{x}_{2}-y\right)\right]\right\Vert ^{2}\\
 & =2\left\Vert \bar{x}_{1}-y\right\Vert ^{2}+2\left\Vert \bar{x}_{2}-y\right\Vert ^{2}-4\left\Vert \frac{\left(\bar{x}_{1}+\bar{x}_{2}\right)}{2}-y\right\Vert ^{2}\\
 & =2\left\Vert \bar{x}_{1}-y\right\Vert ^{2}+2\left\Vert \bar{x}_{2}-y\right\Vert ^{2}-4\left\Vert \hat{x}-y\right\Vert ^{2}
\end{align*}

\end_inset

Since 
\begin_inset Formula $\left\Vert \bar{x}_{1}-y\right\Vert =\left\Vert \bar{x}_{2}-y\right\Vert =s$
\end_inset

 rearranging we have 
\begin_inset Formula $\left\Vert \hat{x}-y\right\Vert <s$
\end_inset

 contradicting that 
\begin_inset Formula $x_{1},x_{2}$
\end_inset

 are minimal projections.
 
\end_layout

\begin_deeper
\begin_layout Corollary*
Let 
\begin_inset Formula $S$
\end_inset

 be the column space of some matrix 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $y\notin S$
\end_inset

.
 Then for the projection 
\begin_inset Formula $\bar{z}$
\end_inset

 it's the case that 
\begin_inset Formula $\left(y-\bar{z}\right)\perp z$
\end_inset

 for all 
\begin_inset Formula $z\in col\left(A\right)$
\end_inset

 and 
\begin_inset Formula $A^{\intercal}\bar{z}=A^{\intercal}y$
\end_inset

 (i.e.
 the projection is the perpendicular projector).
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Corollary*
Least squares.
\end_layout

\end_deeper
\begin_layout Standard

\series bold
Improper separation
\series default
 is when possibly both sets are in the the separating hyperplane.
 
\series bold
Proper separation
\series default
 is when the union of the two sets isn't contained in the separating hyperplane.
 
\series bold
Strict separation 
\series default
is when the convex sets don't intersect the separating hyperplane (but their
 closures might).
 
\series bold
Strong separation
\series default
 is when there's 
\begin_inset Quotes eld
\end_inset

fat
\begin_inset Quotes erd
\end_inset

 in between the sets and the hyperplane.
\end_layout

\begin_layout Standard
Point-to-set separation: for any non-empty closed convex set 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $y\notin S$
\end_inset

 there's a separating hyperplane.
 Proof: use the projection theorem to find the projection.
 Keep in mind the obtuseness (
\begin_inset Formula $\left(y-\bar{x}\right)^{\intercal}\left(x-\bar{x}\right)\leq0$
\end_inset

 says that every vector from 
\begin_inset Formula $x\in S$
\end_inset

 to 
\begin_inset Formula $\bar{x}$
\end_inset

 is obtuse to the normal vector to the separating hyperplane 
\begin_inset Formula $p=y-\bar{x}$
\end_inset

).
\end_layout

\begin_layout Corollary*
Every closed convex set is the intersection of halfspaces (take all the
 separation hyperplanes and intersection the half spaces that the set is
 in).
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Corollary*
Let 
\begin_inset Formula $S$
\end_inset

 be nonempty and 
\begin_inset Formula $y$
\end_inset

 not in the closure of the convex hull of 
\begin_inset Formula $S$
\end_inset

.
 Then you can stronly separate.
\end_layout

\begin_layout Subsection
LP Duality
\end_layout

\begin_layout Theorem*
Farkas' lemma: exactly one of the two following systems has a solution
\begin_inset Formula 
\begin{align*}
x\text{ s.t. } & Ax\preceq0\text{ and }c^{\intercal}x>0\\
y\text{ s.t. } & A^{\intercal}y=c\text{ and }y\succeq0
\end{align*}

\end_inset

The intuition here is if the columns of 
\begin_inset Formula $A^{\intercal}$
\end_inset

 by 
\begin_inset Formula $a_{1},\dots,a_{m}$
\end_inset

 the second system has a solution iff 
\begin_inset Formula $c$
\end_inset

 lies in the conic cone??? of 
\begin_inset Formula $a_{1},\dots,a_{m}$
\end_inset

.
 Apparently a convex cone is a cone closed under conic combinations and
 a cone is any set closed under positive scalings.
 If 
\begin_inset Formula $c$
\end_inset

 doesn't lie in the cone then the polyhedral convex cone
\begin_inset Foot
status open

\begin_layout Plain Layout
Why is this a polyhedral cone? Well 
\begin_inset Formula $x$
\end_inset

 that satisfies 
\begin_inset Formula $Ax\preceq b$
\end_inset

 is the intersection of half-spaces (think hyperplanes defined by the rows
 of 
\begin_inset Formula $A$
\end_inset

).
 If 
\begin_inset Formula $b=0$
\end_inset

 then all of those halfspaces go through the origin.
\end_layout

\end_inset


\begin_inset Formula $Ax\preceq0$
\end_inset

 and the halfspace 
\begin_inset Formula $c^{\intercal}x>0$
\end_inset

 have a nonempty intersection.
 Note that cone defined by this system is actually the polar of the vectors
 themselves (since these vectors actually define the 
\series bold
normals
\series default
 to the plane).
 This is basically all about polar cones.
 Either a vector is in the convex cone or there exists a vector in the polar
 cone that makes an accute angle with it (though not necessarily in the
 polar cone).
 
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename farkas.jpg
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Farkas lemma
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Proof
Suppose system 2 has a solution, i.e.
 
\begin_inset Formula $c$
\end_inset

 is in the polar cone of the rows of 
\begin_inset Formula $A$
\end_inset

.
 Then there exists 
\begin_inset Formula $y\succeq0$
\end_inset

 such that 
\begin_inset Formula $A^{\intercal}y=c$
\end_inset

.
 Let 
\begin_inset Formula $x$
\end_inset

 be in the other cone, i.e.
 
\begin_inset Formula $Ax\preceq0$
\end_inset

.
 Then 
\begin_inset Formula $c^{\intercal}x=y^{\intercal}Ax\preceq0$
\end_inset

 (since 
\begin_inset Formula $y\succeq0$
\end_inset

 times 
\begin_inset Formula $Ax\preceq0$
\end_inset

 is 
\begin_inset Formula $\preceq0$
\end_inset

).
 Hence system 1 has no solution.
 Suppose system 2 doesn't have a solution.
 I.e.
 
\begin_inset Formula $c$
\end_inset

 doesn't lie in the convex cone of the rows of 
\begin_inset Formula $A$
\end_inset

.
 Let 
\begin_inset Formula $S=\left\{ x\big|x=A^{\intercal}y,y\succeq0\right\} $
\end_inset

 i.e.
 the convex cone.
 
\begin_inset Formula $S$
\end_inset

 is closed convex set and 
\begin_inset Formula $c\notin S$
\end_inset

.
 By point set separation there exists a vector 
\begin_inset Formula $p$
\end_inset

 and scalar 
\begin_inset Formula $\alpha$
\end_inset

 such that 
\begin_inset Formula $p^{\intercal}c>\alpha$
\end_inset

 and 
\begin_inset Formula $p^{\intercal}x\leq\alpha$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

 (
\begin_inset Formula $p$
\end_inset

 is the projection vector from 
\begin_inset Formula $S$
\end_inset

 to 
\begin_inset Formula $c$
\end_inset

 and all this says is that there's a plane defined by the normal vector
 
\begin_inset Formula $p$
\end_inset

 that separates 
\begin_inset Formula $c$
\end_inset

 and 
\begin_inset Formula $S$
\end_inset

 with 
\begin_inset Formula $S$
\end_inset

 on the 
\begin_inset Quotes eld
\end_inset

negative
\begin_inset Quotes erd
\end_inset

 side of that plane.
 Since 
\begin_inset Formula $0\in S$
\end_inset

 (since we're talking about a convex cone, 
\begin_inset Formula $y=0$
\end_inset

) 
\begin_inset Formula $\alpha\geq0$
\end_inset

 (just take one of the faces of 
\begin_inset Formula $S$
\end_inset

) so 
\begin_inset Formula $p^{\intercal}c>0$
\end_inset

.
 Also 
\begin_inset Formula $\alpha\geq p^{\intercal}A^{\intercal}y=y^{\intercal}Ap$
\end_inset

 (because 
\begin_inset Formula $x=A^{\intercal}y$
\end_inset

 and 
\begin_inset Formula $\alpha\geq p^{\intercal}x$
\end_inset

) for all 
\begin_inset Formula $y\succeq0$
\end_inset

.
 Since 
\begin_inset Formula $y$
\end_inset

 can be made arbitrarily large the last inequality implies that's really
 negative is 
\begin_inset Formula $Ap$
\end_inset

, i.e.
 
\begin_inset Formula $Ap\preceq0$
\end_inset

.
 Therefore 
\begin_inset Formula $p$
\end_inset

 is a vector such that 
\begin_inset Formula $Ap\preceq0$
\end_inset

 and 
\begin_inset Formula $p^{\intercal}c=c^{\intercal}p<0$
\end_inset

.
\end_layout

\begin_layout Corollary*
Gordan's theorem
\begin_inset Formula 
\begin{align*}
\left\{ x\big|Ax\prec0\right\}  & =\emptyset\\
 & \iff\\
\left\{ y\big|A^{\intercal}y=0,y\succ0\right\}  & =\emptyset
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Note 
\begin_inset Formula $Ax\prec0$
\end_inset

 iff 
\begin_inset Formula $A\mathbf{x}+\mathbf{e}s\preceq0$
\end_inset

 which is equivalent to 
\begin_inset Formula 
\[
\left[A\,\mathbf{e}\right]\begin{bmatrix}\mathbf{x}\\
s
\end{bmatrix}\preceq0\quad\left(0,\dots,0,1\right)\begin{bmatrix}\mathbf{x}\\
s
\end{bmatrix}\succ0
\]

\end_inset

By Farkas' lemma the associated system is 
\begin_inset Formula 
\[
\begin{bmatrix}A^{\intercal}\\
\mathbf{e}^{\intercal}
\end{bmatrix}y=\left(0,\dots,0,1\right)\,y\succeq0
\]

\end_inset

I.e.
 
\begin_inset Formula $A^{\intercal}y=0$
\end_inset

 and 
\begin_inset Formula $\mathbf{e}^{\intercal}y=1$
\end_inset

.
 So 
\begin_inset Formula $y\neq0$
\end_inset

.
 By Farkas' lemma these two systems are alternative systems.
\end_layout

\begin_layout Corollary*
Another form of Farkas' lemma
\begin_inset Formula 
\begin{align*}
\left\{ x\big|Ax\succeq b\right\}  & =\emptyset\\
 & \iff\\
\left\{ y\big|A^{\intercal}y=0,b^{\intercal}y\succ0\right\}  & =\emptyset
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Duality of LPs.
 
\end_layout

\begin_layout Standard
Consider the primary system 
\begin_inset Formula 
\begin{align*}
\max & 2x_{1}+3x_{2}\\
\text{s.t} & 4x_{1}+8x_{2}\leq12\\
 & 2x_{1}+x_{2}\leq3\\
 & 3x_{1}+2x_{2}\leq4\\
 & x_{1},x_{2}\geq0
\end{align*}

\end_inset

Consider solving the problem by successively bounding it from above.
 Note that since 
\begin_inset Formula 
\[
2x_{1}+3x_{2}\leq4x_{1}+8x_{2}\leq12
\]

\end_inset


\begin_inset Formula $\max\left(2x_{1}+3x_{2}\right)\leq12$
\end_inset

.
 Similarly 
\begin_inset Formula 
\[
2x_{1}+3x_{2}\leq\frac{1}{2}\left(4x_{1}+8x_{2}\right)\leq6
\]

\end_inset

or even more creatively
\begin_inset Formula 
\[
2x_{1}+3x_{2}\leq\frac{1}{3}\left(4x_{1}+8x_{2}\right)+\left(2x_{1}+x_{2}\right)\leq5
\]

\end_inset

So basically the game is equating coefficients.
 You can see that if we let 
\begin_inset Formula $y_{1},y_{2},y_{3}$
\end_inset

 be the linear combination coefficents and enforce that after equating the
 coeffients they don't fall below 
\begin_inset Formula $2$
\end_inset

 for 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $3$
\end_inset

 for 
\begin_inset Formula $x_{2}$
\end_inset

, i.e.
 
\begin_inset Formula 
\[
2x_{1}+3x_{2}\leq y_{1}\left(4x_{1}+8x_{2}\right)+y_{2}\left(2x_{1}+x_{2}\right)+y_{3}\left(3x_{1}+2x_{2}\right)\leq y_{1}12+y_{2}3+y_{3}4
\]

\end_inset

or 
\begin_inset Formula 
\begin{align*}
4y_{1}+2y_{2}+3y_{3} & \geq2\\
8y_{1}+y_{2}+2y_{3} & \geq3\\
y_{1},y_{2},y_{3} & \geq0
\end{align*}

\end_inset

and we minimize 
\begin_inset Formula $12y_{1}+3y_{2}+4y_{3}$
\end_inset

 we should get a tight upper bound on the object of the original problem.
 This is obviously an LP as well (called the dual).
 And you can go backwards as well.
 In general we have the 
\series bold
primal
\series default
 
\begin_inset Formula 
\begin{align*}
\max_{x} & c^{\intercal}x\\
\text{s.t} & Ax\preceq b\\
 & x\succeq0
\end{align*}

\end_inset

and the 
\series bold
dual
\series default

\begin_inset Formula 
\begin{align*}
\min_{y} & b^{\intercal}y\\
\text{s.t} & A^{\intercal}y\succeq c\\
 & y\succeq0
\end{align*}

\end_inset

and what holds is that either both problems are infeasible, infeasible and
 unbounded respectively (or vice versa) or both feasible and with optima
 equal.
 This is called 
\series bold
strong
\series default
 
\series bold
duality
\series default
 in LPs.
 On the way to proving that we need to prove 
\series bold
weak duality
\series default
 
\end_layout

\begin_layout Theorem*
Weak duality: for the primal solution 
\begin_inset Formula $x$
\end_inset

 and dual problem solution 
\begin_inset Formula $y$
\end_inset

 
\begin_inset Formula 
\[
c^{\intercal}x\leq b^{\intercal}y
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Proof
To wit
\begin_inset Formula 
\begin{align*}
c^{\intercal}x & =x^{\intercal}c\\
 & \leq x^{\intercal}\left(A^{\intercal}y\right)\text{ since \ensuremath{y}\text{ is feasible for the dual and }\ensuremath{x\succeq0}}\\
 & =\left(Ax\right)^{\intercal}y\\
 & \leq b^{\intercal}y\text{ since \ensuremath{x}\text{ is feasible for the primal and }\ensuremath{y\succeq0}}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout --Separator--

\end_layout

\begin_layout Theorem*
Strong duality for LPs.
 We have to rewrite the dual and primal a little: primal
\begin_inset Formula 
\begin{align*}
\min_{x} & c^{\intercal}x\\
\text{s.t} & Ax\succeq b
\end{align*}

\end_inset

and the dual
\begin_inset Formula 
\begin{align*}
\max_{y} & b^{\intercal}y\\
\text{s.t} & A^{\intercal}y=c\\
 & y\succeq0
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Proof
Suppose the dual is feasible and its max is 
\begin_inset Formula $\delta$
\end_inset

.
 Let 
\begin_inset Formula 
\[
P'=\left\{ x\big|Ax\succeq b,c^{\intercal}x\leq\delta\right\} 
\]

\end_inset

If 
\begin_inset Formula $P'$
\end_inset

 is nonempty then the primal must have a feasible solution with value at
 most 
\begin_inset Formula $\delta$
\end_inset

 (since 
\begin_inset Formula $P'$
\end_inset

 is basically the reduced feasible region of the primal.
 Note that 
\begin_inset Formula $P'=\left\{ x\big|Ax\succeq b,-c^{\intercal}x\geq-\delta\right\} $
\end_inset

.
 Towards a contradiction suppose 
\begin_inset Formula $P'$
\end_inset

 is empty.
 Then by Farkas' lemma (form 2) there exist 
\begin_inset Formula $y,\lambda$
\end_inset

 such that
\begin_inset Formula 
\[
\begin{bmatrix}A^{\intercal}\\
-c
\end{bmatrix}\begin{pmatrix}y\\
\lambda
\end{pmatrix}=0\text{ and }\left(b,-\delta\right)^{\intercal}\begin{pmatrix}y\\
\lambda
\end{pmatrix}>0
\]

\end_inset

This implies that 
\begin_inset Formula $A^{\intercal}y-c\lambda=0$
\end_inset

 and 
\begin_inset Formula $b^{\intercal}y-\lambda\delta>0$
\end_inset

.
 There are two cases
\end_layout

\begin_deeper
\begin_layout Case
If 
\begin_inset Formula $\lambda=0$
\end_inset

 then 
\begin_inset Formula $A^{\intercal}y=0$
\end_inset

 and 
\begin_inset Formula $b^{\intercal}y>0$
\end_inset

.
 Choose 
\begin_inset Formula $z\succeq0$
\end_inset

 such that 
\begin_inset Formula $A^{\intercal}z=c$
\end_inset

 and 
\begin_inset Formula $b^{\intercal}z=\delta$
\end_inset

.
 Then for 
\begin_inset Formula $\varepsilon>0$
\end_inset


\begin_inset Formula 
\begin{align*}
A^{\intercal}\left(z+\varepsilon y\right) & =0\\
z+\varepsilon y & \succeq0\text{ since }y\succeq0\\
b^{\intercal}\left(z+\varepsilon y\right) & =\delta+\varepsilon b^{\intercal}y>\delta
\end{align*}

\end_inset

so 
\begin_inset Formula $\left(z+\varepsilon y\right)$
\end_inset

 is a feasible solution of the dual with value greater than 
\begin_inset Formula $\delta,$
\end_inset

 a contradiction.
\end_layout

\begin_layout Case
Otherwise scale 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

 such that 
\begin_inset Formula $\lambda=1$
\end_inset

 (since both 
\begin_inset Formula $y,\lambda$
\end_inset

 are nonegative) and so 
\begin_inset Formula $A^{\intercal}y=c$
\end_inset

 and 
\begin_inset Formula $b^{\intercal}y>\delta$
\end_inset

.
 This means 
\begin_inset Formula $y$
\end_inset

 is a solution of the dual with value greater than 
\begin_inset Formula $\delta$
\end_inset

, a contradiction.
 
\end_layout

\end_deeper
\begin_layout Proof
Therefore 
\begin_inset Formula $P'$
\end_inset

 is feasible, so the primal is feasible with value at most 
\begin_inset Formula $\delta$
\end_inset

.
 By weak duality its value is at least 
\begin_inset Formula $\delta$
\end_inset

.
 Hence the primal solution and the dual solution are equal.
\end_layout

\end_deeper
\begin_layout Standard
Another way to look at it is a generalization of Lagrange multipliers: consider
 the following constained minimization problem 
\begin_inset Formula 
\begin{align*}
\max_{x} & x^{2}+y^{2}\\
\text{s.t.} & x+y=1
\end{align*}

\end_inset

Let 
\begin_inset Formula $L\left(x,y,\lambda\right)=x^{2}+y^{2}+\lambda\left(1-x-y\right)$
\end_inset

.
 Think of solving the original problem by, instead of enforcing 
\begin_inset Formula $x+y=1$
\end_inset

, allow it to be violated and associate a cost 
\begin_inset Formula $\lambda\left(1-x-y\right)$
\end_inset

, with cost rate 
\begin_inset Formula $\lambda$
\end_inset

.
 This is then an unconstrained minimization problem over 
\begin_inset Formula $x,y,\lambda$
\end_inset

: first minimize with respect to 
\begin_inset Formula $x,y$
\end_inset


\begin_inset Formula 
\[
\nabla_{x,y}L=\left(2x-\lambda x,2y-\lambda y\right)=0
\]

\end_inset

Solving for 
\begin_inset Formula $x,y$
\end_inset

 we get that
\begin_inset Formula $x=y=\frac{p}{2}$
\end_inset

.
 Then the constraint 
\begin_inset Formula $x+y=1$
\end_inset

 gives the additional relation 
\begin_inset Formula $p=1$
\end_inset

 and hence the optimal solution to the original problem is 
\begin_inset Formula $x=y=1/2$
\end_inset

.
 Another way of interpreting this is: when the costrate is properly chosen
 (
\begin_inset Formula $p=1$
\end_inset

) the optimal solution to the constrained problem is also the optimal solution
 to the unconstrained problem.
\end_layout

\begin_layout Standard
For LPs consider the standard form problem
\begin_inset Formula 
\begin{align*}
\min_{x} & c^{\intercal}x\\
\text{s.t.} & Ax=b\\
 & x\succeq0
\end{align*}

\end_inset

called the primal problem.
 Assume an optimal 
\begin_inset Formula $x^{*}$
\end_inset

 exists.
 A relaxed problem is
\begin_inset Formula 
\begin{align*}
\min_{x,\lambda} & c^{\intercal}x+\lambda^{\intercal}\left(b-Ax\right)\\
\text{s.t.} & x\succeq0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Why is this a relaxed problem? Well there are fewer constraints for one,
 and it turns out the objective always lower than the optimal value of the
 primal:
\begin_inset Formula 
\[
g\left(\lambda\right)=\min_{x\succeq0}\left[c^{\intercal}x+\lambda^{\intercal}\left(b-Ax\right)\right]\leq c^{\intercal}x^{*}+\lambda^{\intercal}\left(b-Ax^{*}\right)
\]

\end_inset

where the inequality follows because of the min.
 Then 
\begin_inset Formula $c^{\intercal}x^{*}+\lambda^{\intercal}\left(b-Ax^{*}\right)=c^{\intercal}x^{*}$
\end_inset

 since 
\begin_inset Formula $x^{*}$
\end_inset

 is assumed to satisfy the original system.
 Therefore 
\begin_inset Formula $g\left(\lambda\right)$
\end_inset

 is always a lower bound for the primal and 
\series bold
maximizing
\series default
 over 
\begin_inset Formula $\lambda$
\end_inset

 then yields the tightest lower bound.
 What does this look like?
\begin_inset Formula 
\begin{align*}
g\left(\lambda\right) & =\min_{x\succeq0}\left[c^{\intercal}x+\lambda^{\intercal}\left(b-Ax\right)\right]\\
 & =\lambda^{\intercal}b+\min_{x\succeq0}\left(c^{\intercal}-\lambda^{\intercal}A\right)x
\end{align*}

\end_inset

Note that 
\begin_inset Formula $\min_{x\succeq0}\left(c^{\intercal}-\lambda^{\intercal}A\right)x=\min_{x\succeq0}\left(c^{'}\right)^{\intercal}x$
\end_inset

 is an LP over the positive orthant polyhedron.
 So if 
\begin_inset Formula $\left(c^{\intercal}-\lambda^{\intercal}A\right)\succeq0$
\end_inset

 then the minimum is at 
\begin_inset Formula $x=0$
\end_inset

, otherwise if there exists a coordinate of 
\begin_inset Formula $\left(c_{i}^{\intercal}-\lambda^{\intercal}A_{i}\right)<0$
\end_inset

 then we can crank 
\begin_inset Formula $x_{i}$
\end_inset

 arbitrarily large and the problem is unbounded.
 Hence 
\begin_inset Formula 
\[
\min_{x\succeq0}\left(c^{\intercal}-\lambda^{\intercal}A\right)x=\begin{cases}
0 & \text{if }\left(c^{\intercal}-\lambda^{\intercal}A\right)\succeq0\\
-\infty & \text{o/w}
\end{cases}
\]

\end_inset

Clearly maximizing 
\begin_inset Formula $g\left(\lambda\right)$
\end_inset

 can only happen when the inner minimization isn't equal to 
\begin_inset Formula $-\infty$
\end_inset

.
 So
\begin_inset Formula 
\begin{align*}
\max_{\lambda} & \lambda^{\intercal}b\\
\text{s.t.} & \left(c^{\intercal}-\lambda^{\intercal}A\right)\succeq0
\end{align*}

\end_inset

or
\begin_inset Formula 
\begin{align*}
\max_{\lambda} & \lambda^{\intercal}b\\
\text{s.t.} & \lambda^{\intercal}A\preceq c^{\intercal}\\
 & \lambda\text{ free}
\end{align*}

\end_inset

Compare with
\begin_inset Formula 
\begin{align*}
\min_{x} & c^{\intercal}x\\
\text{s.t.} & Ax=b\\
 & x\succeq0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In general with 
\begin_inset Formula $A$
\end_inset

 rows 
\begin_inset Formula $a_{i}$
\end_inset

 and columns 
\begin_inset Formula $A_{j}$
\end_inset


\begin_inset Formula 
\[
\begin{array}{cccccc}
\min_{x} & c^{\intercal}x &  & \max_{p} & p^{\intercal}b\\
\text{s.t.} & a_{i}x\geq b_{i}\quad & i\in M_{1} &  & p_{i}\geq0 & i\in M_{1}\\
 & a_{i}x\leq b_{i}\quad & i\in M_{2} &  & p_{i}\leq0 & i\in M_{2}\\
 & a_{i}x=b_{i}\quad & i\in M_{3} &  & p_{i}\text{ free} & i\in M_{3}\\
 & x_{j}\geq0 & j\in N_{1} &  & p^{\intercal}A_{j}\leq c_{j}\quad & j\in N_{1}\\
 & x_{j}\leq0 & j\in N_{2} &  & p^{\intercal}A_{j}\geq c_{j}\quad & j\in N_{2}\\
 & x_{j}\text{ free} & j\in N_{3} &  & p^{\intercal}A_{j}=c_{j}\quad & j\in N_{3}
\end{array}
\]

\end_inset

Why? Consider 
\begin_inset Formula 
\begin{align*}
\min_{x} & c^{\intercal}x\\
\text{s.t.} & Ax\preceq b\\
 & x\succeq0
\end{align*}

\end_inset

Then 
\begin_inset Formula 
\begin{align*}
\min_{x} & c^{\intercal}x\\
\text{s.t.} & 0\preceq\left(b-Ax\right)\\
 & x\succeq0
\end{align*}

\end_inset

and
\begin_inset Formula 
\[
g\left(\lambda\right)=\min_{x\succeq0}\left[c^{\intercal}x+\lambda^{\intercal}\left(b-Ax\right)\right]\leq c^{\intercal}x^{*}+\lambda^{\intercal}\left(b-Ax^{*}\right)\leq c^{\intercal}x^{*}
\]

\end_inset

iff 
\begin_inset Formula $\lambda\preceq0$
\end_inset

 since 
\begin_inset Formula $\left(b-Ax^{*}\right)\succeq0$
\end_inset

.
 Then the rest of the proof goes through the same.
 What about if 
\begin_inset Formula 
\begin{align*}
\min_{x} & c^{\intercal}x\\
\text{s.t.} & Ax\preceq b\\
 & x\preceq0
\end{align*}

\end_inset

Then 
\begin_inset Formula 
\[
\min_{x\preceq0}\left(c^{\intercal}-\lambda^{\intercal}A\right)x=\begin{cases}
-\infty & \text{if }\left(c^{\intercal}-\lambda^{\intercal}A\right)\not\preceq0\\
0 & \text{o/w}
\end{cases}
\]

\end_inset

i.e.
 if there exists a component of 
\begin_inset Formula $\left(c^{\intercal}-\lambda^{\intercal}A\right)$
\end_inset

 that's positive (because then we could crank that components to negative
 infinity).
 Therefore 
\begin_inset Formula $\left(c^{\intercal}-\lambda^{\intercal}A\right)\preceq0$
\end_inset

 or 
\begin_inset Formula 
\begin{align*}
\max_{\lambda} & \lambda^{\intercal}b\\
\text{s.t.} & \left(c^{\intercal}-\lambda^{\intercal}A\right)\preceq0
\end{align*}

\end_inset

or
\begin_inset Formula 
\begin{align*}
\max_{\lambda} & \lambda^{\intercal}b\\
\text{s.t.} & c^{\intercal}\preceq\lambda^{\intercal}A
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Supporting hyperplanes
\end_layout

\begin_layout Standard
A hyperplane supports a set if it intersects the set and the entire set
 is on one side of the set.
\end_layout

\begin_layout Theorem*
Supporting hyperplane theorem: if 
\begin_inset Formula $S$
\end_inset

 is a convex set then there exists a supporting hyperplane.
 I.e.
 for every 
\begin_inset Formula $\bar{x}\in\partial S$
\end_inset

 there exists 
\begin_inset Formula $p\neq0$
\end_inset

 such that 
\begin_inset Formula $p^{\intercal}\left(x-\bar{x}\right)\leq0$
\end_inset

 for all 
\begin_inset Formula $x\in cl\left(S\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
Since 
\begin_inset Formula $\bar{x}\in\partial S$
\end_inset

 there exists a sequence 
\begin_inset Formula $\left\{ y_{k}\right\} $
\end_inset

 not in 
\begin_inset Formula $cl\left(S\right)$
\end_inset

 such that 
\begin_inset Formula $y_{k}\rightarrow\bar{x}$
\end_inset

 (since 
\begin_inset Formula $\partial S$
\end_inset

 is also the boundary of the complement of 
\begin_inset Formula $S$
\end_inset

).
 By the point separation theorem there exists a 
\begin_inset Formula $p_{k}$
\end_inset

 (that we can normalize) such that 
\begin_inset Formula 
\[
p_{k}^{\intercal}\left(y_{k}-x\right)>0\iff p_{k}^{\intercal}y_{k}>p_{k}^{\intercal}x
\]

\end_inset

for each 
\begin_inset Formula $x\in cl\left(S\right)$
\end_inset

.
 Since 
\begin_inset Formula $\left\{ p_{k}\right\} $
\end_inset

 are bounded there exists a convergent subsequence 
\begin_inset Formula $p_{k_{j}}$
\end_inset

 with a limit 
\begin_inset Formula $p$
\end_inset

 whose norm is equal to 1.
 Taking both limits simultaneously we get that 
\begin_inset Formula $p^{\intercal}\left(\bar{x}-x\right)\geq0$
\end_inset

 or 
\begin_inset Formula $p^{\intercal}\left(x-\bar{x}\right)\leq0$
\end_inset

.
 Basically you want to construct the hyperplane that goes through the point
 on the boundary but using the point set separation theorem takes using
 a limit to hone in on it (i.e.
 the plane should have wellbehaved properties under taking limit of all
 the planes for points not in).
\end_layout

\end_deeper
\begin_layout Corollary*
For a nonempty convex set 
\begin_inset Formula $S$
\end_inset

 if 
\begin_inset Formula $x\notin int\left(S\right)$
\end_inset

 then there exists a separating hyperplane.
\end_layout

\begin_deeper
\begin_layout Proof
If 
\begin_inset Formula $x\notin cl\left(S\right)$
\end_inset

 then just use point set separation.
 Otherwise just use the immediately previous theorem.
\end_layout

\end_deeper
\begin_layout --Separator--

\end_layout

\begin_layout Corollary*
For a nonempty 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $y\notin int\left(conv\left(S\right)\right)$
\end_inset

 there exists a separating hyperplane that separates 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
By immediately prior since 
\begin_inset Formula $conv\left(S\right)$
\end_inset

 is convex.
\end_layout

\end_deeper
\begin_layout --Separator--

\end_layout

\begin_layout Corollary*
Let 
\begin_inset Formula $S_{1},S_{2}$
\end_inset

 be two nonempty convex sets such that 
\begin_inset Formula $S_{1}\cap S_{2}=\emptyset$
\end_inset

.
 Then there exists a separating hyperplane, i.e.
 there exists 
\begin_inset Formula $p$
\end_inset

 such that
\begin_inset Formula 
\[
\inf\left\{ p^{\intercal}x\big|x\in S_{1}\right\} \geq\sup\left\{ p^{\intercal}x\big|x\in S_{2}\right\} 
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Proof
Let 
\begin_inset Formula $S=S_{1}\ominus S_{2}$
\end_inset

.
 Note that 
\begin_inset Formula $S$
\end_inset

 is convex and 
\begin_inset Formula $0\in S$
\end_inset

 (since otherwise 
\begin_inset Formula $S_{1}\cap S_{2}$
\end_inset

 would nonempty).
 By the first corollary there exists a separating hyperplane between 
\begin_inset Formula $S$
\end_inset

 and 0, i.e.
 
\begin_inset Formula $p^{\intercal}\left(0-x\right)\leq0$
\end_inset

 which is the same as 
\begin_inset Formula $p^{\intercal}x\geq0$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

 which is the same 
\begin_inset Formula 
\[
p^{\intercal}x_{1}\geq p^{\intercal}x_{2}
\]

\end_inset

for all 
\begin_inset Formula $x_{1}\in S_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}\in S_{2}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout --Separator--

\end_layout

\begin_layout Corollary*
Let 
\begin_inset Formula $S_{1},S_{2}$
\end_inset

 be two nonempty convex sets such that 
\begin_inset Formula $S_{1}\cap int\left(S_{2}\right)=\emptyset$
\end_inset

 and 
\begin_inset Formula $int\left(S_{2}\right)\neq\emptyset$
\end_inset

.
 Then there exists a separating hyperplane.
\end_layout

\begin_deeper
\begin_layout Proof
Interior of nonempty convex sets are convex so apply the previous result.
\end_layout

\end_deeper
\begin_layout --Separator--

\end_layout

\begin_layout Corollary*
Let 
\begin_inset Formula $S_{1},S_{2}$
\end_inset

 be two nonempty closed convex sets such that 
\begin_inset Formula $S_{1}$
\end_inset

 is bounded and 
\begin_inset Formula $S_{1}\cap S_{2}=\emptyset$
\end_inset

.
 Then there exists a separating hyperplane that strongly separates.
\end_layout

\begin_layout Section
Inner Representation of convex sets
\end_layout

\begin_layout Subsection
Extreme points
\end_layout

\begin_layout Standard
Consider the polyhedral set 
\begin_inset Formula $S=\left\{ x\big|Ax=b,x\succeq0\right\} $
\end_inset

 where 
\begin_inset Formula $A\in\mathbb{R}^{m\times n}$
\end_inset

.
 Assume 
\begin_inset Formula $\text{rank}\left(A\right)=m$
\end_inset

.
 If not, assuming 
\begin_inset Formula $Ax=b$
\end_inset

 is consistent, you can throw away linearly dependent rows in order to get
 a full row rank matrix.
 Rearrange the columns of 
\begin_inset Formula $A$
\end_inset

 so that 
\begin_inset Formula $A=\left[B,N\right]$
\end_inset

 where 
\begin_inset Formula $B\in\mathbb{R}^{m\times m}$
\end_inset

 and full rank and 
\begin_inset Formula $N\in\mathbb{R}^{m\times\left(n-m\right)}$
\end_inset

 is the rest of the matrix.
 Then 
\begin_inset Formula 
\begin{align*}
Ax & =Bx_{B}+Nx_{N}=b\\
 & x_{B}\succeq0\\
 & x_{N}\succeq0
\end{align*}

\end_inset


\end_layout

\begin_layout Theorem*
x is an extreme point of 
\begin_inset Formula $S$
\end_inset

 iff 
\begin_inset Formula $A$
\end_inset

 can be decomposed into 
\begin_inset Formula $\left[B,N\right]$
\end_inset

 such that 
\begin_inset Formula 
\[
x=\begin{bmatrix}x_{B}\\
x_{N}
\end{bmatrix}=\begin{bmatrix}B^{-1}b\\
0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Proof
Why would this be true? A vertex (i.e.
 extreme point) of a polyhedron is a unique solution to a set of constraints.
 If it weren't unique then it would be free (a line) and therefore not a
 vertex.
 The question is how many constraints? For a polyhedron in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 it has to be at least 
\begin_inset Formula $n$
\end_inset

 constraints (a 
\begin_inset Quotes eld
\end_inset

point
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Formula $n-1$
\end_inset

 entries constrained and 
\begin_inset Formula $1$
\end_inset

 free is a line in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

).
 And the constraints have to be linearly independent (otherwise you could
 get rid of redundancies and you'd only be satisfying 
\begin_inset Formula $n'<n$
\end_inset

 constraints).
 But could there be more? For example 3 lines intersecting? There's definitely
 a unique point satisfying that (if no two are colinear) but one of the
 lines is linearly dependent on the other two, i.e.
 redundant.
 More than 
\begin_inset Formula $n$
\end_inset

 equations can't be linearly independent in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

.
 Therefore for a polyhedron in 
\begin_inset Formula $n$
\end_inset

 variables, i.e.
 some number of constraints on 
\begin_inset Formula $x\in\mathbb{R}^{n}$
\end_inset

, we need at least 
\begin_inset Formula $n$
\end_inset

 constraints to force a unique solution.
 If the rank of 
\begin_inset Formula $A$
\end_inset

 is 
\begin_inset Formula $m$
\end_inset

 then we can get a unique solution to 
\begin_inset Formula $m$
\end_inset

 constraints by solving that system of 
\begin_inset Formula $m$
\end_inset

 equations.
 Where do we get the rest from? We set the positivity constraints to be
 tight, i.e.
 
\begin_inset Formula $x_{i}=0$
\end_inset

.
 This partitioning of 
\begin_inset Formula $x=\left[x_{B},x_{N}\right]=\left[x_{B},0\right]$
\end_inset

 effects exactly this.
 Such solutions 
\begin_inset Formula $x$
\end_inset

 are called 
\series bold
basic feasible solutions
\series default
.
 
\end_layout

\begin_layout Proof
\begin_inset Formula $\Leftarrow$
\end_inset

Suppose that 
\begin_inset Formula $A$
\end_inset

 can decomposed 
\begin_inset Formula $A=\left[B,N\right]$
\end_inset

 with 
\begin_inset Formula $x=\begin{bmatrix}x_{B}\\
x_{N}
\end{bmatrix}$
\end_inset

 and 
\begin_inset Formula $x_{B}\succeq0$
\end_inset

.
 Then immediately 
\begin_inset Formula $x\in S$
\end_inset

.
 Suppose, towards a contradiction, that 
\begin_inset Formula $x=\lambda x_{1}+\left(1-\lambda\right)x_{2}$
\end_inset

 for some 
\begin_inset Formula $x_{1},x_{2}\in S$
\end_inset

 for some 
\begin_inset Formula $\lambda\in\left(0,1\right)$
\end_inset

, i.e.
 
\begin_inset Formula $x$
\end_inset

 is not an extreme point.
 Then 
\begin_inset Formula 
\[
x_{1}=\begin{bmatrix}x_{1B}\\
x_{1N}
\end{bmatrix},x_{2}=\begin{bmatrix}x_{2B}\\
x_{2N}
\end{bmatrix}
\]

\end_inset

and 
\begin_inset Formula 
\[
\begin{bmatrix}B^{-1}b\\
0
\end{bmatrix}=\lambda\begin{bmatrix}x_{1B}\\
x_{1N}
\end{bmatrix}+\left(1-\lambda\right)\begin{bmatrix}x_{2B}\\
x_{2N}
\end{bmatrix}
\]

\end_inset

Then 
\begin_inset Formula $0=\lambda x_{1N}+\left(1-\lambda\right)x_{2N}$
\end_inset

 and 
\begin_inset Formula $\lambda\in\left(0,1\right)$
\end_inset

 forces 
\begin_inset Formula $x_{1N}=x_{2N}=0$
\end_inset

.
 But then 
\begin_inset Formula $\lambda x_{1B}+\left(1-\lambda\right)x_{2B}=B^{-1}b$
\end_inset

 and 
\begin_inset Formula $B^{-1}b$
\end_inset

 is unique and hence 
\begin_inset Formula $x_{1B}=x_{2B}$
\end_inset

 and 
\begin_inset Formula $x_{1}=x_{2}$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 is therefore an extreme point.
 
\end_layout

\begin_layout Proof
\begin_inset Formula $\Rightarrow$
\end_inset

Suppose 
\begin_inset Formula $x$
\end_inset

 is an extreme point (vertex).
 Without loss of generality 
\begin_inset Formula $x=\left(x_{1},\dots,x_{k},0,\dots,0\right)$
\end_inset

 where 
\begin_inset Formula $x_{i}\geq0$
\end_inset

 (we allow 
\begin_inset Formula $k=n$
\end_inset

).
 Firstly 
\begin_inset Formula $a_{1},\dots,a_{k}$
\end_inset

 are linearly independent: 
\end_layout

\begin_deeper
\begin_layout Proof
Towards a contradiction, suppose 
\begin_inset Formula $\sum_{j=1}^{k}\lambda_{j}a_{j}=0$
\end_inset

 with 
\begin_inset Formula $\lambda_{j}$
\end_inset

 not all equal to 0.
 Let 
\begin_inset Formula 
\[
\lambda=\left(\lambda_{1},\dots,\lambda_{k},0,\dots,0\right)
\]

\end_inset

and 
\begin_inset Formula $\alpha>0$
\end_inset

 such that 
\begin_inset Formula $x_{1},x_{2}\succeq0$
\end_inset

 and 
\begin_inset Formula 
\[
x_{1}=x+\alpha\lambda\text{ and }x_{2}=x-\alpha\lambda
\]

\end_inset

Note that 
\begin_inset Formula 
\[
Ax_{1}=Ax+\alpha A\lambda=Ax+\alpha\sum_{j=1}^{k}\lambda_{i}a_{j}=b
\]

\end_inset

and similarly 
\begin_inset Formula $Ax_{2}=b$
\end_inset

.
 Therefore 
\begin_inset Formula $x_{1},x_{2}\in S$
\end_inset

 and since 
\begin_inset Formula $\alpha>0$
\end_inset

 and 
\begin_inset Formula $\lambda\neq0$
\end_inset

 and 
\begin_inset Formula $x_{1}\neq x_{2}$
\end_inset

 and 
\begin_inset Formula $x=\left(1/2\right)x_{1}+\left(1/2\right)x_{2}$
\end_inset

 contradicting that 
\begin_inset Formula $x$
\end_inset

 is an extreme point.
\end_layout

\end_deeper
\begin_layout Proof
Thus 
\begin_inset Formula $a_{1},\dots,a_{k}$
\end_inset

 are linearly independent and since 
\begin_inset Formula $A$
\end_inset

 has rank 
\begin_inset Formula $m$
\end_inset

, 
\begin_inset Formula $m-k$
\end_inset

 of the last 
\begin_inset Formula $n-k$
\end_inset

 columns may be chosen such that they, together with the first 
\begin_inset Formula $k$
\end_inset

 columns, form a linearly independent set of 
\begin_inset Formula $m$
\end_inset

 vectors; suppose 
\begin_inset Formula $a_{k+1},\dots,a_{m}$
\end_inset

 are these columns.
 Therefore 
\begin_inset Formula 
\begin{align*}
A & =\left[\left[a_{1},\dots,a_{m}\right],N\right]\\
 & =\left[B,N\right]
\end{align*}

\end_inset

where 
\begin_inset Formula $B$
\end_inset

 is full rank 
\begin_inset Formula $m$
\end_inset

.
 Furthermore for 
\begin_inset Formula $x=\left(x_{1},\dots,x_{k},0\right)$
\end_inset


\begin_inset Formula 
\[
Ax=Bx+N0=b
\]

\end_inset

and therefore 
\begin_inset Formula $\left(x,0\right)=\left(B^{-1}b,0\right)$
\end_inset

 and since 
\begin_inset Formula $x_{j}>0$
\end_inset

 it's the case that 
\begin_inset Formula $B^{-1}b\succeq0$
\end_inset

.
\end_layout

\begin_layout Corollary*
The number of extreme points of a polyhedron is less than or equal to
\begin_inset Formula 
\[
{n \choose n-m}={n \choose m}
\]

\end_inset

because you can choose 
\begin_inset Formula $n-m$
\end_inset

 constraints to set to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Corollary*
Let 
\begin_inset Formula $S=\left\{ x\big|Ax=b,x\succeq0\right\} $
\end_inset

 be nonempty and 
\begin_inset Formula $A\in\mathbb{R}^{m\times n}$
\end_inset

 and 
\begin_inset Formula $\text{rank}\left(A\right)=m$
\end_inset

.
 Then 
\begin_inset Formula $S$
\end_inset

 has at least one extreme point.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $x\in S$
\end_inset

, i.e.
 
\begin_inset Formula $Ax=b$
\end_inset

, and without loss of generality, suppose that 
\begin_inset Formula $x=\left(x_{1},\dots,x_{k},0,\dots,0\right)$
\end_inset

 where 
\begin_inset Formula $x_{j}>0$
\end_inset

.
 If 
\begin_inset Formula $a_{1},\dots,a_{k}$
\end_inset

 are linearly independent then 
\begin_inset Formula $k\leq m$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 is an extreme point (since 
\begin_inset Formula $x$
\end_inset

 is a unique solution to 
\begin_inset Formula $A\left(x_{1},\dots,x_{k},0\right)=b$
\end_inset

).
 Otherwise 
\begin_inset Formula $\sum_{j=1}^{k}\lambda_{j}a_{j}=0$
\end_inset

.
 Let 
\begin_inset Formula 
\[
\alpha=\min_{1\leq j\leq k}\left\{ \frac{x_{j}}{\lambda_{j}}\bigg|\lambda_{j}>0\right\} =\frac{x_{i}}{\lambda_{i}}
\]

\end_inset

Consider the point 
\begin_inset Formula $x'$
\end_inset

 such that 
\begin_inset Formula 
\[
x_{j}^{'}=\begin{cases}
x_{j}-\alpha\lambda_{j} & j=1,\dots,k\\
0 & j=k+1,\dots,n
\end{cases}
\]

\end_inset

Note that 
\begin_inset Formula $x_{i}^{'}=0$
\end_inset

 and
\begin_inset Formula 
\[
\sum_{j=1}^{n}a_{j}x_{j}^{'}=\sum_{j=1}^{n}a_{j}\left(x_{j}-\alpha\lambda_{j}\right)=b-0=b
\]

\end_inset

Thus 
\begin_inset Formula $x'$
\end_inset

 is feasible and has at most 
\begin_inset Formula $k-1$
\end_inset

 positive components.
 Repeat the process until the number of components corresponds to the number
 of linearly independent columns in 
\begin_inset Formula $A$
\end_inset

 and you have an extreme point.
\end_layout

\begin_layout Standard
Every basic feasible solution corresponds to an extreme point but an extreme
 point might be represented by several extreme points.
 The number of extreme points of a polytope is 
\begin_inset Formula ${n \choose m}$
\end_inset

 where 
\begin_inset Formula $\text{rank}\left(A\right)=m$
\end_inset

.
 
\end_layout

\begin_layout Standard
A 
\series bold
recession direction
\series default
 is 
\begin_inset Formula $d$
\end_inset

 such that 
\begin_inset Formula $x+\lambda d\in P$
\end_inset

 for all 
\begin_inset Formula $\lambda\geq0$
\end_inset

.
 A recession direction is one such that 
\begin_inset Formula 
\[
d\neq\lambda_{1}d_{1}+\lambda_{2}d_{2}
\]

\end_inset

for any distinct directions.
 The set of all recession directions is a cone.
 If 
\begin_inset Formula $P=\left\{ x\big|Ax=b,x\succeq0\right\} $
\end_inset

 then 
\begin_inset Formula $rec\left(P\right)=\left\{ d\big|Ad=0,x\succeq0\right\} $
\end_inset

, since 
\begin_inset Formula 
\[
A(x+\lambda d)=Ax+\lambda Ad)=b
\]

\end_inset

for all 
\begin_inset Formula $\lambda$
\end_inset

 necessitates that 
\begin_inset Formula $Ad=0$
\end_inset

.
 Also 
\begin_inset Formula $d$
\end_inset

 is a recession direction iff it's in the recession cone.
 To find extreme directions find the extreme points (i.e.
 basic feasible solutions of the system 
\begin_inset Formula $Ad=0$
\end_inset

).
 An extreme ray is the entire ray while an extreme direction is just the
 direction.
\end_layout

\begin_layout Theorem*
Characterization of extreme directions.
 Let 
\begin_inset Formula $S=\left\{ x\big|Ax=b,x\succeq0\right\} $
\end_inset

 be nonempty and 
\begin_inset Formula $A\in\mathbb{R}^{m\times n}$
\end_inset

 and 
\begin_inset Formula $\text{rank}\left(A\right)=m$
\end_inset

.
 Then 
\begin_inset Formula $\bar{d}$
\end_inset

 is an extreme direction of 
\begin_inset Formula $S$
\end_inset

 iff 
\begin_inset Formula $A=\left[B,N\right]$
\end_inset

 such that 
\begin_inset Formula $B^{-1}a_{j}\preceq0$
\end_inset

 for some column of 
\begin_inset Formula $N$
\end_inset

 and 
\begin_inset Formula 
\[
\bar{d}=\lambda\begin{pmatrix}-B^{-1}a_{j}\\
e_{j}
\end{pmatrix}
\]

\end_inset

with 
\begin_inset Formula $\lambda>0$
\end_inset

.
 
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem*
Minkowski's theorem (representation theorem): every point in a polyhedron
 can be represented as the 
\series bold
convex
\series default
 combination of extreme points plus 
\series bold
conic
\series default
 combination of extreme directions, i.e.
 
\begin_inset Formula 
\[
x=\sum_{j=1}^{k}\lambda_{j}x_{j}+\sum_{j=1}^{l}\mu_{j}d_{j}
\]

\end_inset

where 
\begin_inset Formula $x_{j}$
\end_inset

 are extreme points and 
\begin_inset Formula $d_{j}$
\end_inset

 are extreme directions and 
\begin_inset Formula $\sum_{j=1}^{k}\lambda_{j}=1$
\end_inset

 and 
\begin_inset Formula $\mu_{j},\lambda_{j}\geq0$
\end_inset

.
 This is the inner representation of the polygon.
\end_layout

\begin_layout Corollary*
If 
\begin_inset Formula $S$
\end_inset

 is nonempty of the form 
\begin_inset Formula $\left\{ x\big|Ax=b,x\succeq0\right\} $
\end_inset

, i.e.
 a nonempty polyhedron.
 Then 
\begin_inset Formula $S$
\end_inset

 has at least one extreme direction iff it's unbounded.
\end_layout

\begin_layout Proof
If 
\begin_inset Formula $S$
\end_inset

 has no extreme directions then by Minkowski's representation theorem (and
 Cauchy-Schwartz since 
\begin_inset Formula $\lambda_{j}\geq0$
\end_inset

)
\begin_inset Formula 
\[
\left\Vert x\right\Vert =\left\Vert \sum_{j=1}^{k}\lambda_{j}x_{j}\right\Vert \leq\sum_{j=1}^{k}\lambda_{j}\left\Vert x_{j}\right\Vert =\sum_{j=1}^{k}\left\Vert x_{j}\right\Vert 
\]

\end_inset

for all 
\begin_inset Formula $x\in S$
\end_inset

.
 Therefore 
\begin_inset Formula $S$
\end_inset

 is bounded.
 If 
\begin_inset Formula $S$
\end_inset

 has an extreme direction then obviously it's unbounded.
\end_layout

\begin_layout Corollary*
The linear program 
\begin_inset Formula $\mathcal{P}$
\end_inset


\begin_inset Formula 
\begin{align*}
\min_{x} & c^{\intercal}x\\
\text{s.t.} & Ax=b\\
 & x\succeq0
\end{align*}

\end_inset

with nonempty feasible region.
 Let 
\begin_inset Formula $\left\{ x_{j}\right\} $
\end_inset

 be the set of extreme points of the feasible region and 
\begin_inset Formula $\left\{ d_{j}\right\} $
\end_inset

 be the set of extreme directions.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathcal{P}$
\end_inset

 has a finite optimal solution iff 
\begin_inset Formula $c^{\intercal}d_{j}\geq0$
\end_inset

, i.e.
 the objective normal makes an acute angle with each extreme direction.
 Why does this make sense? For a minimization LP 
\begin_inset Formula $c$
\end_inset

 is opposite of the direction in which the objective increases.
 If there exists 
\begin_inset Formula $c^{\intercal}d_{j}<0$
\end_inset

 then 
\begin_inset Formula $\left(-c\right)^{\intercal}d_{j}>0$
\end_inset

 and therefore going in the direction 
\begin_inset Formula $d_{j}$
\end_inset

 decreases the objective arbitrarily.
\end_layout

\begin_layout Enumerate
If no extreme directions ruin it then there exists an extreme point 
\begin_inset Formula $x_{j}$
\end_inset

 that's optimal.
\end_layout

\begin_layout Proof
By representation theorem 
\begin_inset Formula $Ax=b$
\end_inset

 and 
\begin_inset Formula $x\succeq0$
\end_inset

 iff 
\begin_inset Formula 
\begin{align*}
x & =\sum_{j=1}^{k}\lambda_{j}x_{j}+\sum_{j=1}^{l}\mu_{j}d_{j}\\
 & \in\text{conv}\left(x_{1},\dots,x_{k}\right)\cup\text{coni}\left(d_{1},\dots,d_{l}\right)
\end{align*}

\end_inset

i.e.
 
\begin_inset Formula $\sum_{j=1}^{k}\lambda_{j}=1$
\end_inset

, 
\begin_inset Formula $\lambda_{j}\geq0$
\end_inset

, 
\begin_inset Formula $\mu_{j}\geq0$
\end_inset

 and the LP ca be re-expressed as 
\begin_inset Formula 
\begin{align*}
\min_{\boldsymbol{\lambda},\boldsymbol{\mu}} & c^{\intercal}\left(\sum_{j=1}^{k}\lambda_{j}x_{j}+\sum_{j=1}^{l}\mu_{j}d_{j}\right)\\
\text{s.t.} & \sum_{i=1}^{k}\lambda_{i}=1\\
 & \lambda_{i}\geq0\\
 & \mu_{j}\geq0
\end{align*}

\end_inset

So if 
\begin_inset Formula $c^{\intercal}d_{q}<0$
\end_inset

 for some 
\begin_inset Formula $q$
\end_inset

 then 
\begin_inset Formula 
\[
c^{\intercal}\left(\sum_{j=1}^{k}\lambda_{j}x_{j}+\sum_{j=1}^{l}\mu_{j}d_{j}\right)=c^{\intercal}\left(\sum_{j=1}^{k}\lambda_{j}x_{j}+\sum_{\underset{j\neq q}{j=1}}^{l}\mu_{j}d_{j}\right)+\mu_{q}\left(c^{\intercal}d_{q}\right)
\]

\end_inset

and so 
\begin_inset Formula $u_{q}$
\end_inset

 could be chosen arbitrarily large in order to decrease the objective.
 So feasibility iff 
\begin_inset Formula $c^{\intercal}d_{j}\geq0$
\end_inset

 for all 
\begin_inset Formula $j=1,\dots,l$
\end_inset

.
 Therefore
\begin_inset Formula 
\[
\min_{\boldsymbol{\mathbf{\lambda}}}\,c^{\intercal}\left(\sum_{j=1}^{k}\lambda_{j}x_{j}\right)\leq\min_{\boldsymbol{\lambda},\boldsymbol{\mu}}\,c^{\intercal}\left(\sum_{j=1}^{k}\lambda_{j}x_{j}+\sum_{j=1}^{l}\mu_{j}d_{j}\right)
\]

\end_inset

(again since 
\begin_inset Formula $\mu_{j}c^{\intercal}d_{j}\geq0$
\end_inset

 for all 
\begin_inset Formula $j$
\end_inset

), i.e.
 choose 
\begin_inset Formula $\mu_{j}=0$
\end_inset

 for all 
\begin_inset Formula $j$
\end_inset

.
 Thus
\begin_inset Formula 
\[
\min_{\boldsymbol{\mathbf{\lambda}}}\,c^{\intercal}\left(\sum_{j=1}^{k}\lambda_{j}x_{j}\right)
\]

\end_inset

Clearly this is minimized by choosing 
\begin_inset Formula $\lambda_{q}=1$
\end_inset

 for 
\begin_inset Formula $q$
\end_inset

 such that 
\begin_inset Formula $c^{\intercal}x_{q}=\min_{1\leq j\leq k}c^{\intercal}x_{j}$
\end_inset

.
 Why?
\begin_inset Formula 
\begin{align*}
\min_{\boldsymbol{\mathbf{\lambda}}}\,c^{\intercal}\left(\sum_{j=1}^{k}\lambda_{j}x_{j}\right)\geq & \min_{\boldsymbol{\mathbf{\lambda}}}\,c^{\intercal}\left(\sum_{j=1}^{k}\lambda_{j}x_{q}\right)\\
 & =\min_{\boldsymbol{\mathbf{\lambda}}}\,c^{\intercal}x_{q}\left(\sum_{j=1}^{k}\lambda_{j}\right)\\
 & =c^{\intercal}x_{q}\times1
\end{align*}

\end_inset

 The smallest weighted average is found by putting all of the weight on
 the smallest object (since all the objects are positive).
\end_layout

\begin_layout Subsection
Exposed points
\end_layout

\begin_layout Definition*
Let 
\begin_inset Formula $C$
\end_inset

 be a nonempty closed convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

.
 
\begin_inset Formula $x^{*}\in C$
\end_inset

 is called an exposed solution if there exists a linear objective 
\begin_inset Formula $f\left(x\right)=c^{t}x$
\end_inset

 for which 
\begin_inset Formula $x^{*}=\min_{x\in C}f\left(X\right)$
\end_inset

.
\end_layout

\begin_layout Theorem*
Straszewicz's theorem.
 For any closed convex set 
\begin_inset Formula $C$
\end_inset

, the set of exposed solutions of 
\begin_inset Formula $C$
\end_inset

 is a dense subset of the set of extreme points of 
\begin_inset Formula $C$
\end_inset

.
 Thus every extreme point of 
\begin_inset Formula $C$
\end_inset

 is the limit point of some sequence of exposed points.
\end_layout

\begin_layout Corollary*
Any closed bounded convex set 
\begin_inset Formula $C$
\end_inset

 can be expressed as the closure of the convex hull of its exposed points.
\end_layout

\begin_layout Section
Convex functions
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex subset of 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset


\end_layout

\begin_layout Definition*
A function 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 is said to be 
\emph on
convex
\emph default
 if for all 
\begin_inset Formula $x_{1},x_{2}\in S$
\end_inset

 and 
\begin_inset Formula $\lambda\in\left[0,1\right]$
\end_inset

 
\begin_inset Formula 
\[
f\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right)\leq\lambda f\left(x_{1}\right)+\left(1-\lambda\right)f\left(x_{2}\right)
\]

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Definition*
A function 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 is said to be 
\emph on
strictly convex
\emph default
 if for all 
\begin_inset Formula $x_{1},x_{2}\in S$
\end_inset

, 
\begin_inset Formula $x_{1}\ne x_{2}$
\end_inset

 and 
\begin_inset Formula $\lambda\in\left(0,1\right)$
\end_inset

 
\begin_inset Formula 
\[
f\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right)<\lambda f\left(x_{1}\right)+\left(1-\lambda\right)f\left(x_{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
A strictly convex function basically has no linear pieces.
\end_layout

\begin_layout Standard
Facts: 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\left\{ f_{i}\right\} $
\end_inset

 convex a conic 
\begin_inset Formula $\alpha_{j}>0$
\end_inset

 combination 
\begin_inset Formula $f\left(\mathbf{x}\right)=\sum_{j=1}^{k}\alpha_{j}f_{j}\left(\mathbf{x}\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $g$
\end_inset

 concave then 
\begin_inset Formula $f:\left\{ x\big|g\left(x\right)>0\right\} \rightarrow\mathbb{R}$
\end_inset

, i.e.
 
\begin_inset Formula $f\left(x\right)=1/g\left(x\right)$
\end_inset

 is convex.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $g$
\end_inset

 be nondecreasing, convex, and 
\begin_inset Formula $h$
\end_inset

 convex then 
\begin_inset Formula $f\left(x\right)=g\left(h\left(x\right)\right)$
\end_inset

 is convex.
 
\begin_inset Formula $g$
\end_inset

 has to be nondecreasing!
\end_layout

\begin_layout Enumerate
\begin_inset Formula $g$
\end_inset

 be convex and 
\begin_inset Formula $h\left(x\right)=Ax+b$
\end_inset

 then 
\begin_inset Formula $f\left(x\right)=g\left(Ax+b\right)$
\end_inset

 is convex.
\end_layout

\begin_layout Theorem*
Multivariable 
\begin_inset Formula $f$
\end_inset

 is convex iff 
\begin_inset Formula $f$
\end_inset

 is convex on any line, i.e.
 
\begin_inset Formula $F_{\bar{x},d}\left(\lambda\right)=f\left(\bar{x}+\lambda d\right)$
\end_inset

 is convex for all 
\begin_inset Formula $\bar{x},d\in\mathbb{R}^{n}$
\end_inset

 as a function of 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 a convex function.
 Then the 
\begin_inset Formula $\alpha$
\end_inset

-level-set of 
\begin_inset Formula $f$
\end_inset

 is a convex for each 
\begin_inset Formula $\alpha\in\mathbb{R}$
\end_inset

, i.e.
 
\begin_inset Formula $S_{\alpha}=\left\{ x\in S\big|f\left(x\right)\leq\alpha\right\} \subset\mathbb{R}^{n}$
\end_inset

 is convex.
 The 
\begin_inset Formula $\alpha$
\end_inset

-level-set is in the domain of the function.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $x_{1},x_{2}\in S_{\alpha}$
\end_inset

.
 Thus 
\begin_inset Formula $x_{1},x_{2}\in S$
\end_inset

 and 
\begin_inset Formula $f\left(x_{i}\right)\leq\alpha$
\end_inset

.
 Then
\begin_inset Formula 
\[
f\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right)\leq\lambda f\left(x_{1}\right)+\left(1-\lambda\right)f\left(x_{2}\right)\leq\lambda\alpha+\left(1-\lambda\right)\alpha=\alpha
\]

\end_inset


\end_layout

\begin_layout Definition*
Let 
\begin_inset Formula $S\subset\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

, then 
\begin_inset Formula $\left\{ \left(x,f\left(x\right)\right)\big|x\in S\right\} \subset\mathbb{R}^{n+1}$
\end_inset

 is the 
\emph on
graph
\emph default
 of 
\begin_inset Formula $f$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Definition*
Let 
\begin_inset Formula $S\subset\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $S\neq\emptyset$
\end_inset

.
 The 
\emph on
epigraph
\emph default
 of 
\begin_inset Formula $f$
\end_inset

, denoted 
\begin_inset Formula $\text{epi}\left(f\right)$
\end_inset

, 
\begin_inset Formula $\left\{ \left(x,y\right)\big|y\geq f\left(x\right),x\in S\right\} \subset\mathbb{R}^{n+1}$
\end_inset

.
 The 
\emph on
hypograph
\emph default
 of 
\begin_inset Formula $f$
\end_inset

, denoted 
\begin_inset Formula $\text{hypo}\left(f\right)$
\end_inset

, 
\begin_inset Formula $\left\{ \left(x,y\right)\big|y\leq f\left(x\right),x\in S\right\} \subset\mathbb{R}^{n+1}$
\end_inset

.
 
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 a convex function.
 Then 
\begin_inset Formula $f$
\end_inset

 is convex iff 
\begin_inset Formula $\text{epi}\left(f\right)$
\end_inset

 is convex.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 a convex function.
 Then 
\begin_inset Formula $f$
\end_inset

 is continuous on the interior of 
\begin_inset Formula $S$
\end_inset

.
 But only the interior!
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename conv_cont.jpg
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Convex functions are continuous on interiors of convex sets.
\begin_inset CommandInset label
LatexCommand label
name "fig:Convex-functions-are"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Definition*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 a convex function.
 Then 
\begin_inset Formula $\xi$
\end_inset

 is a 
\emph on
subgradient
\emph default
 of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $\bar{x}$
\end_inset

 if
\begin_inset Formula 
\[
f\left(x\right)\geq f\left(\bar{x}\right)+\xi^{t}\left(x-\bar{x}\right)
\]

\end_inset

I.e.
 
\begin_inset Formula $f\left(x\right)$
\end_inset

 is above the plane defined by 
\begin_inset Formula $f\left(\bar{x}\right)+\xi^{t}\left(x-\bar{x}\right)$
\end_inset

.
 
\begin_inset Formula $\xi$
\end_inset

 is the 
\series bold
slope of the line or the gradient
\series default
.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Definition*
The set of all subgradients 
\begin_inset Formula $\partial f\left(\bar{x}\right)$
\end_inset

 of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $\bar{x}$
\end_inset

 is called the 
\emph on
subdifferential
\emph default
 of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $\bar{x}$
\end_inset

.
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 a convex function.
 Then 
\begin_inset Formula $f$
\end_inset

 has a subgradient 
\begin_inset Formula $\xi$
\end_inset

 at 
\begin_inset Formula $\bar{x}\in\text{int}\left(S\right)$
\end_inset

.
 In particular, the hyperplane 
\begin_inset Formula 
\[
\mathcal{H}=\left\{ \left(x,y\right)\big|y=f\left(\bar{x}\right)+\xi^{t}\left(x-\bar{x}\right)\right\} 
\]

\end_inset

supports 
\begin_inset Formula $\text{epi}\left(f\right)$
\end_inset

 at 
\begin_inset Formula $\left(\bar{x},f\left(\bar{x}\right)\right)$
\end_inset

.
\end_layout

\begin_layout Proof
Note that 
\begin_inset Formula $\text{epi}\left(f\right)$
\end_inset

 is convex and 
\begin_inset Formula $\left(\bar{x},f\left(\bar{x}\right)\right)$
\end_inset

 belongs to its boundary.
 Therefore by separation theorem there exists 
\begin_inset Formula $\left(\xi_{0},\mu\right)\in\mathbb{R}^{n}\times\mathbb{R}$
\end_inset

.
 For all 
\begin_inset Formula $\left(\mathbf{x},y\right)\in\text{epi}\left(f\right)$
\end_inset

.
 
\begin_inset Formula 
\[
\xi_{0}^{t}\left(x-\bar{x}\right)+\mu\left(y-f\left(\bar{x}\right)\right)\leq0
\]

\end_inset


\end_layout

\begin_layout Proof
Note that 
\begin_inset Formula $\mu\leq0$
\end_inset

 because otherwise take 
\begin_inset Formula $y$
\end_inset

 large enough and the inequality would be violated.
 In fact 
\begin_inset Formula $\mu<0$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Proof
Toward a contradiction suppose 
\begin_inset Formula $\mu=0$
\end_inset

.
 Then 
\begin_inset Formula $\xi_{0}^{t}\left(x-\bar{x}\right)\leq0$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

.
 Since 
\begin_inset Formula $\bar{x}\in\text{int}\left(S\right)$
\end_inset

 there exists 
\begin_inset Formula $\lambda>0$
\end_inset

 such that 
\begin_inset Formula $x=\bar{x}+\lambda\xi_{0}\in S$
\end_inset

 and hence 
\begin_inset Formula $\lambda\xi_{0}^{t}\xi_{0}\leq0$
\end_inset

.
 This implies that 
\begin_inset Formula $\xi_{0}=0$
\end_inset

 (otherwise how could 
\begin_inset Formula $\lambda\xi_{0}^{t}\xi_{0}$
\end_inset

 negative or equal to zero with 
\begin_inset Formula $\lambda$
\end_inset

 strictly positive).
 But then 
\begin_inset Formula $\left(\xi_{0},\mu\right)=0$
\end_inset

 contradicting the separation theorem.
 Therefore 
\begin_inset Formula $\mu<0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Proof
Dividing 
\begin_inset Formula $\xi_{0}^{t}\left(x-\bar{x}\right)+\mu\left(y-f\left(\bar{x}\right)\right)\leq0$
\end_inset

 by 
\begin_inset Formula $\left|\mu\right|$
\end_inset


\begin_inset Formula 
\[
\xi^{t}\left(x-\bar{x}\right)+-1\left(y-f\left(\bar{x}\right)\right)=\xi^{t}\left(x-\bar{x}\right)-y+f\left(\bar{x}\right)\leq0
\]

\end_inset

for all 
\begin_inset Formula $\left(x,y\right)\in\text{epi}\left(f\right)$
\end_inset

.
 Then by letting 
\begin_inset Formula $y\rightarrow f\left(x\right)$
\end_inset

 we satisfy the theorem
\begin_inset Formula 
\[
\xi^{t}\left(x-\bar{x}\right)+f\left(\bar{x}\right)\leq f\left(x\right)
\]

\end_inset

In particular, the hyperplane 
\begin_inset Formula $H=\left\{ \left(x,y\right)\big|y=f\left(\bar{x}\right)+\xi^{t}\left(x-\bar{x}\right)\right\} $
\end_inset

 supports 
\begin_inset Formula $\text{epi}\left(f\right)$
\end_inset

 at 
\begin_inset Formula $\left(\bar{x},f\left(\bar{x}\right)\right)$
\end_inset

.
\end_layout

\begin_layout Corollary*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 a strictly convex function.
 Then for 
\begin_inset Formula $\bar{x}\in\text{int}\left(S\right)$
\end_inset

, there exists a vector 
\begin_inset Formula $\xi$
\end_inset

 such that 
\begin_inset Formula 
\[
f\left(x\right)>f\left(\bar{x}\right)+\xi^{t}\left(x-\bar{x}\right)
\]

\end_inset


\end_layout

\begin_layout Theorem*
Partial converse: let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

.
 Suppose for each 
\begin_inset Formula $\bar{x}\in\text{int}\left(S\right)$
\end_inset

 there exists 
\begin_inset Formula $\xi$
\end_inset


\begin_inset Formula 
\[
f\left(x\right)\geq f\left(\bar{x}\right)+\xi^{t}\left(x-\bar{x}\right)
\]

\end_inset

for all 
\begin_inset Formula $x\in S$
\end_inset

.
 Then 
\begin_inset Formula $f$
\end_inset

 is convex on 
\begin_inset Formula $\text{int}\left(S\right)$
\end_inset

.
\end_layout

\begin_layout Definition*
The 
\emph on
directional derivative
\emph default
 is 
\begin_inset Formula 
\[
f'\left(\bar{x},d\right)=\lim_{\lambda\rightarrow0^{+}}\frac{f\left(\bar{x}+\lambda d\right)-f\left(\bar{x}\right)}{\lambda}
\]

\end_inset

Alternatively it's 
\begin_inset Formula $d\cdot\nabla f\left(\bar{x}\right)$
\end_inset


\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 be a convex function.
 Then 
\begin_inset Formula $f$
\end_inset

 has all directional derivatives.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $\lambda_{2}>\lambda_{1}>0$
\end_inset

.
 By convexity of 
\begin_inset Formula $f$
\end_inset

 we have
\begin_inset Formula 
\begin{align*}
f\left(\bar{x}+\lambda d\right) & =f\left(\frac{\lambda_{1}}{\lambda_{2}}\left(\bar{x}+\lambda_{2}d\right)+\left(1-\frac{\lambda_{1}}{\lambda_{2}}\right)\bar{x}\right)\\
 & \leq\frac{\lambda_{1}}{\lambda_{2}}f\left(\bar{x}+\lambda_{2}d\right)+\left(1-\frac{\lambda_{1}}{\lambda_{2}}\right)f\left(\bar{x}\right)
\end{align*}

\end_inset

which implies
\begin_inset Formula 
\[
\frac{f\left(\bar{x}+\lambda_{1}d\right)-f\left(\bar{x}\right)}{\lambda_{1}}\leq\frac{f\left(\bar{x}+\lambda_{2}d\right)-f\left(\bar{x}\right)}{\lambda_{2}}
\]

\end_inset

Thus the difference quotient is monotonically decreasing.
 Then since the function is convex it has a subgradient at 
\begin_inset Formula $\bar{x}$
\end_inset

 and so bounded below.
 Therefore the limit converges.
\end_layout

\begin_layout Definition*
A function is called differentiable if there exists 
\begin_inset Formula $v$
\end_inset

 such that
\begin_inset Formula 
\[
f\left(x\right)=f\left(\bar{x}\right)+v^{t}\left(x-\bar{x}\right)+\alpha\left(\bar{x},x-\bar{x}\right)\left\Vert x-\bar{x}\right\Vert 
\]

\end_inset

The function 
\begin_inset Formula $\alpha$
\end_inset

 is the Lagrange form of the first order remainder term in the Taylor series
 approximation of 
\begin_inset Formula $f$
\end_inset

, i.e.
\begin_inset Formula 
\[
\alpha\left(\bar{x},x-\bar{x}\right)\left\Vert x-\bar{x}\right\Vert =\frac{f^{''}\left(\bar{x}\right)}{3!}\left(x-\bar{x}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $v$
\end_inset

 is called the gradient and duh is 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=\left(\frac{\partial f\left(\bar{x}\right)}{\partial x_{1}},\dots,\frac{\partial f\left(\bar{x}\right)}{\partial x_{n}}\right)$
\end_inset

.
\end_layout

\begin_layout Lemma*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 a convex function and differentiable at 
\begin_inset Formula $\bar{x}$
\end_inset

.
 Then the subdifferential at 
\begin_inset Formula $\bar{x}$
\end_inset

 is the singleton 
\begin_inset Formula $\left\{ \nabla f\left(\bar{x}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $f$
\end_inset

 is a convex set the subdifferential at 
\begin_inset Formula $\bar{x}$
\end_inset

 is not empty.
 Let 
\begin_inset Formula $\xi$
\end_inset

 be a subgradient of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $\bar{x}$
\end_inset

.
 Again by the same theorem (existence of subgradients)
\begin_inset Formula 
\[
f\left(\bar{x}+\lambda d\right)\geq f\left(\bar{x}\right)+\xi^{t}\left(\lambda d\right)
\]

\end_inset


\end_layout

\begin_layout Proof
By differentiability at 
\begin_inset Formula $\bar{x}$
\end_inset


\begin_inset Formula 
\[
f\left(\bar{x}+\lambda d\right)=f\left(\bar{x}\right)+\left(\nabla f\left(\bar{x}\right)\right)^{t}\left(\lambda d\right)+\alpha\left(\bar{x},\lambda d\right)\left\Vert \lambda d\right\Vert 
\]

\end_inset

Subtracting the equation from the inequality we get that 
\begin_inset Formula 
\[
0\geq\xi^{t}\left(\lambda d\right)-\left(\nabla f\left(\bar{x}\right)\right)^{t}\left(\lambda d\right)-\alpha\left(\bar{x},\lambda d\right)\left\Vert \lambda d\right\Vert 
\]

\end_inset

Dividing by 
\begin_inset Formula $\lambda$
\end_inset

 and letting 
\begin_inset Formula $\lambda\rightarrow0^{+}$
\end_inset

 we get that 
\begin_inset Formula $\left(\xi-\nabla f\left(\bar{x}\right)\right)^{t}d\leq0$
\end_inset

.
 Since this is true for all 
\begin_inset Formula $d$
\end_inset

, choosing 
\begin_inset Formula $d=\xi-\nabla f\left(\bar{x}\right)$
\end_inset

 proves that 
\begin_inset Formula $\xi-\nabla f\left(\bar{x}\right)=0$
\end_inset

 (how else would the norm squared of it be nonpositive).
\end_layout

\begin_layout Standard
In light of the lemma, and supporting hyperplane, and the partial converse
 we have another characterization of convex functions:
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty open (or 
\begin_inset Formula $\bar{x}\in\text{int}\left(S\right)$
\end_inset

) convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 differentiable.
 Then 
\begin_inset Formula $f$
\end_inset

 is convex iff 
\begin_inset Formula $\forall\bar{x}\in S$
\end_inset


\begin_inset Formula 
\[
f\left(x\right)\geq f\left(\bar{x}\right)+\left(\nabla f\left(\bar{x}\right)\right)^{t}\left(x-\bar{x}\right)
\]

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty open (or 
\begin_inset Formula $\bar{x}\in\text{int}\left(S\right)$
\end_inset

) convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 differentiable.
 Then 
\begin_inset Formula $f$
\end_inset

 is convex iff 
\begin_inset Formula $\forall x_{1},x_{2}\in S$
\end_inset


\begin_inset Formula 
\[
\left(\nabla f\left(x_{2}\right)-\nabla f\left(x_{1}\right)\right)^{t}\left(x_{2}-x_{1}\right)\geq0
\]

\end_inset


\end_layout

\begin_layout Proof
By characterization of convexity we have that 
\begin_inset Formula 
\begin{align*}
f\left(x_{1}\right) & \geq f\left(x_{2}\right)+\left(\nabla f\left(x_{2}\right)\right)^{t}\left(x_{1}-x_{2}\right)\\
f\left(x_{2}\right) & \geq f\left(x_{1}\right)+\left(\nabla f\left(x_{1}\right)\right)^{t}\left(x_{2}-x_{1}\right)
\end{align*}

\end_inset

Adding the two inequalities gets the result.
 To prove the converse use the mean value theorem:
\begin_inset Formula 
\[
f\left(x_{2}\right)-f\left(x_{1}\right)=\left(\nabla f\left(x\right)\right)^{t}\left(x_{2}-x_{1}\right)
\]

\end_inset

where 
\begin_inset Formula $x=\lambda x_{1}+\left(1-\lambda\right)x_{2}$
\end_inset

 for 
\begin_inset Formula $\lambda\in\left(0,1\right)$
\end_inset

.
 By assumption 
\begin_inset Formula $\left(\nabla f\left(x\right)-\nabla f\left(x_{1}\right)\right)^{t}\left(x-x_{1}\right)\geq0$
\end_inset

 which is equivalent to 
\begin_inset Formula 
\[
\left(1-\lambda\right)\left(\nabla f\left(x\right)-\nabla f\left(x_{1}\right)\right)^{t}\left(x_{2}-x_{1}\right)\geq0
\]

\end_inset

This implies that 
\begin_inset Formula 
\[
\left(\nabla f\left(x\right)\right)^{t}\left(x_{2}-x_{1}\right)\geq\left(\nabla f\left(x_{1}\right)\right)^{t}\left(x_{2}-x_{1}\right)
\]

\end_inset

But by mean value theorem result we get that 
\begin_inset Formula 
\[
f\left(x_{2}\right)-f\left(x_{1}\right)\geq\left(\nabla f\left(x_{1}\right)\right)^{t}\left(x_{2}-x_{1}\right)
\]

\end_inset

and so by previous characterization we get that 
\begin_inset Formula $f$
\end_inset

 is convex.
\end_layout

\begin_layout Standard
This is a first order condition on convexity.
 How about second order conditions?
\end_layout

\begin_layout Definition*
A function 
\begin_inset Formula $f$
\end_inset

 is twice differentiable if 
\begin_inset Formula 
\[
f\left(x\right)=f\left(\bar{x}\right)+\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)+\frac{1}{2}\left(x-\bar{x}\right)^{t}H\left(\bar{x}\right)\left(x-\bar{x}\right)+\alpha\left(\bar{x},x-\bar{x}\right)\left\Vert x-\bar{x}\right\Vert ^{2}
\]

\end_inset

and 
\begin_inset Formula $\lim_{x\rightarrow\bar{x}}\alpha\left(\bar{x},x-\bar{x}\right)=0$
\end_inset

.
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty open (or 
\begin_inset Formula $\bar{x}\in\text{int}\left(S\right)$
\end_inset

) convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 twice differentiable.
 Then 
\begin_inset Formula $f$
\end_inset

 is convex iff 
\begin_inset Formula $\nabla^{2}f\left(\bar{x}\right)$
\end_inset

 is psd.
\end_layout

\begin_layout Standard
For strict convexity you need something stronger.
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty open (or 
\begin_inset Formula $\bar{x}\in\text{int}\left(S\right)$
\end_inset

) convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 twice differentiable.
 Then 
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\nabla^{2}f\left(\bar{x}\right)$
\end_inset

 is pd then 
\begin_inset Formula $f$
\end_inset

 is strictly convex.
\end_layout

\begin_layout Enumerate
f is strictly convex then 
\begin_inset Formula $\nabla^{2}f\left(\bar{x}\right)$
\end_inset

 is psd.
\end_layout

\begin_layout Enumerate
f is strictly convex and quadratic then 
\begin_inset Formula $\nabla^{2}f\left(\bar{x}\right)$
\end_inset

 is pd.
\end_layout

\end_deeper
\begin_layout Section
Optimality conditions for Convex programs
\end_layout

\begin_layout Definition*
Consider 
\begin_inset Formula 
\begin{align*}
\min_{x} & f\left(x\right)\\
\text{s.t.} & x\in S
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\bar{x}\in S$
\end_inset

 is a 
\series bold
feasible solution
\series default
 to the problem.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\bar{x}\in S$
\end_inset

 such that 
\begin_inset Formula $f\left(\bar{x}\right)\leq f\left(x\right)$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

 is a 
\series bold
globally optimal solution
\series default
.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\bar{x}\in S$
\end_inset

 such that 
\begin_inset Formula $f\left(\bar{x}\right)\leq f\left(x\right)$
\end_inset

 for all 
\begin_inset Formula $x\in S\cap\mathcal{N}_{\epsilon}\left(\bar{x}\right)$
\end_inset

, i.e.
 in some neighborhood of 
\begin_inset Formula $\bar{x}$
\end_inset

 is a 
\series bold
locally optimal
\series default
 
\series bold
solution
\series default
.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\bar{x}\in S$
\end_inset

 such that 
\begin_inset Formula $f\left(\bar{x}\right)<f\left(x\right)$
\end_inset

 for all 
\begin_inset Formula $x\in S\cap\mathcal{N}_{\epsilon}\left(\bar{x}\right)$
\end_inset

, i.e.
 in some neighborhood of 
\begin_inset Formula $\bar{x}$
\end_inset

 is a 
\series bold
strict locally optimal
\series default
 
\series bold
solution
\series default
.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\bar{x}\in S$
\end_inset

 such that 
\begin_inset Formula $f\left(\bar{x}\right)<f\left(x\right)$
\end_inset

 for all 
\begin_inset Formula $x\in S\cap\mathcal{N}_{\epsilon}\left(\bar{x}\right)$
\end_inset

, i.e.
 in some neighborhood of 
\begin_inset Formula $\bar{x}$
\end_inset

 and 
\begin_inset Formula $\bar{x}$
\end_inset

 is the only such point is an 
\series bold
isolated locally optimal
\series default
 
\series bold
solution
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename solns.jpg
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Characterizing solutions
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty convex set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $f:S\rightarrow\mathbb{R}$
\end_inset

 convex and 
\begin_inset Formula $\bar{x}$
\end_inset

 be a locally optimal solution (optimal, but not uniquely, within an 
\begin_inset Formula $\epsilon$
\end_inset

-ball) to 
\begin_inset Formula $\min_{x\in S}f\left(x\right)$
\end_inset

.
 Then
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\bar{x}$
\end_inset

 is globally optimal.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $\bar{x}$
\end_inset

 is strictly locally optimal (
\begin_inset Formula $f\left(\bar{x}\right)<f\left(x\right)$
\end_inset

 in the 
\begin_inset Formula $\epsilon$
\end_inset

-ball).
 or 
\begin_inset Formula $f$
\end_inset

 is strictly convex then
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\bar{x}$
\end_inset

 is uniquely globally optimal.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\bar{x}$
\end_inset

 is strongly (isolated) locally optimal.
\end_layout

\end_deeper
\end_deeper
\begin_layout Proof
Towards a contradiction suppose 
\begin_inset Formula $\bar{x}$
\end_inset

 is not globally optimal.
 Note 
\begin_inset Formula $\bar{x}$
\end_inset

 locally optimal means 
\begin_inset Formula $f\left(\bar{x}\right)\leq f\left(x\right)$
\end_inset

 for 
\begin_inset Formula $x\in S\cap\mathcal{N}_{\epsilon}\left(\bar{x}\right)$
\end_inset

 with 
\begin_inset Formula $\epsilon>0$
\end_inset

.
 Then not being global means there exists 
\begin_inset Formula $\hat{x}$
\end_inset

 such that 
\begin_inset Formula $f\left(\hat{x}\right)<f\left(\bar{x}\right)$
\end_inset

.
 By convexity of 
\begin_inset Formula $f$
\end_inset

 
\begin_inset Formula 
\[
f\left(\lambda\hat{x}+\left(1-\lambda\right)\bar{x}\right)\leq\lambda f\left(\hat{x}\right)+\left(1-\lambda\right)f\left(\bar{x}\right)
\]

\end_inset

Then using 
\begin_inset Formula $f\left(\hat{x}\right)<f\left(\bar{x}\right)$
\end_inset


\begin_inset Formula 
\[
f\left(\lambda\hat{x}+\left(1-\lambda\right)\bar{x}\right)\leq\lambda f\left(\hat{x}\right)+\left(1-\lambda\right)f\left(\bar{x}\right)<\lambda f\left(\bar{x}\right)+\left(1-\lambda\right)f\left(\bar{x}\right)=f\left(\bar{x}\right)
\]

\end_inset

Taking 
\begin_inset Formula $\lambda<\epsilon$
\end_inset

 we get a contradiction.
 That handles part 1.
\end_layout

\begin_layout Proof
Part 2(a): Suppose 
\begin_inset Formula $\bar{x}$
\end_inset

 is a strict local optimal.
 Then by part 1 it's a global optimum.
 Suppose there exists 
\begin_inset Formula $\hat{x}$
\end_inset

 such that 
\begin_inset Formula $f\left(\bar{x}\right)=f\left(\hat{x}\right)$
\end_inset

.
 Then 
\begin_inset Formula 
\[
f\left(x_{\lambda}\right)=f\left(\lambda\hat{x}+\left(1-\lambda\right)\bar{x}\right)\leq\lambda f\left(\hat{x}\right)+\left(1-\lambda\right)f\left(\bar{x}\right)=\lambda f\left(\bar{x}\right)+\left(1-\lambda\right)f\left(\bar{x}\right)=f\left(\bar{x}\right)
\]

\end_inset

and by taking 
\begin_inset Formula $\lambda<\epsilon$
\end_inset

 we get a contradiction to strict local optimality.
 Hence 
\begin_inset Formula $\bar{x}$
\end_inset

 is the unique global minimum.
 
\end_layout

\begin_layout Proof
Part 2(b): Since it's a unique global minimum it must be isolated since
 any other local minimum 
\begin_inset Formula $\bar{x}$
\end_inset

 in 
\begin_inset Formula $S\cap\mathcal{N}_{\epsilon}\left(\bar{x}\right)$
\end_inset

 would be a global minimum.
\end_layout

\begin_layout Proof
Part 2'(a/b): Suppose 
\begin_inset Formula $\bar{x}$
\end_inset

 is a local optimum and 
\begin_inset Formula $f$
\end_inset

 is strictly convex.
 Since strictly convex implies convexity 
\begin_inset Formula $\bar{x}$
\end_inset

 is still a global minimum.
 Towards a contradiction suppose 
\begin_inset Formula $\bar{x}$
\end_inset

 is not unique, i.e.
 there exists 
\begin_inset Formula $\hat{x}$
\end_inset

 such that 
\begin_inset Formula $f\left(\hat{x}\right)=f\left(\bar{x}\right)$
\end_inset

.
 By strict convexity
\begin_inset Formula 
\[
f\left(\frac{\bar{x}+\hat{x}}{2}\right)<\frac{1}{2}f\left(\bar{x}\right)+\frac{1}{2}f\left(\hat{x}\right)=f\left(\bar{x}\right)
\]

\end_inset

By convexity of 
\begin_inset Formula $S$
\end_inset

 we have 
\begin_inset Formula $\frac{\bar{x}+\hat{x}}{2}\in S$
\end_inset

 and therefore a contradiction of global optimality.
 Further similarly 
\begin_inset Formula $\bar{x}$
\end_inset

 is also isolated.
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 be a convex function and 
\begin_inset Formula $S\neq\emptyset$
\end_inset

 a convex subset of 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $\bar{x}\in S$
\end_inset

.
 Then for 
\begin_inset Formula 
\[
\min_{x\in S}\,f\left(x\right)
\]

\end_inset


\begin_inset Formula $\bar{x}$
\end_inset

 is globally optimal iff 
\begin_inset Formula $f$
\end_inset

 has a supporting subgradient 
\begin_inset Formula $\xi$
\end_inset

 such that 
\begin_inset Formula $\xi^{t}\left(x-\bar{x}\right)\geq0$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

.
\end_layout

\begin_layout Proof
\begin_inset Formula $\Leftarrow$
\end_inset

 Suppose there exists 
\begin_inset Formula $\xi$
\end_inset

 such that 
\begin_inset Formula $\xi^{t}\left(x-\bar{x}\right)\geq0$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

.
 By convexity of 
\begin_inset Formula $f$
\end_inset

 
\begin_inset Formula 
\[
f\left(x\right)\geq f\left(\bar{x}\right)+\xi^{t}\left(x-\bar{x}\right)\geq f\left(\bar{x}\right)
\]

\end_inset

since 
\begin_inset Formula $\xi^{t}\left(x-\bar{x}\right)$
\end_inset

.
 Hence 
\begin_inset Formula $\bar{x}$
\end_inset

 is optimal.
\end_layout

\begin_layout Proof
\begin_inset Formula $\Rightarrow$
\end_inset

 Suppose 
\begin_inset Formula $\bar{x}$
\end_inset

 is optimal.
 Let 
\begin_inset Formula 
\begin{align*}
\Lambda_{1} & =\left\{ \left(x-\bar{x},y\right)\big|x\in\mathbb{R}^{n},y>f\left(x\right)-f\left(\bar{x}\right)\right\} \\
\Lambda_{2} & =\left\{ \left(x-\bar{x},y\right)\big|x\in S,y\leq0\right\} 
\end{align*}

\end_inset

Note that 
\begin_inset Formula $\Lambda_{1}$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

shifted
\begin_inset Quotes erd
\end_inset

 epigraph of 
\begin_inset Formula $f$
\end_inset

, shifted such that 
\begin_inset Formula $\left(\bar{x},f\left(\bar{x}\right)\right)=\left(0,0\right)$
\end_inset

.
 Both 
\begin_inset Formula $\Lambda_{1}$
\end_inset

 and 
\begin_inset Formula $\Lambda_{2}$
\end_inset

 are convex.
 Also 
\begin_inset Formula $\Lambda_{1}\cap\Lambda_{2}=\emptyset$
\end_inset

 since otherwise there exists 
\begin_inset Formula $\left(x,y\right)$
\end_inset

 such that 
\begin_inset Formula $x\in S$
\end_inset

 and 
\begin_inset Formula $0>y>f\left(x\right)-f\left(\bar{x}\right)$
\end_inset

 contradicting 
\begin_inset Formula $\bar{x}$
\end_inset

 is optimal (since 
\begin_inset Formula $0>f\left(x\right)-f\left(\bar{x}\right)\iff f\left(\bar{x}\right)>f\left(x\right)$
\end_inset

).
 By separating hyperplane theorem there exists a hyperplane that separates:
 there exists nonzero 
\begin_inset Formula $\left(\xi_{0},\mu\right)$
\end_inset

 and 
\begin_inset Formula $\alpha\neq0$
\end_inset

 such that 
\begin_inset Formula 
\begin{align*}
\xi_{0}^{t}\left(x-\bar{x}\right)+\mu y & \leq\alpha,\,\forall x\in\mathbb{R}^{n},\,y>f\left(x\right)-f\left(\bar{x}\right)\\
\xi_{0}^{t}\left(x-\bar{x}\right)+\mu y & \geq\alpha,\,\forall x\in S,\,y\leq0
\end{align*}

\end_inset

Then letting 
\begin_inset Formula $x=\bar{x}$
\end_inset

 and 
\begin_inset Formula $y=0$
\end_inset

 in the second inequality you get that 
\begin_inset Formula $\alpha\leq0$
\end_inset

.
 Now letting 
\begin_inset Formula $x=\bar{x}$
\end_inset

 and 
\begin_inset Formula $y=\epsilon>0$
\end_inset

 in the first inequality and you get 
\begin_inset Formula $\mu\epsilon\leq\alpha$
\end_inset

.
 Since this is true for every 
\begin_inset Formula $\epsilon>0$
\end_inset

 it's the case that 
\begin_inset Formula $\alpha\geq0$
\end_inset

 and therefore
\begin_inset Formula $\mu\leq0$
\end_inset

.
 Therefore 
\begin_inset Formula $\mu\leq0$
\end_inset

 and 
\begin_inset Formula $\alpha=0$
\end_inset

.
 So summarizing 
\begin_inset Formula 
\begin{align*}
\xi_{0}^{t}\left(x-\bar{x}\right)+\mu y & \leq0,\,\forall x\in\mathbb{R}^{n},\,y>f\left(x\right)-f\left(\bar{x}\right)\\
\xi_{0}^{t}\left(x-\bar{x}\right)+\mu y & \geq0,\,\forall x\in S,\,y\leq0
\end{align*}

\end_inset

If 
\begin_inset Formula $\mu$
\end_inset

 were 0 then 
\begin_inset Formula $\xi_{0}^{t}\left(x-\bar{x}\right)\leq0$
\end_inset

 for all 
\begin_inset Formula $x\in\mathbb{R}^{n}$
\end_inset

 and then letting 
\begin_inset Formula $x=\bar{x}+\xi_{0}$
\end_inset

 shows that 
\begin_inset Formula $\xi_{0}=0$
\end_inset

 which isn't possible.
 So 
\begin_inset Formula $\mu<0$
\end_inset

.
 Dividing by 
\begin_inset Formula $\left|\mu\right|$
\end_inset

 everywhere we get that 
\begin_inset Formula 
\begin{align*}
\xi^{t}\left(x-\bar{x}\right)-y & \leq0,\,\forall x\in\mathbb{R}^{n},\,y>f\left(x\right)-f\left(\bar{x}\right)\\
\xi^{t}\left(x-\bar{x}\right)-y & \geq0,\,\forall x\in S,\,y\leq0
\end{align*}

\end_inset

Letting 
\begin_inset Formula $y=0$
\end_inset

 in the second inequality we get that 
\begin_inset Formula $\xi^{t}\left(x-\bar{x}\right)\geq0$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

.
 From the first inequality we conclude that since 
\begin_inset Formula $\xi^{t}\left(x-\bar{x}\right)-y\leq0$
\end_inset

 for all 
\begin_inset Formula $\left(x,y\right)$
\end_inset

 in the 
\begin_inset Quotes eld
\end_inset

shifted
\begin_inset Quotes erd
\end_inset

 strict epigraph of 
\begin_inset Formula $f$
\end_inset

 it must therefore also hold in the closure, i.e.
 where 
\begin_inset Formula $y=f\left(x\right)-f\left(\bar{x}\right)$
\end_inset

, which gives us
\begin_inset Formula 
\[
f\left(x\right)\geq f\left(\bar{x}\right)+\xi^{t}\left(x-\bar{x}\right)
\]

\end_inset


\end_layout

\begin_layout Corollary*
If 
\begin_inset Formula $S$
\end_inset

 is open then 
\begin_inset Formula $\bar{x}$
\end_inset

 is global optimal iff 
\begin_inset Formula $0\in\partial f\left(\bar{x}\right)$
\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula $\bar{x}$
\end_inset

 is optimal iff there exists 
\begin_inset Formula $\xi$
\end_inset

 such that 
\begin_inset Formula $\xi^{t}\left(x-\bar{x}\right)\geq0$
\end_inset

.
 Since 
\begin_inset Formula $S$
\end_inset

 is open take 
\begin_inset Formula $\lambda$
\end_inset

 such that 
\begin_inset Formula $x=\bar{x}-\lambda\xi\in S$
\end_inset

 and then 
\begin_inset Formula 
\[
-\lambda\left\Vert \xi\right\Vert ^{2}\geq0
\]

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Corollary*
If 
\begin_inset Formula $f$
\end_inset

 differentiable (and convex) then
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\bar{x}$
\end_inset

 is globally optimal iff 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)\geq0$
\end_inset


\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $S$
\end_inset

 is open then 
\begin_inset Formula $\bar{x}$
\end_inset

 is globally optimal iff 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset


\end_layout

\end_deeper
\begin_layout Proof
Obvious since 
\begin_inset Formula $\partial f\left(\bar{x}\right)=\left\{ \nabla f\left(\bar{x}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename grad_dir.jpg
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Gradient angle
\begin_inset CommandInset label
LatexCommand label
name "fig:Gradient-angle"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Consider figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Gradient-angle"

\end_inset

.
 Suppose the problem is to minimize 
\begin_inset Formula $f$
\end_inset

 subject to 
\begin_inset Formula $x\in S$
\end_inset

 and 
\begin_inset Formula $f$
\end_inset

 is differentiable and convex but 
\begin_inset Formula $S$
\end_inset

 is arbitrary.
 Suppose at some 
\begin_inset Formula $\bar{x}$
\end_inset

 the directional derivative 
\begin_inset Formula $\nabla f\left(\bar{x}\right)\left(x-\bar{x}\right)\geq0$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

.
 Then going in any direction in 
\begin_inset Formula $S$
\end_inset

 would potentially increase the objective, regardless of what 
\begin_inset Formula $S$
\end_inset

 is like.
 Why? By convexity
\begin_inset Foot
status open

\begin_layout Plain Layout
At every 
\begin_inset Formula $\bar{x}$
\end_inset

 we have that 
\begin_inset Formula $f\left(x\right)\geq f\left(\bar{x}\right)+\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)$
\end_inset

.
 
\end_layout

\end_inset

and differentiability of 
\begin_inset Formula $f$
\end_inset

 any solution 
\begin_inset Formula $\hat{x}$
\end_inset

 (anywhere in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

, which is a convex set) that improves on 
\begin_inset Formula $\bar{x}$
\end_inset


\begin_inset Formula 
\[
f\left(\bar{x}\right)>f\left(\hat{x}\right)\geq f\left(\bar{x}\right)+\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)
\]

\end_inset

which implies 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)<f\left(\bar{x}\right)-f\left(\bar{x}\right)=0$
\end_inset

 but 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)\geq0$
\end_inset

 for all 
\begin_inset Formula $x\in S$
\end_inset

.
 Hence the hyperplane 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)=0$
\end_inset

 separates 
\begin_inset Formula $S$
\end_inset

 (arbitrary 
\begin_inset Formula $S$
\end_inset

) from solutions that improve cost.
 In the non-differentiable case the supporting hyperplane 
\begin_inset Formula $\xi$
\end_inset

 plays the same role as 
\begin_inset Formula $\nabla f\left(\bar{x}\right)$
\end_inset

.
 Conversely suppose 
\begin_inset Formula $f$
\end_inset

 is differentiable but arbitrary otherwise and 
\begin_inset Formula $S$
\end_inset

 is convex.
 Then 
\begin_inset Formula $\bar{x}$
\end_inset

 is a global minimum then again 
\begin_inset Formula $\nabla f\left(\bar{x}\right)\left(x-\bar{x}\right)\geq0$
\end_inset

 since otherwise if there exists 
\begin_inset Formula $x\in\text{int}\left(S\right)$
\end_inset

 such that 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)<0$
\end_inset

 you could do go in the direction 
\begin_inset Formula $d=x-\bar{x}$
\end_inset

, improve cost, and still satisfy constraints.
 The general take away is: if 
\begin_inset Formula $f$
\end_inset

 is differentiable and otherwise 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $S$
\end_inset

 are arbitrary and 
\begin_inset Formula $\bar{x}$
\end_inset

 is a local minimum then for any feasible direction 
\begin_inset Formula $d$
\end_inset

 such that 
\begin_inset Formula $x=\bar{x}+\lambda d$
\end_inset

 it must be the case that 
\begin_inset Formula 
\[
\nabla f\left(\bar{x}\right)^{t}d\geq0
\]

\end_inset

for some 
\begin_inset Formula $0<\lambda\leq\delta$
\end_inset

 i.e.
 going in that direction for a small enough step does not improve the objective.
 
\end_layout

\begin_layout Standard
Back to your original program:
\end_layout

\begin_layout Standard
What characterizes the set of optimal solutions to 
\begin_inset Formula $\min\left\{ f\left(x\right)\big|x\in S\right\} $
\end_inset

 when 
\begin_inset Formula $f$
\end_inset

 is convex and differentiable and so is 
\begin_inset Formula $S$
\end_inset

? 
\end_layout

\begin_layout Theorem*
If 
\begin_inset Formula $f$
\end_inset

 is convex and differentiable and 
\begin_inset Formula $S$
\end_inset

 is convex.
 Suppose there exists an optimal solution 
\begin_inset Formula $\bar{x}$
\end_inset

.
 Then the set of optimal solution 
\begin_inset Formula $S^{*}$
\end_inset


\begin_inset Formula 
\[
S^{*}=\left\{ x\in S\big|\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)\leq0,\nabla f\left(x\right)=\nabla f\left(\bar{x}\right)\right\} 
\]

\end_inset


\end_layout

\begin_layout Proof
Denote the candidate set of optimal solutions by 
\begin_inset Formula $\bar{S}$
\end_inset

 and note that 
\begin_inset Formula $\bar{x}\in\bar{S}$
\end_inset

.
 Consider 
\begin_inset Formula $\hat{x}\in S^{*}$
\end_inset

.
 By convexity of 
\begin_inset Formula $f$
\end_inset

 (and convexity of 
\begin_inset Formula $S$
\end_inset

) and definition of 
\begin_inset Formula $S^{*}$
\end_inset

, it's the case that 
\begin_inset Formula $\hat{x}\in S$
\end_inset


\begin_inset Formula 
\[
f\left(\bar{x}\right)\geq f\left(\hat{x}\right)+\nabla f\left(\hat{x}\right)^{t}\left(\bar{x}-\hat{x}\right)=f\left(\hat{x}\right)+\nabla f\left(\bar{x}\right)^{t}\left(\bar{x}-\hat{x}\right)=f\left(\hat{x}\right)+\left(-\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)\right)\geq f\left(\hat{x}\right)
\]

\end_inset

Hence 
\begin_inset Formula $\hat{x}\in\bar{S}$
\end_inset

 and so 
\begin_inset Formula $S^{*}\subset\bar{S}$
\end_inset

.
 Conversely, suppose 
\begin_inset Formula $\hat{x}\in\bar{S}$
\end_inset

 then 
\begin_inset Formula $f\left(\hat{x}\right)=f\left(\bar{x}\right)$
\end_inset

 and so
\begin_inset Formula 
\[
f\left(\bar{x}\right)=f\left(\hat{x}\right)\geq f\left(\bar{x}\right)+\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)
\]

\end_inset

and hence 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)\leq0$
\end_inset

 but by corollary above 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)\geq0$
\end_inset

 and hence 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)=0$
\end_inset

.
 Interchanging 
\begin_inset Formula $\hat{x}$
\end_inset

 and 
\begin_inset Formula $\bar{x}$
\end_inset

 we get that 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(\bar{x}-\hat{x}\right)=0$
\end_inset

 and subtracting we get
\begin_inset Formula 
\[
\left(\nabla f\left(\bar{x}\right)-\nabla f\left(\hat{x}\right)\right)^{t}\left(\bar{x}-\hat{x}\right)=0
\]

\end_inset

But 
\begin_inset Formula 
\begin{align*}
\left(\nabla f\left(\bar{x}\right)-\nabla f\left(\hat{x}\right)\right) & =\left.\nabla f\left(\hat{x}+\lambda\left(\bar{x}-\hat{x}\right)\right)\right|_{\lambda=0}^{\lambda=1}\\
 & =\int_{0}^{1}\nabla^{2}f\left(\hat{x}+\lambda\left(\bar{x}-\hat{x}\right)\right)\left(\bar{x}-\hat{x}\right)d\lambda\eqqcolon G\left(\bar{x}-\hat{x}\right)
\end{align*}

\end_inset

Note that 
\begin_inset Formula $G$
\end_inset

 is psd since 
\begin_inset Formula $\nabla^{2}f$
\end_inset

 is psd (since 
\begin_inset Formula $f$
\end_inset

 is convex).
 Then 
\begin_inset Formula 
\[
0=\left(\bar{x}-\hat{x}\right)^{t}\left(\nabla f\left(\bar{x}\right)-\nabla f\left(\hat{x}\right)\right)=\left(\bar{x}-\hat{x}\right)^{t}G\left(\bar{x}-\hat{x}\right)
\]

\end_inset

and by psd-ness 
\begin_inset Formula $G\left(\bar{x}-\hat{x}\right)$
\end_inset

 must be 0 and hence 
\begin_inset Formula $\nabla f\left(\bar{x}\right)-\nabla f\left(\hat{x}\right)=0$
\end_inset

 (and 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(\hat{x}-\bar{x}\right)\leq0$
\end_inset

) and hence 
\begin_inset Formula $\hat{x}\in S^{*}$
\end_inset

 and so 
\begin_inset Formula $\bar{S}\subset S^{*}$
\end_inset

.
\end_layout

\begin_layout Corollary*
The set of alternative solutions 
\begin_inset Formula $S^{*}$
\end_inset

 for convex, twice differentiable, and convex 
\begin_inset Formula $S$
\end_inset

 can be characterized as
\begin_inset Formula 
\[
S^{*}=\left\{ x\in S\big|\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)=0,\nabla f\left(x\right)=\nabla f\left(\bar{x}\right)\right\} 
\]

\end_inset


\end_layout

\begin_layout Proof
Follows from previous theorem and for optimal 
\begin_inset Formula $\bar{x}$
\end_inset

, and all other 
\begin_inset Formula $x\in S$
\end_inset

, 
\begin_inset Formula $\nabla f\left(\bar{x}\right)\left(x-\bar{x}\right)\geq0$
\end_inset

.
\end_layout

\begin_layout Corollary*
Suppose 
\begin_inset Formula $f=c^{t}x+\frac{1}{2}x^{t}Hx$
\end_inset

 and 
\begin_inset Formula $S$
\end_inset

 is polyhedral.
 Then 
\begin_inset Formula $S^{*}$
\end_inset

 is the polyhedral set given by
\begin_inset Formula 
\[
S^{*}=\left\{ x\in S\big|c^{t}\left(x-\bar{x}\right)=0,H\left(x-\bar{x}\right)=0\right\} 
\]

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula $\nabla f\left(x\right)=c+Hx$
\end_inset


\end_layout

\begin_layout Standard
For maxima of convex functions similar things apply (but for different reasons).
 
\end_layout

\begin_layout Theorem*
If 
\begin_inset Formula $f$
\end_inset

 is convex and 
\begin_inset Formula $S$
\end_inset

 a nonempty convex set, if 
\begin_inset Formula $\bar{x}$
\end_inset

 is a local optimal then for all 
\begin_inset Formula $\xi$
\end_inset

 it's the case that 
\begin_inset Formula $\xi^{t}\left(x-\bar{x}\right)\leq0$
\end_inset

 for each 
\begin_inset Formula $x\in S$
\end_inset

.
\end_layout

\begin_layout Proof
Suppose 
\begin_inset Formula $\bar{x}\in S$
\end_inset

 is a local optimum.
 Then for all 
\begin_inset Formula $x\in S\cap\mathcal{N}_{\epsilon}\left(\bar{x}\right)$
\end_inset

 it's the case that 
\begin_inset Formula $f\left(x\right)\leq f\left(\bar{x}\right)$
\end_inset

.
 Let 
\begin_inset Formula $\lambda<\epsilon$
\end_inset

 and then
\begin_inset Formula 
\[
f\left(\bar{x}+\lambda\left(x-\bar{x}\right)\right)\le f\left(\bar{x}\right)
\]

\end_inset

Then by convexity of 
\begin_inset Formula $f$
\end_inset

 for all 
\begin_inset Formula $\xi$
\end_inset


\begin_inset Formula 
\[
f\left(\bar{x}+\lambda\left(x-\bar{x}\right)\right)\geq f\left(\bar{x}\right)+\lambda\xi^{t}\left(x-\bar{x}\right)
\]

\end_inset

implying 
\begin_inset Formula $\lambda\xi^{t}\left(x-\bar{x}\right)\leq0$
\end_inset

.
 Divinding by 
\begin_inset Formula $\lambda$
\end_inset

 we have it.
\end_layout

\begin_layout Corollary*
If 
\begin_inset Formula $f$
\end_inset

 is differentiable them 
\begin_inset Formula $\nabla f\left(\bar{x}\right)\left(x-\bar{x}\right)\le0$
\end_inset

.
 
\end_layout

\begin_layout Standard
The result is necessary but not sufficient for gradients to be as such.
\end_layout

\begin_layout Theorem*
If 
\begin_inset Formula $f$
\end_inset

 is a convex function and 
\begin_inset Formula $S$
\end_inset

 a nonempty polyhedral set then the solution to a maximization problem is
 at the boudary (i.e.
 extreme point).
\end_layout

\begin_layout Proof
Note 
\begin_inset Formula $f$
\end_inset

 is continuous because it's convex and 
\begin_inset Formula $S$
\end_inset

 is compact (bounded over 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

) and hence 
\begin_inset Formula $f$
\end_inset

 achieves its maximum at 
\begin_inset Formula $x'\in S$
\end_inset

.
 By the representation theorem 
\begin_inset Formula $S$
\end_inset

 is defined only as a convex combination of extreme points hence 
\begin_inset Formula 
\[
x^{'}=\sum_{j=1}^{k}\lambda_{j}x_{j},\sum_{j=1}^{k}\lambda_{j}=1
\]

\end_inset

and 
\begin_inset Formula $\lambda_{j}\geq0$
\end_inset

 and 
\begin_inset Formula $x_{j}$
\end_inset

 are extreme points.
 By convexity of 
\begin_inset Formula $f$
\end_inset

 
\begin_inset Formula 
\[
f\left(\sum_{j=1}^{k}\lambda_{j}x_{j}\right)\leq\sum_{j=1}^{k}\lambda_{j}f\left(x_{j}\right)
\]

\end_inset

But 
\begin_inset Formula $f\left(x^{'}\right)\geq f\left(x_{j}\right)$
\end_inset

 for all 
\begin_inset Formula $j$
\end_inset

 and thus 
\begin_inset Formula $f\left(x^{'}\right)=f\left(x_{j}\right)$
\end_inset

.
 Therefore the solutions are all at the bounday points.
\end_layout

\begin_layout Section
Optimality conditions for unconstrained problems
\end_layout

\begin_layout Definition*
Consider 
\begin_inset Formula $\min_{x\in\mathbb{R}^{n}}f\left(x\right)$
\end_inset

.
 Then 
\begin_inset Formula $d$
\end_inset

 is an 
\emph on
improving
\emph default
 direction at 
\begin_inset Formula $\bar{x}$
\end_inset

 if 
\begin_inset Formula $f\left(\bar{x}+\lambda d\right)<f\left(\bar{x}\right)$
\end_inset

 for 
\begin_inset Formula $0<\lambda<\epsilon$
\end_inset

 for some 
\begin_inset Formula $\epsilon$
\end_inset

.
 
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $d$
\end_inset

 be such that 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}d<0$
\end_inset

.
 Then 
\begin_inset Formula $d$
\end_inset

 is an improving direction.
\end_layout

\begin_layout Proof
Why? This says that going in the direction of is strictly acute with 
\begin_inset Formula $-\nabla f\left(\bar{x}\right)$
\end_inset

, i.e.
 sort of aligned with the direction of steepest descent.
 By differentiability of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $\bar{x}$
\end_inset

 
\begin_inset Formula 
\[
f\left(\bar{x}+\lambda d\right)=f\left(\bar{x}\right)+\lambda\nabla f\left(\bar{x}\right)^{t}d+\lambda\left\Vert d\right\Vert \alpha\left(\bar{x},\lambda d\right)
\]

\end_inset

Re-arranging terms we get
\begin_inset Formula 
\[
\frac{f\left(\bar{x}+\lambda d\right)-f\left(\bar{x}\right)}{\lambda}=\nabla f\left(\bar{x}\right)^{t}d+\left\Vert d\right\Vert \alpha\left(\bar{x},\lambda d\right)
\]

\end_inset

Since 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}d<0$
\end_inset

 and 
\begin_inset Formula $\alpha\rightarrow0$
\end_inset

 as 
\begin_inset Formula $\lambda\rightarrow0$
\end_inset

 there exists 
\begin_inset Formula $\epsilon$
\end_inset

 such that 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}d+\left\Vert d\right\Vert \alpha\left(\bar{x},\lambda d\right)<0$
\end_inset

 (we can make 
\begin_inset Formula $\left\Vert d\right\Vert \alpha\left(\bar{x},\lambda d\right)$
\end_inset

 smaller than 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}d$
\end_inset

) for all 
\begin_inset Formula $0<\lambda<\epsilon$
\end_inset

 and therefore 
\begin_inset Formula 
\[
\frac{f\left(\bar{x}+\lambda d\right)-f\left(\bar{x}\right)}{\lambda}<0
\]

\end_inset

 and since 
\begin_inset Formula $\lambda>0$
\end_inset

 we have that 
\begin_inset Formula $f\left(\bar{x}+\lambda d\right)-f\left(\bar{x}\right)<0$
\end_inset

.
\end_layout

\begin_layout Corollary*
If 
\begin_inset Formula $\bar{x}$
\end_inset

 is a local minimum then 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset

.
\end_layout

\begin_layout Proof
Towards a contradiction suppose 
\begin_inset Formula $\nabla f\left(\bar{x}\right)\neq0$
\end_inset

 and set 
\begin_inset Formula $d=-\nabla f\left(\bar{x}\right)$
\end_inset

.
 Then 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}d=-\left\Vert \nabla f\left(\bar{x}\right)\right\Vert ^{2}<0$
\end_inset

, hence satisfying the previous result for this direction, hence 
\begin_inset Formula $d$
\end_inset

 is an improving direction contradicting that 
\begin_inset Formula $\bar{x}$
\end_inset

 is a local minimum.
\end_layout

\begin_layout Standard

\series bold
The converse is not true!
\end_layout

\begin_layout Standard
Second order necessary conditions
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $f$
\end_inset

 be twice differentiable at 
\begin_inset Formula $\bar{x}$
\end_inset

.
 If 
\begin_inset Formula $\bar{x}$
\end_inset

 is a local minimum of 
\begin_inset Formula $f$
\end_inset

 then 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset

 and 
\begin_inset Formula $\nabla^{2}f\left(\bar{x}\right)$
\end_inset

 is psd.
\end_layout

\begin_layout Proof
First part follows from above.
 For the second part consider and arbitrary direction 
\begin_inset Formula 
\[
f\left(\bar{x}+\lambda d\right)=f\left(\bar{x}\right)+\lambda\nabla f\left(\bar{x}\right)^{t}d+\frac{1}{2}\lambda^{2}d^{t}H\left(\bar{x}\right)d+\lambda^{2}\left\Vert d\right\Vert ^{2}\alpha\left(\bar{x},\lambda d\right)
\]

\end_inset

Again 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset

 and so
\begin_inset Formula 
\[
\frac{f\left(\bar{x}+\lambda d\right)-f\left(\bar{x}\right)}{\lambda^{2}}=+\frac{1}{2}d^{t}H\left(\bar{x}\right)d+\left\Vert d\right\Vert ^{2}\alpha\left(\bar{x},\lambda d\right)
\]

\end_inset

Since 
\begin_inset Formula $\bar{x}$
\end_inset

 is local minimum 
\begin_inset Formula $f\left(\bar{x}+\lambda d\right)-f\left(\bar{x}\right)\geq0$
\end_inset

 for sufficiently small 
\begin_inset Formula $\lambda$
\end_inset

.
 Thus 
\begin_inset Formula 
\[
\frac{1}{2}d^{t}H\left(\bar{x}\right)d+\left\Vert d\right\Vert ^{2}\alpha\left(\bar{x},\lambda d\right)\geq0
\]

\end_inset

Taking the limit as 
\begin_inset Formula $\lambda\rightarrow0$
\end_inset

 we get that 
\begin_inset Formula $d^{t}H\left(\bar{x}\right)d\geq0$
\end_inset

 for all directions 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
The converse is not true! But there is a partial converse
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $f$
\end_inset

 be twice differentiable at 
\begin_inset Formula $\bar{x}$
\end_inset

.
 If 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset

 and 
\begin_inset Formula $\nabla^{2}f\left(\bar{x}\right)$
\end_inset

 pd then 
\begin_inset Formula $\bar{x}$
\end_inset

 is a strict local minimum.
\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $f$
\end_inset

 is twice differentiable at 
\begin_inset Formula $\bar{x}$
\end_inset

 
\begin_inset Formula 
\[
f\left(\bar{x}+\lambda d\right)=f\left(\bar{x}\right)+\lambda\nabla f\left(\bar{x}\right)^{t}d+\frac{1}{2}\lambda^{2}d^{t}H\left(\bar{x}\right)d+\lambda^{2}\left\Vert d\right\Vert ^{2}\alpha\left(\bar{x},\lambda d\right)
\]

\end_inset

Towards a contradiction suppose 
\begin_inset Formula $\bar{x}$
\end_inset

 is not a strict local minimum.
 We want find a direction along which 
\begin_inset Formula $d^{t}Hd\leq0$
\end_inset

.
 Since 
\begin_inset Formula $\bar{x}$
\end_inset

 is not a strict local minimum there exists a sequence 
\begin_inset Formula $x_{k}$
\end_inset

 such that 
\begin_inset Formula $f\left(x_{k}\right)\le f\left(\bar{x}\right)$
\end_inset

.
 Denote 
\begin_inset Formula 
\[
d_{k}=\frac{x_{k}-\bar{x}}{\left\Vert x_{k}-\bar{x}\right\Vert }
\]

\end_inset

and noting that 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset

 we have that 
\begin_inset Formula 
\[
\frac{1}{2}d_{k}^{t}H\left(\bar{x}\right)d+\alpha\left(\bar{x},x_{k}-\bar{x}\right)\leq0
\]

\end_inset

But 
\begin_inset Formula $d_{k}$
\end_inset

 is bounded (norm 1) and therefore there exists a subsequence that converges
 to a direction 
\begin_inset Formula $d$
\end_inset

 such that 
\begin_inset Formula $d^{t}Hd\leq0$
\end_inset

.
\end_layout

\begin_layout Standard
In certain instances we can say stronger things: 
\end_layout

\begin_layout Theorem*
Let 
\begin_inset Formula $f$
\end_inset

 be 
\series bold
convex
\series default
 and differentiable at 
\begin_inset Formula $\bar{x}$
\end_inset

.
 Then 
\begin_inset Formula $\bar{x}$
\end_inset

 is a global minimum iff 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset


\end_layout

\begin_layout Proof
By corollary to two theorems ago if global minimum then 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset

.
 For the converse suppose that 
\begin_inset Formula $\nabla f\left(\bar{x}\right)=0$
\end_inset

 so that 
\begin_inset Formula $\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)=0$
\end_inset

.
 By convexity 
\begin_inset Formula 
\[
f\left(x\right)\geq f\left(\bar{x}\right)+\nabla f\left(\bar{x}\right)^{t}\left(x-\bar{x}\right)=f\left(\bar{x}\right)
\]

\end_inset


\end_layout

\end_body
\end_document
