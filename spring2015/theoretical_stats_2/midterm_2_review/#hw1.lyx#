#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass amsart
\begin_preamble

%
\usepackage{amsfonts}
\usepackage{cancel}
\usepackage{mathabx}
\usepackage{nopageno}%%%  The following few lines affect the margin sizes. 
\usepackage{bm}
\addtolength{\topmargin}{-.5in}
\setlength{\textwidth}{6in}       
\setlength{\oddsidemargin}{.25in}              
\setlength{\evensidemargin}{.25in}         
  
\setlength{\textheight}{9in}
\renewcommand{\baselinestretch}{1}
\reversemarginpar   
%
%
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 1
\use_mhchem 0
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Midterm 2
\end_layout

\begin_layout Standard
Four problems
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Find the posterior distribution for a given Bayesian model.

\series default
 A Bayesian model includes prior information and updates the prior with
 the data, i.e.
 
\begin_inset Formula 
\begin{align*}
\pi\left(\boldsymbol{\theta}|\mathbf{x}\right) & =\frac{f\left(\mathbf{x}|\boldsymbol{\theta}\right)\pi\left(\boldsymbol{\theta}\right)}{m\left(\mathbf{x}\right)}\\
 & =\frac{f\left(\mathbf{x}|\boldsymbol{\theta}\right)\pi\left(\boldsymbol{\theta}\right)}{\int f\left(\mathbf{x}|\boldsymbol{\theta}\right)\pi\left(\boldsymbol{\theta}\right)d\boldsymbol{\theta}}
\end{align*}

\end_inset

So practice marginalizing over the parameter.
\end_layout

\begin_layout Enumerate

\series bold
Derive a property of the posterior distribution (e.g.
 mean, variance).

\series default
 So just practice computing summary statistics of the posterior.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Provide a complete sufficient statistic in a given exponential family model.

\series default
 
\end_layout

\begin_deeper
\begin_layout Theorem*
Let 
\begin_inset Formula $X_{i}$
\end_inset

 be iid from an exponential family with common pdf
\begin_inset Formula 
\[
f\left(x|\boldsymbol{\theta}\right)=h\left(x\right)c\left(\boldsymbol{\theta}\right)e^{\sum_{j}w_{j}\left(\boldsymbol{\theta}\right)t_{j}\left(x\right)}
\]

\end_inset

Then 
\begin_inset Formula 
\[
T\left(\mathbf{X}\right)=\left(\sum_{i}t_{1}\left(X_{i}\right),\dots,\sum_{i}t_{m}\left(X_{i}\right)\right)
\]

\end_inset

is complete sufficient if 
\begin_inset Formula $W$
\end_inset

 is invertible and the parameter space contains an open set.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Find an unbiased estimator of a function of the underyling parameter.

\series default
 So MLE or MOM estimator.
 MLE is invariant under transformation so maybe just find the MLE and then
 take a function of it?
\end_layout

\begin_layout Enumerate

\series bold
Find a UMVUE estimator of a function of a function of the underlying parameter.
 
\series default
Conditioning an unbiased estimator on a complete sufficient statistic produces
 the UMVUE.
 Since in the first a complete sufficient statistic will have been calculated,
 conditioning the result from the second part on it will produce the UMVUE.
 Let 
\begin_inset Formula $T$
\end_inset

 be the complete sufficient statistic computed in the first part and 
\begin_inset Formula $W$
\end_inset

 be the unbiased statistic computed in the second part.
 Then 
\begin_inset Formula 
\[
\phi\left(T\right)=E\left(W|T\right)
\]

\end_inset

 is UMVUE.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Find the Cramer-Rao lower bound for unbiased estimators of a function of
 the underlying parameter in the model.
 
\series default
The Cramer-Rao lower bound is 
\begin_inset Formula 
\[
\text{Var}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left(\frac{d}{d\theta}E_{\theta}W\left(\mathbf{X}\right)\right)^{2}}{E_{\theta}\left(\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right)}
\]

\end_inset

and if 
\begin_inset Formula $X_{i}$
\end_inset

 is iid then
\begin_inset Formula 
\[
\text{Var}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left(\frac{d}{d\theta}E_{\theta}W\left(\mathbf{X}\right)\right)^{2}}{nE_{\theta}\left(\left(\frac{\partial}{\partial\theta}\log f\left(X|\theta\right)\right)^{2}\right)}
\]

\end_inset


\end_layout

\begin_layout Enumerate

\series bold
Determine is the Cramer-Rao lower bound is attained.

\series default
 The conditions for attainment are that 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 be unbiased (duh) and 
\begin_inset Formula $X_{i}$
\end_inset

 be iid and that 
\begin_inset Formula 
\[
a\left(\theta\right)\left(W\left(\mathbf{x}\right)-\tau\left(\theta\right)\right)=\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{x}\right)
\]

\end_inset

 where 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 is the likelihood function, just 
\begin_inset Formula $\prod_{i}f\left(x_{i}|\theta\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Derive the likelihood ratio test (with a given size 
\begin_inset Formula $\alpha$
\end_inset

) for a testing problem.
 
\series default
If 
\begin_inset Formula $\Theta_{o}$
\end_inset

 and 
\begin_inset Formula $\Theta_{o}^{c}$
\end_inset

 are the null space and the alternative hypothesis space then the hypotheses
 are 
\begin_inset Formula 
\[
H_{0}\,:\,\theta\in\Theta_{0}\: H_{1}\,:\,\theta\in\Theta_{0}^{c}
\]

\end_inset

The LRT is 
\begin_inset Formula 
\begin{align*}
\lambda\left(\mathbf{x}\right) & =\frac{\sup_{\theta\in\Theta_{0}}L\left(\theta|\mathbf{x}\right)}{\sup_{\theta\in\Theta}L\left(\theta|\mathbf{x}\right)}\\
 & =\frac{L\left(\hat{\theta}_{0}|\mathbf{x}\right)}{L\left(\hat{\theta}|\mathbf{x}\right)}
\end{align*}

\end_inset

where 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the unrestricted MLE for the model and 
\begin_inset Formula $\hat{\theta}_{0}$
\end_inset

is the restricted MLE.
 The test dictates that we should reject 
\begin_inset Formula $H_{0}$
\end_inset

 for some value such that 
\begin_inset Formula $\lambda\left(\mathbf{x}\right)<c$
\end_inset

.
 Then the power function of the test is 
\begin_inset Formula 
\begin{align*}
\beta\left(\theta\right) & =P_{\theta}\left(\text{Making Type I error}\right)\\
 & =P_{\theta}\left(\text{Rejecting when }\theta\in\Theta_{0}\right)\\
 & =P_{\theta}\left(\mathbf{X}\in\text{Rejection region as dictated by the test}\right)
\end{align*}

\end_inset

Then the size is 
\begin_inset Formula 
\[
\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)
\]

\end_inset

Note that this sup is computed over the null space.
\end_layout

\begin_layout Enumerate

\series bold
Same.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[7.24]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}\sim\text{Poisson}\left(\Lambda\right)$
\end_inset

 and let 
\begin_inset Formula $\Lambda\sim\text{Gamma}\left(\alpha,\beta\right)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
The posterior distribution is 
\begin_inset Formula 
\begin{align*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{f\left(\mathbf{x}|\theta\right)\pi\left(\lambda\right)}{m\left(\mathbf{x}\right)}\\
 & =\frac{\frac{1}{\prod_{i=1}^{n}x_{i}!}e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}}{\int_{0}^{\infty}\left(\frac{1}{\prod_{i=1}^{n}x_{i}!}e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}\right)d\lambda}\\
 & =\frac{\frac{1}{\prod_{i=1}^{n}x_{i}!}e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}}{\int_{0}^{\infty}\left(\frac{1}{\prod_{i=1}^{n}x_{i}!}e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}\right)d\lambda}\\
 & =ce^{-\lambda/\left(n+\frac{1}{\beta}\right)^{-1}}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\\
 & =\frac{1}{\Gamma\left(\sum_{i=1}^{n}x_{i}+\alpha\right)\left(n+\frac{1}{\beta}\right)^{-\left(\sum_{i=1}^{n}x_{i}+\alpha\right)}}e^{-\lambda/\left(n+\frac{1}{\beta}\right)^{-1}}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}
\end{align*}

\end_inset

Define 
\begin_inset Formula $y=\sum_{i=1}^{n}y_{i}$
\end_inset

 so
\begin_inset Formula 
\begin{align*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{1}{\Gamma\left(\sum_{i=1}^{n}x_{i}+\alpha\right)\left(n+\frac{1}{\beta}\right)^{-\left(\sum_{i=1}^{n}x_{i}+\alpha\right)}}e^{-\lambda/\left(n+\frac{1}{\beta}\right)^{-1}}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\\
 & =\frac{1}{\Gamma\left(y+\alpha\right)\left(1\Bigg/\left(n+\frac{1}{\beta}\right)\right)^{\left(y+\alpha\right)}}e^{-\lambda/\left(n+\frac{1}{\beta}\right)^{-1}}\lambda^{y+\alpha-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Since the posterior distribution is 
\begin_inset Formula $\text{Gamma}\left(y+\alpha,\frac{\beta}{\beta n+1}\right)$
\end_inset

 the posterior mean is 
\begin_inset Formula $\left(y+\alpha\right)\left(\frac{\beta}{\beta n+1}\right)$
\end_inset

 and the posterior variance is 
\begin_inset Formula $\left(y+\alpha\right)\left(\frac{\beta}{\beta n+1}\right)^{2}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[7.60]
\end_layout

\end_inset

 Let 
\begin_inset Formula $X_{i}\sim\text{Gamma}\left(\alpha,\beta\right)$
\end_inset

 with 
\begin_inset Formula $\alpha$
\end_inset

 known.
\end_layout

\begin_deeper
\begin_layout Enumerate
A complete sufficient statistic.
 By
\begin_inset Formula 
\begin{align*}
f\left(\mathbf{x}|\theta\right) & =\frac{1}{\left(\Gamma\left(\alpha\right)\beta^{\alpha}\right)^{n}}\left(\prod_{i=1}^{n}x_{i}\right)^{\alpha-1}e^{-\frac{1}{\beta}\sum_{i=1}^{n}x_{i}}\\
 & =\frac{1}{\left(\Gamma\left(\alpha\right)\beta^{\alpha}\right)^{n}}e^{\left(\alpha-1\right)\sum_{i=1}^{n}\log x_{i}}e^{-\frac{1}{\beta}\sum_{i=1}^{n}x_{i}}
\end{align*}

\end_inset

and
\begin_inset Formula 
\begin{align*}
f\left(x|\theta\right) & =\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\left(x\right)^{\alpha-1}e^{-\frac{1}{\beta}x}\\
 & =\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}e^{\left(\alpha-1\right)\log x}e^{-\frac{1}{\beta}x}
\end{align*}

\end_inset

According to Theorem blah blah blah with 
\begin_inset Formula $h\left(\mathbf{x}\right)=1,c\left(\alpha,\beta\right)=\Gamma\left(\alpha\right)\beta^{\alpha},w_{1}=\left(\alpha-1\right),t_{1}=\log x,w_{2}=-\frac{1}{\beta},t_{2}=x$
\end_inset

 the complete sufficient statistic for 
\begin_inset Formula $\left(\alpha,\beta\right)$
\end_inset

 is 
\begin_inset Formula $\left(\sum_{i=1}^{n}\log X_{i},\sum_{i=1}^{n}X_{i}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
I need to construct an unbiased estimator of 
\begin_inset Formula $1/\beta$
\end_inset

.
 The MOM estimators for 
\begin_inset Formula $\left(\alpha,\beta\right)$
\end_inset


\begin_inset Formula 
\begin{align*}
\bar{X} & =E\left(X\right)=\hat{\alpha}\hat{\beta}\\
\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2} & =E\left(X^{2}\right)=\hat{\alpha}\hat{\beta}^{2}+\left(\hat{\alpha}\hat{\beta}\right)^{2}\\
 & =\left(\frac{1}{\hat{\alpha}}+1\right)\left(\bar{X}\right)^{2}
\end{align*}

\end_inset

So 
\begin_inset Formula 
\begin{align*}
\hat{\alpha} & =\frac{1}{\frac{1}{\bar{X}n}\sum_{i=1}^{n}X_{i}^{2}-1}\\
 & =\frac{n\bar{X}}{\sum_{i=1}^{n}\left(X_{i}^{2}-X_{i}\right)}\\
\hat{\beta} & =\frac{\sum_{i=1}^{n}\left(X_{i}^{2}-X_{i}\right)}{n}
\end{align*}

\end_inset

God damn it.
 The MOM isn't invariant.
 Calculating the MLE by hand for 
\begin_inset Formula $\beta$
\end_inset

.
 Actually an even bigger idiot.
 With 
\begin_inset Formula $\alpha$
\end_inset

 known 
\begin_inset Formula $\bar{X}/\alpha$
\end_inset

 is unbiased for 
\begin_inset Formula $\beta$
\end_inset

.
 
\begin_inset Formula $X_{i}\sim\Gamma\left(\alpha,\beta\right)$
\end_inset

 then 
\begin_inset Formula $\sum_{i=1}^{n}X_{i}\sim\Gamma\left(n\alpha,\beta\right)$
\end_inset

 and 
\begin_inset Formula 
\begin{align*}
\frac{1}{\sum_{i=1}^{n}X_{i}} & \sim\text{Inv}\Gamma\left(n\alpha,\frac{1}{\beta}\right)
\end{align*}

\end_inset

and 
\begin_inset Formula 
\[
E\left(\frac{1}{\sum_{i=1}^{n}X_{i}}\right)=\frac{1}{\beta}\frac{1}{n\alpha-1}
\]

\end_inset

so with 
\begin_inset Formula $W\left(\mathbf{X}\right)=\frac{n\alpha-1}{\sum_{i=1}^{n}X_{i}}$
\end_inset

 
\begin_inset Formula 
\[
E\left(W\left(\mathbf{X}\right)\right)=\left(n\alpha-1\right)E\left(\frac{1}{\sum_{i=1}^{n}X_{i}}\right)=\frac{1}{\beta}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Finally conditioning 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 on 
\begin_inset Formula $\bar{X}$
\end_inset

 is just 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 since 
\begin_inset Formula $\frac{n\alpha-1}{\sum_{i=1}^{n}X_{i}}=\frac{\alpha-1/n}{\frac{1}{n}\sum_{i=1}^{n}X_{i}}=\frac{\alpha-1/n}{\bar{X}}$
\end_inset

 and conditioning on a function of 
\begin_inset Formula $\bar{X}$
\end_inset

 is just the function again.
\end_layout

\begin_layout Enumerate
Interestingly enough it doesn't attain the Cramer-Rao lower bound.
 So first of all 
\begin_inset Formula 
\begin{align*}
\left(\frac{d}{d\beta}E_{\beta}\left(W\left(\mathbf{X}\right)\right)\right)^{2} & =\left(\frac{d}{d\beta}\frac{1}{\beta}\right)^{2}\\
 & =\frac{1}{\beta^{4}}
\end{align*}

\end_inset

and
\begin_inset Formula 
\begin{align*}
nE_{\theta}\left(\left(\frac{\partial}{\partial\theta}\log f\left(X|\theta\right)\right)^{2}\right) & =nE_{\beta}\left(\left[\frac{\partial}{\partial\beta}\left(-\log\Gamma\left(\alpha\right)-\alpha\log\left(\beta\right)+\left(\alpha-1\right)\log X-\frac{1}{\beta}X\right)\right]^{2}\right)\\
 & =nE_{\beta}\left(\left[-\frac{\alpha}{\beta}+\frac{X}{\beta^{2}}\right]^{2}\right)\\
 & =nE_{\beta}\left(\frac{\alpha^{2}}{\beta^{2}}+\frac{X^{2}}{\beta^{4}}-\frac{2\alpha}{\beta^{3}}X\right)\\
 & =n\left(\frac{\alpha^{2}}{\beta^{2}}+\frac{1}{\beta^{4}}\left(\alpha\beta^{2}+\left(\alpha\beta\right)^{2}\right)-\frac{2\alpha}{\beta^{3}}\left(\alpha\beta\right)\right)\\
 & =\frac{n\alpha}{\beta^{2}}
\end{align*}

\end_inset

and so the lower bound is
\begin_inset Formula 
\[
\frac{1/\beta^{4}}{n\alpha/\beta^{2}}=\frac{1}{\beta^{2}n\alpha}
\]

\end_inset

Then 
\begin_inset Formula $\frac{n\alpha-1}{\sum_{i=1}^{n}X_{i}}\sim\text{Inv}\Gamma\left(n\alpha,\frac{n\alpha-1}{\beta}\right)$
\end_inset

 implies 
\begin_inset Formula 
\begin{align*}
\text{Var}\left(W\left(\mathbf{X}\right)\right) & =\frac{\left(\frac{n\alpha-1}{\beta}\right)^{2}}{\left(n\alpha-1\right)^{2}\left(n\alpha-2\right)}\\
 & =\frac{1}{\left(n\alpha-2\right)}\frac{1}{\beta^{2}}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[7.44]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}\sim\text{n}\left(\theta,1\right)$
\end_inset

.
 Find the best unbiased estimator of 
\begin_inset Formula $\theta^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
If 
\begin_inset Formula $\bar{X}^{2}-1/n$
\end_inset

 is unbiased for 
\begin_inset Formula $\theta^{2}$
\end_inset

 then it's UMVUE by Lehmann-Scheffe.
 First of all 
\begin_inset Formula $\bar{X}\sim\text{n}\left(\theta,\frac{1}{n}\right)$
\end_inset

 so 
\begin_inset Formula 
\[
E\left(\bar{X}^{2}\right)=\text{Var}\left(\bar{X}\right)+\left(E\bar{X}\right)^{2}=\frac{1}{n}+\theta^{2}
\]

\end_inset

and hence
\begin_inset Formula 
\[
E\left(\bar{X}^{2}-\frac{1}{n}\right)=\frac{1}{n}+\theta^{2}-\frac{1}{n}=\theta^{2}
\]

\end_inset

so since 
\begin_inset Formula $\bar{X}$
\end_inset

 is complete sufficient for 
\begin_inset Formula $\theta$
\end_inset

 for this model it's the case that 
\begin_inset Formula $\bar{X}^{2}-1/n$
\end_inset

 is UMVUE for 
\begin_inset Formula $\theta^{2}$
\end_inset

.
 To calculate its variance we need to compute 
\begin_inset Formula 
\begin{align*}
\text{Var}\left(\bar{X}^{2}-\frac{1}{n}\right) & =\text{Var}\left(\bar{X}^{2}\right)=E\left(\bar{X}^{4}\right)-\left(E\bar{X}^{2}\right)^{4}
\end{align*}

\end_inset

Stein's lemma is for 
\begin_inset Formula $X\sim\text{n}\left(\theta,\sigma^{2}\right)$
\end_inset

 it's the case that 
\begin_inset Formula $E\left[g\left(X\right)\left(X-\theta\right)\right]=\sigma^{2}Eg'\left(X\right)$
\end_inset

.
 So then
\begin_inset Formula 
\begin{align*}
E\left(\bar{X}^{4}\right) & =E\left[\bar{X}^{3}\left(\bar{X}-\theta\right)\right]+E\left(\bar{X}^{3}\theta\right)\\
E\left[\bar{X}^{3}\left(\bar{X}-\theta\right)\right] & =3\left(\frac{1}{n}\right)E\bar{X}^{2}=\frac{3}{n}\left(\theta^{2}+\frac{1}{n}\right)=\frac{3\theta^{2}}{n}+\frac{3}{n^{2}}\\
\theta E\bar{X}^{3} & =\theta\left(E\bar{X}^{2}\left(\bar{X}-\theta\right)+\theta E\bar{X}^{2}\right)=\theta\left(\frac{2}{n}E\bar{X}+\theta\left(\theta^{2}+\frac{1}{n}\right)\right)\\
 & =\theta\left(\frac{2}{n}\theta+\theta\left(\theta^{2}+\frac{1}{n}\right)\right)=\frac{3}{n}\theta^{2}+\theta^{4}\\
E\left(\bar{X}^{4}\right) & =\frac{3\theta^{2}}{n}+\frac{3}{n^{2}}+\frac{3}{n}\theta^{2}+\theta^{4}\\
 & =\frac{3}{n^{2}}+6\left(\frac{1}{n}\right)\theta^{2}+\theta^{4}
\end{align*}

\end_inset

So 
\begin_inset Formula 
\begin{align*}
\text{Var}\left(\bar{X}^{2}\right) & =\frac{3}{n^{2}}+6\left(\frac{1}{n}\right)\theta^{2}+\theta^{4}-\left(E\bar{X}^{2}\right)^{2}\\
 & =\frac{3}{n^{2}}+6\left(\frac{1}{n}\right)\theta^{2}+\theta^{4}-\left(\theta^{2}+\frac{1}{n}\right)^{2}\\
 & =\frac{3}{n^{2}}+6\left(\frac{1}{n}\right)\theta^{2}+\theta^{4}-\left(\theta^{4}+\frac{1}{n^{2}}+2\frac{\theta^{2}}{n}\right)\\
 & =\frac{2}{n^{2}}+4\left(\frac{1}{n}\right)\theta^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
The Cramer-Rao lower bound
\begin_inset Formula 
\begin{align*}
\text{Var}\left(W\left(\mathbf{X}\right)\right) & \geq\frac{\left(\frac{\partial}{\partial\theta}E_{\theta}W\left(\mathbf{X}\right)\right)^{2}}{E\left[\left(\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{X}\right)\right)^{2}\right]}\\
 & =\frac{\left(\frac{\partial}{\partial\theta}\theta^{2}\right)^{2}}{nE\left[\left(\frac{\partial}{\partial\theta}\left(-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2}\left(X-\theta\right)^{2}\right)\right)^{2}\right]}\\
 & =\frac{4\theta^{2}}{nE\left[\left(-\left(X-\theta\right)\right)^{2}\right]}\\
 & =\frac{4\theta^{2}}{nE\left[\left(X-\theta\right)^{2}\right]}\\
 & =\frac{4\theta^{2}}{n}
\end{align*}

\end_inset

and so 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\frac{2}{n^{2}}+4\left(\frac{1}{n}\right)\theta^{2}>\frac{4\theta^{2}}{n}$
\end_inset

.
 An alternative way to compute the Cramer-Rao lower bound is using the identity
 
\begin_inset Formula 
\[
E\left[\left(\frac{\partial}{\partial\theta}\log L\left(\theta|X\right)\right)^{2}\right]=-E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log L\left(\theta|X\right)\right]
\]

\end_inset

 So
\begin_inset Formula 
\begin{align*}
-E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log L\left(\theta|X\right)\right] & =-E\left[\frac{\partial^{2}}{\partial\theta^{2}}\left(-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2}\left(X-\theta\right)^{2}\right)\right]\\
 & =-E\left[\frac{\partial}{\partial\theta}\left(X-\theta\right)\right]\\
 & =1
\end{align*}

\end_inset

and hence 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula 
\begin{align*}
\text{Var}\left(W\left(\mathbf{X}\right)\right) & \geq\frac{\left(\frac{\partial}{\partial\theta}E_{\theta}W\left(\mathbf{X}\right)\right)^{2}}{-nE\left[\frac{\partial^{2}}{\partial\theta^{2}}\log L\left(\theta|X\right)\right]}\\
 & =\frac{4\theta^{2}}{n}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[8.5]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}$
\end_inset

 be such that 
\begin_inset Formula 
\[
f\left(x|\theta,\nu\right)=\frac{\theta\nu^{\theta}}{x^{\theta+1}}I\left(x\right)_{\left[\nu,\infty\right)}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
The likelihood is 
\begin_inset Formula 
\[
L\left(\theta,\nu|\mathbf{x}\right)=\frac{\theta^{n}\nu^{n\theta}}{\left(\prod_{i=1}^{n}x_{i}\right)^{\theta+1}}I\left(x_{\left(1\right)}\right)_{\left[\nu,\infty\right)}
\]

\end_inset

Then the log-likelihood is 
\begin_inset Formula 
\[
\log L\left(\theta,\nu|\mathbf{x}\right)=n\log\theta+n\theta\log\nu-\left(\theta+1\right)\sum_{i=1}^{n}\log x_{i}
\]

\end_inset

This is an increasing function of 
\begin_inset Formula $\nu<x_{\left(1\right)}$
\end_inset

.
 So 
\begin_inset Formula $\hat{\nu}=x_{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula 
\begin{align*}
\partial_{\theta}\log L\left(\theta,\nu|\mathbf{x}\right) & =\frac{n}{\theta}+n\log x_{\left(1\right)}-\sum_{i=1}^{n}\log x_{i}\\
0 & =\frac{n}{\theta}+n\log x_{\left(1\right)}-\sum_{i=1}^{n}\log x_{i}\\
\sum_{i=1}^{n}\log x_{i}-n\log x_{\left(1\right)} & =\frac{n}{\theta}\\
\hat{\theta} & =\frac{n}{\sum_{i=1}^{n}\log x_{i}-n\log x_{\left(1\right)}}\\
\hat{\theta} & =\frac{n}{\log\left(\frac{\prod_{i=1}^{n}x_{i}}{x_{\left(1\right)^{n}}}\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $\Theta_{0}=\left\{ 1\right\} $
\end_inset

 and 
\begin_inset Formula $\Theta_{0}^{c}=\mathbb{R}-\left\{ 1\right\} $
\end_inset

.
 Then
\begin_inset Formula 
\[
H_{0}\,:\,\theta=1,\nu\text{ uknown}\: H_{1}\,:\,\theta\neq1,\nu\text{ uknown}
\]

\end_inset

Then the likelihood ratio test statistic is 
\begin_inset Formula 
\begin{align*}
\lambda\left(\mathbf{x}\right) & =\frac{L\left(1,\nu|\mathbf{x}\right)}{L\left(\hat{\theta},\nu|\mathbf{x}\right)}\\
 & =\frac{\frac{x_{\left(1\right)}^{n}}{\left(\prod_{i=1}^{n}x_{i}\right)^{2}}}{\frac{\left(n/T\right)^{n}x_{\left(1\right)}^{n^{2}/T}}{\left(\prod_{i=1}^{n}x_{i}\right)^{\left(n/T\right)+1}}}\\
 & =\left(x_{\left(1\right)}^{n}e^{T}\right)^{\left(n/T\right)-1}\frac{x_{\left(1\right)}^{n}}{\left(n/T\right)^{n}x_{\left(1\right)}^{n^{2}/T}}\\
 & =e^{n-T}\left(\frac{T}{n}\right)^{n}
\end{align*}

\end_inset

Then 
\begin_inset Formula $e^{n-T}\left(\frac{T}{n}\right)^{n}$
\end_inset

 is an increasing function 
\begin_inset Formula $T$
\end_inset

 for 
\begin_inset Formula $T\leq n$
\end_inset

 since 
\begin_inset Formula 
\begin{align*}
\partial_{T}\log\lambda\left(\mathbf{x}\right) & =\partial_{T}\left(\left(n-T\right)+n\log\left(\frac{T}{n}\right)\right)\\
 & =-1+\frac{n}{T/n}\frac{1}{n}\\
 & =\frac{n}{T}-1
\end{align*}

\end_inset

 and decreasing for 
\begin_inset Formula $T>n$
\end_inset

.
 Therefore 
\begin_inset Formula $T\leq c$
\end_inset

 is equivalent to 
\begin_inset Formula $T\leq c_{1}$
\end_inset

 and 
\begin_inset Formula $T\ge c_{2}$
\end_inset

.
\end_layout

\begin_layout Enumerate
A size 
\begin_inset Formula $\alpha$
\end_inset

 test means 
\begin_inset Formula 
\[
\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)=P\left(T\leq c_{1}\vee\, T\geq c_{1}\right)
\]

\end_inset

Firstly 
\begin_inset Formula $\left(X_{1},\dots,X_{n}|X_{\left(1\right)}\right)=\left(X_{\left(1\right)},X_{\left(2\right)},\dots,X_{\left(n\right)}|X_{\left(1\right)}\right)$
\end_inset

.
 So then 
\begin_inset Formula $\left(X_{\left(2\right)},\dots,X_{\left(n\right)}|X_{\left(1\right)}\right)$
\end_inset

 are still iid and so are 
\begin_inset Formula $\left(\frac{X_{\left(2\right)}}{X_{\left(1\right)},}\dots,\frac{X_{\left(n\right)}}{X_{\left(1\right)}}|X_{\left(1\right)}\right)$
\end_inset

.
 The joint distribution of 
\begin_inset Formula $\left(X_{1},\dots,X_{n}\right)$
\end_inset

 is 
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}\right)=\frac{\theta^{n}\nu^{n\theta}}{\left(\prod_{i=1}^{n}x_{i}\right)^{\theta+1}}I\left(x_{\left(1\right)}\right)_{\left[\nu,\infty\right)}
\]

\end_inset

Because 
\begin_inset Formula $\nu>x_{\left(1\right)}$
\end_inset

 and iid of 
\begin_inset Formula $X_{i}$
\end_inset

 the marginal of 
\begin_inset Formula $X_{\left(1\right)}$
\end_inset

 
\begin_inset Formula 
\begin{align*}
f_{X_{\left(1\right)}}\left(x_{\left(1\right)}\right) & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\underbrace{\int_{x_{\left(1\right)}}^{\infty}\cdots\int_{x_{\left(1\right)}}^{\infty}}_{n-1}\prod_{i=2}^{n}f_{X_{\left(i\right)}}\left(x_{\left(i\right)}\right)d_{x_{\left(i\right)}}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\left(P\left(X_{i}>x_{\left(1\right)}\right)\right)^{n-1}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\left(1-P\left(X_{i}\leq x_{\left(1\right)}\right)\right)^{n-1}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\left[\left(\frac{\nu}{x_{\left(1\right)}}\right)^{\theta}\right]^{n-1}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\left(\frac{\nu}{x_{\left(1\right)}}\right)^{n\theta-\theta}\\
 & =\frac{\theta\nu^{n\theta}}{\left(x_{\left(1\right)}\right)^{n\theta+1}}
\end{align*}

\end_inset

and so 
\begin_inset Formula 
\begin{align*}
f_{\left(X_{\left(2\right)},\dots,X_{\left(n\right)}\bigg|X_{\left(1\right)}\right)}\left(x_{\left(2\right)},\dots,x_{\left(n\right)}\big|x_{\left(1\right)}\right) & =\frac{f_{\mathbf{X}}\left(\mathbf{x}\right)}{f_{X_{\left(1\right)}}\left(x_{\left(1\right)}\right)}\\
 & =\frac{\frac{\theta^{n}\nu^{n\theta}}{\left(\prod_{i=1}^{n}x_{i}\right)^{\theta+1}}}{\frac{\theta\nu^{n\theta}}{\left(x_{\left(1\right)}\right)^{n\theta+1}}}\\
 & \frac{\theta^{n-1}\left(x_{\left(1\right)}\right)^{\theta\left(n-1\right)}}{\left(\prod_{i=2}^{n}x_{\left(i\right)}\right)^{\theta+1}}\\
 & =\prod_{i=2}^{n}\frac{\theta\left(x_{\left(1\right)}\right)^{\theta}}{x_{\left(i\right)}^{\theta+1}}
\end{align*}

\end_inset

Therefore for 
\begin_inset Formula $j\neq1$
\end_inset

 it's the case that 
\begin_inset Formula $X_{\left(j\right)}|X_{\left(1\right)}\sim\text{Pareto}\left(\theta,X_{\left(1\right)}\right)$
\end_inset

.
 The distribution of The joint distribution of 
\begin_inset Formula $\left(X_{\left(1\right)},X_{\left(j\right)}\right)$
\end_inset

 
\begin_inset Formula 
\begin{align*}
f_{X_{\left(1\right)},X_{\left(j\right)}}\left(x_{\left(1\right)},x_{\left(j\right)}\right) & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\underbrace{\int_{x_{\left(1\right)}}^{\infty}\cdots\int_{x_{\left(1\right)}}^{\infty}}_{n-2}\prod_{i\neq1,j}^{n}f_{X_{\left(i\right)}}\left(x_{\left(i\right)}\right)d_{x_{\left(i\right)}}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\frac{\theta\nu^{\theta}}{\left(x_{\left(j\right)}\right)^{\theta+1}}\left(P\left(X_{i}>x_{\left(1\right)}\right)\right)^{n-2}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\frac{\theta\nu^{\theta}}{\left(x_{\left(j\right)}\right)^{\theta+1}}\left(1-P\left(X_{i}\leq x_{\left(1\right)}\right)\right)^{n-2}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\frac{\theta\nu^{\theta}}{\left(x_{\left(j\right)}\right)^{\theta+1}}\left[\left(\frac{\nu}{x_{\left(1\right)}}\right)^{\theta}\right]^{\left(n-1\right)-1}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\frac{\theta\nu^{\theta}}{\left(x_{\left(j\right)}\right)^{\theta+1}}\left(\frac{\nu}{x_{\left(1\right)}}\right)^{\left(n-1\right)\theta-\theta}\\
 & =\frac{\theta^{2}\nu^{n\theta}}{\left(x_{\left(1\right)}\right)^{\left(n-1\right)\theta+1}\left(x_{\left(j\right)}\right)^{\theta+1}}
\end{align*}

\end_inset

Then 
\begin_inset Formula 
\begin{align*}
P\left(\frac{X_{\left(j\right)}}{X_{\left(1\right)}}\leq y\right) & =P\left(X_{\left(j\right)}\leq yX_{\left(1\right)}\right)\\
 & =\int_{\nu}^{\infty}\int_{x_{\left(1\right)}}^{yx_{\left(1\right)}}\frac{\theta^{2}\nu^{n\theta}}{\left(x_{\left(1\right)}\right)^{\left(n-1\right)\theta+1}\left(x_{\left(j\right)}\right)^{\theta+1}}dx_{\left(j\right)}dx_{\left(1\right)}\\
 & =\frac{1-y^{-\theta}}{n}
\end{align*}

\end_inset

and therefore for 
\begin_inset Formula $Y=\frac{X_{\left(j\right)}}{X_{\left(1\right)}}$
\end_inset


\begin_inset Formula 
\[
f_{Y}\left(y\right)=\frac{\theta}{ny^{\theta+1}}
\]

\end_inset


\end_layout

\end_deeper
\end_body
\end_document
