#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass amsart
\begin_preamble

%
\usepackage{amsfonts}
\usepackage{cancel}
\usepackage{mathabx}
\usepackage{nopageno}%%%  The following few lines affect the margin sizes. 
\usepackage{bm}
\addtolength{\topmargin}{-.5in}
\setlength{\textwidth}{6in}       
\setlength{\oddsidemargin}{.25in}              
\setlength{\evensidemargin}{.25in}         
  
\setlength{\textheight}{9in}
\renewcommand{\baselinestretch}{1}
\reversemarginpar   
%
%
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 1
\use_mhchem 0
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Midterm 2
\end_layout

\begin_layout Standard
Four problems
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Find the posterior distribution for a given Bayesian model.

\series default
 A Bayesian model includes prior information and updates the prior with
 the data, i.e.
 
\begin_inset Formula 
\begin{align*}
\pi\left(\boldsymbol{\theta}|\mathbf{x}\right) & =\frac{f\left(\mathbf{x}|\boldsymbol{\theta}\right)\pi\left(\boldsymbol{\theta}\right)}{m\left(\mathbf{x}\right)}\\
 & =\frac{f\left(\mathbf{x}|\boldsymbol{\theta}\right)\pi\left(\boldsymbol{\theta}\right)}{\int f\left(\mathbf{x}|\boldsymbol{\theta}\right)\pi\left(\boldsymbol{\theta}\right)d\boldsymbol{\theta}}
\end{align*}

\end_inset

So practice marginalizing over the parameter.
\end_layout

\begin_layout Enumerate

\series bold
Derive a property of the posterior distribution (e.g.
 mean, variance).

\series default
 So just practice computing summary statistics of the posterior.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Provide a complete sufficient statistic in a given exponential family model.

\series default
 
\end_layout

\begin_deeper
\begin_layout Theorem*
Let 
\begin_inset Formula $X_{i}$
\end_inset

 be iid from an exponential family with common pdf
\begin_inset Formula 
\[
f\left(x|\boldsymbol{\theta}\right)=h\left(x\right)c\left(\boldsymbol{\theta}\right)e^{\sum_{j}w_{j}\left(\boldsymbol{\theta}\right)t_{j}\left(x\right)}
\]

\end_inset

Then 
\begin_inset Formula 
\[
T\left(\mathbf{X}\right)=\left(\sum_{i}t_{1}\left(X_{i}\right),\dots,\sum_{i}t_{m}\left(X_{i}\right)\right)
\]

\end_inset

is complete sufficient if 
\begin_inset Formula $W$
\end_inset

 is invertible and the parameter space contains an open set.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Find an unbiased estimator of a function of the underyling parameter.

\series default
 So MLE or MOM estimator.
 MLE is invariant under transformation so maybe just find the MLE and then
 take a function of it?
\end_layout

\begin_layout Enumerate

\series bold
Find a UMVUE estimator of a function of a function of the underlying parameter.
 
\series default
Conditioning an unbiased estimator on a complete sufficient statistic produces
 the UMVUE.
 Since in the first a complete sufficient statistic will have been calculated,
 conditioning the result from the second part on it will produce the UMVUE.
 Let 
\begin_inset Formula $T$
\end_inset

 be the complete sufficient statistic computed in the first part and 
\begin_inset Formula $W$
\end_inset

 be the unbiased statistic computed in the second part.
 Then 
\begin_inset Formula 
\[
\phi\left(T\right)=E\left(W|T\right)
\]

\end_inset

 is UMVUE.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Find the Cramer-Rao lower bound for unbiased estimators of a function of
 the underlying parameter in the model.
 
\series default
The Cramer-Rao lower bound is 
\begin_inset Formula 
\[
\text{Var}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left(\frac{d}{d\theta}E_{\theta}W\left(\mathbf{X}\right)\right)^{2}}{E_{\theta}\left(\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right)}
\]

\end_inset

and if 
\begin_inset Formula $X_{i}$
\end_inset

 is iid then
\begin_inset Formula 
\[
\text{Var}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left(\frac{d}{d\theta}E_{\theta}W\left(\mathbf{X}\right)\right)^{2}}{nE_{\theta}\left(\left(\frac{\partial}{\partial\theta}\log f\left(X|\theta\right)\right)^{2}\right)}
\]

\end_inset


\end_layout

\begin_layout Enumerate

\series bold
Determine is the Cramer-Rao lower bound is attained.

\series default
 The conditions for attainment are that 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 be unbiased (duh) and 
\begin_inset Formula $X_{i}$
\end_inset

 be iid and that 
\begin_inset Formula 
\[
a\left(\theta\right)\left(W\left(\mathbf{x}\right)-\tau\left(\theta\right)\right)=\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{x}\right)
\]

\end_inset

 where 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 is the likelihood function, just 
\begin_inset Formula $\prod_{i}f\left(x_{i}|\theta\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Derive the likelihood ratio test (with a given size 
\begin_inset Formula $\alpha$
\end_inset

) for a testing problem.
 
\series default
If 
\begin_inset Formula $\Theta_{o}$
\end_inset

 and 
\begin_inset Formula $\Theta_{o}^{c}$
\end_inset

 are the null space and the alternative hypothesis space then the hypotheses
 are 
\begin_inset Formula 
\[
H_{0}\,:\,\theta\in\Theta_{0}\: H_{1}\,:\,\theta\in\Theta_{0}^{c}
\]

\end_inset

The LRT is 
\begin_inset Formula 
\begin{align*}
\lambda\left(\mathbf{x}\right) & =\frac{\sup_{\theta\in\Theta_{0}}L\left(\theta|\mathbf{x}\right)}{\sup_{\theta\in\Theta}L\left(\theta|\mathbf{x}\right)}\\
 & =\frac{L\left(\hat{\theta}_{0}|\mathbf{x}\right)}{L\left(\hat{\theta}|\mathbf{x}\right)}
\end{align*}

\end_inset

where 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the unrestricted MLE for the model and 
\begin_inset Formula $\hat{\theta}_{0}$
\end_inset

is the restricted MLE.
 The test dictates that we should reject 
\begin_inset Formula $H_{0}$
\end_inset

 for some value such that 
\begin_inset Formula $\lambda\left(\mathbf{x}\right)<c$
\end_inset

.
 Then the power function of the test is 
\begin_inset Formula 
\begin{align*}
\beta\left(\theta\right) & =P_{\theta}\left(\text{Making Type I error}\right)\\
 & =P_{\theta}\left(\text{Rejecting when }\theta\in\Theta_{0}\right)\\
 & =P_{\theta}\left(\mathbf{X}\in\text{Rejection region as dictated by the test}\right)
\end{align*}

\end_inset

Then the size is 
\begin_inset Formula 
\[
\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)
\]

\end_inset

Note that this sup is computed over the null space.
\end_layout

\begin_layout Enumerate

\series bold
Same.
\end_layout

\end_deeper
\begin_layout Itemize
Distributions
\end_layout

\begin_deeper
\begin_layout Itemize
Bernoulli
\begin_inset Formula $\left(p\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
pmf: 
\begin_inset Formula $P\left(X=x|p\right)=p^{x}\left(1-p\right)^{x}$
\end_inset


\end_layout

\begin_layout Itemize
mean and variance: 
\begin_inset Formula $EX=p$
\end_inset

 and 
\begin_inset Formula $\text{Var}\left(X\right)=p\left(1-p\right)$
\end_inset


\end_layout

\begin_layout Itemize
MLE 
\begin_inset Formula $\hat{p}=\bar{X}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Binomial
\begin_inset Formula $\left(n,p\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
pmf: 
\begin_inset Formula $P\left(X=x|n,p\right)=\binom{n}{x}p^{x}\left(1-p\right)^{n-x}$
\end_inset


\end_layout

\begin_layout Itemize
mean and variance: 
\begin_inset Formula $EX=np$
\end_inset

 and 
\begin_inset Formula $\text{Var}\left(X\right)=np\left(1-p\right)$
\end_inset


\end_layout

\begin_layout Itemize
MLE 
\begin_inset Formula $\hat{p}=X/n$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Geometric
\begin_inset Formula $\left(p\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
pmf: 
\begin_inset Formula $P\left(X=x|p\right)=\left(1-p\right)^{x-1}p$
\end_inset


\end_layout

\begin_layout Itemize
mean and variance: 
\begin_inset Formula $EX=\frac{1}{p}$
\end_inset

 and 
\begin_inset Formula $\text{Var}\left(X\right)=\frac{\left(1-p\right)}{p^{2}}$
\end_inset


\end_layout

\begin_layout Itemize
MLE 
\begin_inset Formula $\hat{p}=1/\bar{X}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Poisson
\begin_inset Formula $\left(\lambda\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
pmf: 
\begin_inset Formula $P\left(X=x|p\right)=\frac{e^{-\lambda}\lambda^{x}}{x!}$
\end_inset


\end_layout

\begin_layout Itemize
mean and variance: 
\begin_inset Formula $EX=\lambda$
\end_inset

 and 
\begin_inset Formula $\text{Var}\left(X\right)=\lambda$
\end_inset


\end_layout

\begin_layout Itemize
MLE 
\begin_inset Formula $\hat{\lambda}=\bar{X}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Beta
\begin_inset Formula $\left(\alpha,\beta\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
pdf: 
\begin_inset Formula $f_{X}\left(x|\alpha,\beta\right)=\frac{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}{\Gamma\left(\alpha+\beta\right)}x^{\alpha-1}\left(1-x\right)^{\beta-1}$
\end_inset


\end_layout

\begin_layout Itemize
mean and variance: 
\begin_inset Formula $EX=\frac{\alpha}{\alpha+\beta}$
\end_inset

 and 
\begin_inset Formula $\text{Var}\left(X\right)=\frac{\alpha\beta}{\left(\alpha+\beta\right)\left(\alpha+\beta\right)\left(\alpha+\beta+1\right)}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Gamma
\begin_inset Formula $\left(\alpha,\beta\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
pdf: 
\begin_inset Formula $f_{X}\left(x|\alpha,\beta\right)=\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}x^{\alpha-1}e^{-x/\beta}$
\end_inset


\end_layout

\begin_layout Itemize
mean and variance: 
\begin_inset Formula $EX=\alpha\beta$
\end_inset

 and 
\begin_inset Formula $\text{Var}\left(X\right)=\alpha\beta^{2}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Exponential
\begin_inset Formula $\left(\beta\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
pdf: 
\begin_inset Formula $f_{X}\left(x|\beta\right)=\frac{1}{\beta}e^{-x/\beta}$
\end_inset


\end_layout

\begin_layout Itemize
mean and variance: 
\begin_inset Formula $EX=\beta$
\end_inset

 and 
\begin_inset Formula $\text{Var}\left(X\right)=\beta^{2}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[7.24]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}\sim\text{Poisson}\left(\Lambda\right)$
\end_inset

 and let 
\begin_inset Formula $\Lambda\sim\text{Gamma}\left(\alpha,\beta\right)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
The posterior distribution is 
\begin_inset Formula 
\begin{align*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{f\left(\mathbf{x}|\theta\right)\pi\left(\lambda\right)}{m\left(\mathbf{x}\right)}\\
 & =\frac{\frac{1}{\prod_{i=1}^{n}x_{i}!}e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}}{\int_{0}^{\infty}\left(\frac{1}{\prod_{i=1}^{n}x_{i}!}e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}\right)d\lambda}\\
 & =\frac{\frac{1}{\prod_{i=1}^{n}x_{i}!}e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}}{\int_{0}^{\infty}\left(\frac{1}{\prod_{i=1}^{n}x_{i}!}e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}\right)d\lambda}\\
 & =ce^{-\lambda/\left(n+\frac{1}{\beta}\right)^{-1}}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\\
 & =\frac{1}{\Gamma\left(\sum_{i=1}^{n}x_{i}+\alpha\right)\left(n+\frac{1}{\beta}\right)^{-\left(\sum_{i=1}^{n}x_{i}+\alpha\right)}}e^{-\lambda/\left(n+\frac{1}{\beta}\right)^{-1}}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}
\end{align*}

\end_inset

Define 
\begin_inset Formula $y=\sum_{i=1}^{n}y_{i}$
\end_inset

 so
\begin_inset Formula 
\begin{align*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{1}{\Gamma\left(\sum_{i=1}^{n}x_{i}+\alpha\right)\left(n+\frac{1}{\beta}\right)^{-\left(\sum_{i=1}^{n}x_{i}+\alpha\right)}}e^{-\lambda/\left(n+\frac{1}{\beta}\right)^{-1}}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\\
 & =\frac{1}{\Gamma\left(y+\alpha\right)\left(1\Bigg/\left(n+\frac{1}{\beta}\right)\right)^{\left(y+\alpha\right)}}e^{-\lambda/\left(n+\frac{1}{\beta}\right)^{-1}}\lambda^{y+\alpha-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Since the posterior distribution is 
\begin_inset Formula $\text{Gamma}\left(y+\alpha,\frac{\beta}{\beta n+1}\right)$
\end_inset

 the posterior mean is 
\begin_inset Formula $\left(y+\alpha\right)\left(\frac{\beta}{\beta n+1}\right)$
\end_inset

 and the posterior variance is 
\begin_inset Formula $\left(y+\alpha\right)\left(\frac{\beta}{\beta n+1}\right)^{2}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[7.60]
\end_layout

\end_inset

 Let 
\begin_inset Formula $X_{i}\sim\text{Gamma}\left(\alpha,\beta\right)$
\end_inset

 with 
\begin_inset Formula $\alpha$
\end_inset

 known.
\end_layout

\begin_deeper
\begin_layout Enumerate
A complete sufficient statistic.
 By
\begin_inset Formula 
\begin{align*}
f\left(\mathbf{x}|\theta\right) & =\frac{1}{\left(\Gamma\left(\alpha\right)\beta^{\alpha}\right)^{n}}\left(\prod_{i=1}^{n}x_{i}\right)^{\alpha-1}e^{-\frac{1}{\beta}\sum_{i=1}^{n}x_{i}}\\
 & =\frac{1}{\left(\Gamma\left(\alpha\right)\beta^{\alpha}\right)^{n}}e^{\left(\alpha-1\right)\sum_{i=1}^{n}\log x_{i}}e^{-\frac{1}{\beta}\sum_{i=1}^{n}x_{i}}
\end{align*}

\end_inset

and
\begin_inset Formula 
\begin{align*}
f\left(x|\theta\right) & =\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\left(x\right)^{\alpha-1}e^{-\frac{1}{\beta}x}\\
 & =\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}e^{\left(\alpha-1\right)\log x}e^{-\frac{1}{\beta}x}
\end{align*}

\end_inset

According to Theorem blah blah blah with 
\begin_inset Formula $h\left(\mathbf{x}\right)=1,c\left(\alpha,\beta\right)=\Gamma\left(\alpha\right)\beta^{\alpha},w_{1}=\left(\alpha-1\right),t_{1}=\log x,w_{2}=-\frac{1}{\beta},t_{2}=x$
\end_inset

 the complete sufficient statistic for 
\begin_inset Formula $\left(\alpha,\beta\right)$
\end_inset

 is 
\begin_inset Formula $\left(\sum_{i=1}^{n}\log X_{i},\sum_{i=1}^{n}X_{i}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
I need to construct an unbiased estimator of 
\begin_inset Formula $1/\beta$
\end_inset

.
 The MOM estimators for 
\begin_inset Formula $\left(\alpha,\beta\right)$
\end_inset


\begin_inset Formula 
\begin{align*}
\bar{X} & =E\left(X\right)=\hat{\alpha}\hat{\beta}\\
\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2} & =E\left(X^{2}\right)=\hat{\alpha}\hat{\beta}^{2}+\left(\hat{\alpha}\hat{\beta}\right)^{2}\\
 & =\left(\frac{1}{\hat{\alpha}}+1\right)\left(\bar{X}\right)^{2}
\end{align*}

\end_inset

So 
\begin_inset Formula 
\begin{align*}
\hat{\alpha} & =\frac{1}{\frac{1}{\bar{X}n}\sum_{i=1}^{n}X_{i}^{2}-1}\\
 & =\frac{n\bar{X}}{\sum_{i=1}^{n}\left(X_{i}^{2}-X_{i}\right)}\\
\hat{\beta} & =\frac{\sum_{i=1}^{n}\left(X_{i}^{2}-X_{i}\right)}{n}
\end{align*}

\end_inset

God damn it.
 The MOM isn't invariant.
 Calculating the MLE by hand for 
\begin_inset Formula $\beta$
\end_inset

.
 Actually an even bigger idiot.
 With 
\begin_inset Formula $\alpha$
\end_inset

 known 
\begin_inset Formula $\bar{X}/\alpha$
\end_inset

 is unbiased for 
\begin_inset Formula $\beta$
\end_inset

.
 
\begin_inset Formula $X_{i}\sim\Gamma\left(\alpha,\beta\right)$
\end_inset

 then 
\begin_inset Formula $\sum_{i=1}^{n}X_{i}\sim\Gamma\left(n\alpha,\beta\right)$
\end_inset

 and 
\begin_inset Formula 
\begin{align*}
\frac{1}{\sum_{i=1}^{n}X_{i}} & \sim\text{Inv}\Gamma\left(n\alpha,\frac{1}{\beta}\right)
\end{align*}

\end_inset

and 
\begin_inset Formula 
\[
E\left(\frac{1}{\sum_{i=1}^{n}X_{i}}\right)=\frac{1}{\beta}\frac{1}{n\alpha-1}
\]

\end_inset

so with 
\begin_inset Formula $W\left(\mathbf{X}\right)=\frac{n\alpha-1}{\sum_{i=1}^{n}X_{i}}$
\end_inset

 
\begin_inset Formula 
\[
E\left(W\left(\mathbf{X}\right)\right)=\left(n\alpha-1\right)E\left(\frac{1}{\sum_{i=1}^{n}X_{i}}\right)=\frac{1}{\beta}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Finally conditioning 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 on 
\begin_inset Formula $\bar{X}$
\end_inset

 is just 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 since 
\begin_inset Formula $\frac{n\alpha-1}{\sum_{i=1}^{n}X_{i}}=\frac{\alpha-1/n}{\frac{1}{n}\sum_{i=1}^{n}X_{i}}=\frac{\alpha-1/n}{\bar{X}}$
\end_inset

 and conditioning on a function of 
\begin_inset Formula $\bar{X}$
\end_inset

 is just the function again.
\end_layout

\begin_layout Enumerate
Interestingly enough it doesn't attain the Cramer-Rao lower bound.
 So first of all 
\begin_inset Formula 
\begin{align*}
\left(\frac{d}{d\beta}E_{\beta}\left(W\left(\mathbf{X}\right)\right)\right)^{2} & =\left(\frac{d}{d\beta}\frac{1}{\beta}\right)^{2}\\
 & =\frac{1}{\beta^{4}}
\end{align*}

\end_inset

and
\begin_inset Formula 
\begin{align*}
nE_{\theta}\left(\left(\frac{\partial}{\partial\theta}\log f\left(X|\theta\right)\right)^{2}\right) & =nE_{\beta}\left(\left[\frac{\partial}{\partial\beta}\left(-\log\Gamma\left(\alpha\right)-\alpha\log\left(\beta\right)+\left(\alpha-1\right)\log X-\frac{1}{\beta}X\right)\right]^{2}\right)\\
 & =nE_{\beta}\left(\left[-\frac{\alpha}{\beta}+\frac{X}{\beta^{2}}\right]^{2}\right)\\
 & =nE_{\beta}\left(\frac{\alpha^{2}}{\beta^{2}}+\frac{X^{2}}{\beta^{4}}-\frac{2\alpha}{\beta^{3}}X\right)\\
 & =n\left(\frac{\alpha^{2}}{\beta^{2}}+\frac{1}{\beta^{4}}\left(\alpha\beta^{2}+\left(\alpha\beta\right)^{2}\right)-\frac{2\alpha}{\beta^{3}}\left(\alpha\beta\right)\right)\\
 & =\frac{n\alpha}{\beta^{2}}
\end{align*}

\end_inset

and so the lower bound is
\begin_inset Formula 
\[
\frac{1/\beta^{4}}{n\alpha/\beta^{2}}=\frac{1}{\beta^{2}n\alpha}
\]

\end_inset

Then 
\begin_inset Formula $\frac{n\alpha-1}{\sum_{i=1}^{n}X_{i}}\sim\text{Inv}\Gamma\left(n\alpha,\frac{n\alpha-1}{\beta}\right)$
\end_inset

 implies 
\begin_inset Formula 
\begin{align*}
\text{Var}\left(W\left(\mathbf{X}\right)\right) & =\frac{\left(\frac{n\alpha-1}{\beta}\right)^{2}}{\left(n\alpha-1\right)^{2}\left(n\alpha-2\right)}\\
 & =\frac{1}{\left(n\alpha-2\right)}\frac{1}{\beta^{2}}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[7.44]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}\sim\text{n}\left(\theta,1\right)$
\end_inset

.
 Find the best unbiased estimator of 
\begin_inset Formula $\theta^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
If 
\begin_inset Formula $\bar{X}^{2}-1/n$
\end_inset

 is unbiased for 
\begin_inset Formula $\theta^{2}$
\end_inset

 then it's UMVUE by Lehmann-Scheffe.
 First of all 
\begin_inset Formula $\bar{X}\sim\text{n}\left(\theta,\frac{1}{n}\right)$
\end_inset

 so 
\begin_inset Formula 
\[
E\left(\bar{X}^{2}\right)=\text{Var}\left(\bar{X}\right)+\left(E\bar{X}\right)^{2}=\frac{1}{n}+\theta^{2}
\]

\end_inset

and hence
\begin_inset Formula 
\[
E\left(\bar{X}^{2}-\frac{1}{n}\right)=\frac{1}{n}+\theta^{2}-\frac{1}{n}=\theta^{2}
\]

\end_inset

so since 
\begin_inset Formula $\bar{X}$
\end_inset

 is complete sufficient for 
\begin_inset Formula $\theta$
\end_inset

 for this model it's the case that 
\begin_inset Formula $\bar{X}^{2}-1/n$
\end_inset

 is UMVUE for 
\begin_inset Formula $\theta^{2}$
\end_inset

.
 To calculate its variance we need to compute 
\begin_inset Formula 
\begin{align*}
\text{Var}\left(\bar{X}^{2}-\frac{1}{n}\right) & =\text{Var}\left(\bar{X}^{2}\right)=E\left(\bar{X}^{4}\right)-\left(E\bar{X}^{2}\right)^{4}
\end{align*}

\end_inset

Stein's lemma is for 
\begin_inset Formula $X\sim\text{n}\left(\theta,\sigma^{2}\right)$
\end_inset

 it's the case that 
\begin_inset Formula $E\left[g\left(X\right)\left(X-\theta\right)\right]=\sigma^{2}Eg'\left(X\right)$
\end_inset

.
 So then
\begin_inset Formula 
\begin{align*}
E\left(\bar{X}^{4}\right) & =E\left[\bar{X}^{3}\left(\bar{X}-\theta\right)\right]+E\left(\bar{X}^{3}\theta\right)\\
E\left[\bar{X}^{3}\left(\bar{X}-\theta\right)\right] & =3\left(\frac{1}{n}\right)E\bar{X}^{2}=\frac{3}{n}\left(\theta^{2}+\frac{1}{n}\right)=\frac{3\theta^{2}}{n}+\frac{3}{n^{2}}\\
\theta E\bar{X}^{3} & =\theta\left(E\bar{X}^{2}\left(\bar{X}-\theta\right)+\theta E\bar{X}^{2}\right)=\theta\left(\frac{2}{n}E\bar{X}+\theta\left(\theta^{2}+\frac{1}{n}\right)\right)\\
 & =\theta\left(\frac{2}{n}\theta+\theta\left(\theta^{2}+\frac{1}{n}\right)\right)=\frac{3}{n}\theta^{2}+\theta^{4}\\
E\left(\bar{X}^{4}\right) & =\frac{3\theta^{2}}{n}+\frac{3}{n^{2}}+\frac{3}{n}\theta^{2}+\theta^{4}\\
 & =\frac{3}{n^{2}}+6\left(\frac{1}{n}\right)\theta^{2}+\theta^{4}
\end{align*}

\end_inset

So 
\begin_inset Formula 
\begin{align*}
\text{Var}\left(\bar{X}^{2}\right) & =\frac{3}{n^{2}}+6\left(\frac{1}{n}\right)\theta^{2}+\theta^{4}-\left(E\bar{X}^{2}\right)^{2}\\
 & =\frac{3}{n^{2}}+6\left(\frac{1}{n}\right)\theta^{2}+\theta^{4}-\left(\theta^{2}+\frac{1}{n}\right)^{2}\\
 & =\frac{3}{n^{2}}+6\left(\frac{1}{n}\right)\theta^{2}+\theta^{4}-\left(\theta^{4}+\frac{1}{n^{2}}+2\frac{\theta^{2}}{n}\right)\\
 & =\frac{2}{n^{2}}+4\left(\frac{1}{n}\right)\theta^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
The Cramer-Rao lower bound
\begin_inset Formula 
\begin{align*}
\text{Var}\left(W\left(\mathbf{X}\right)\right) & \geq\frac{\left(\frac{\partial}{\partial\theta}E_{\theta}W\left(\mathbf{X}\right)\right)^{2}}{E\left[\left(\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{X}\right)\right)^{2}\right]}\\
 & =\frac{\left(\frac{\partial}{\partial\theta}\theta^{2}\right)^{2}}{nE\left[\left(\frac{\partial}{\partial\theta}\left(-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2}\left(X-\theta\right)^{2}\right)\right)^{2}\right]}\\
 & =\frac{4\theta^{2}}{nE\left[\left(-\left(X-\theta\right)\right)^{2}\right]}\\
 & =\frac{4\theta^{2}}{nE\left[\left(X-\theta\right)^{2}\right]}\\
 & =\frac{4\theta^{2}}{n}
\end{align*}

\end_inset

and so 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\frac{2}{n^{2}}+4\left(\frac{1}{n}\right)\theta^{2}>\frac{4\theta^{2}}{n}$
\end_inset

.
 An alternative way to compute the Cramer-Rao lower bound is using the identity
 
\begin_inset Formula 
\[
E\left[\left(\frac{\partial}{\partial\theta}\log L\left(\theta|X\right)\right)^{2}\right]=-E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log L\left(\theta|X\right)\right]
\]

\end_inset

 So
\begin_inset Formula 
\begin{align*}
-E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log L\left(\theta|X\right)\right] & =-E\left[\frac{\partial^{2}}{\partial\theta^{2}}\left(-\frac{1}{2}\log\left(2\pi\right)-\frac{1}{2}\left(X-\theta\right)^{2}\right)\right]\\
 & =-E\left[\frac{\partial}{\partial\theta}\left(X-\theta\right)\right]\\
 & =1
\end{align*}

\end_inset

and hence 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula 
\begin{align*}
\text{Var}\left(W\left(\mathbf{X}\right)\right) & \geq\frac{\left(\frac{\partial}{\partial\theta}E_{\theta}W\left(\mathbf{X}\right)\right)^{2}}{-nE\left[\frac{\partial^{2}}{\partial\theta^{2}}\log L\left(\theta|X\right)\right]}\\
 & =\frac{4\theta^{2}}{n}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[8.5]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}$
\end_inset

 be such that 
\begin_inset Formula 
\[
f\left(x|\theta,\nu\right)=\frac{\theta\nu^{\theta}}{x^{\theta+1}}I\left(x\right)_{\left[\nu,\infty\right)}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
The likelihood is 
\begin_inset Formula 
\[
L\left(\theta,\nu|\mathbf{x}\right)=\frac{\theta^{n}\nu^{n\theta}}{\left(\prod_{i=1}^{n}x_{i}\right)^{\theta+1}}I\left(x_{\left(1\right)}\right)_{\left[\nu,\infty\right)}
\]

\end_inset

Then the log-likelihood is 
\begin_inset Formula 
\[
\log L\left(\theta,\nu|\mathbf{x}\right)=n\log\theta+n\theta\log\nu-\left(\theta+1\right)\sum_{i=1}^{n}\log x_{i}
\]

\end_inset

This is an increasing function of 
\begin_inset Formula $\nu<x_{\left(1\right)}$
\end_inset

.
 So 
\begin_inset Formula $\hat{\nu}=x_{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula 
\begin{align*}
\partial_{\theta}\log L\left(\theta,\nu|\mathbf{x}\right) & =\frac{n}{\theta}+n\log x_{\left(1\right)}-\sum_{i=1}^{n}\log x_{i}\\
0 & =\frac{n}{\theta}+n\log x_{\left(1\right)}-\sum_{i=1}^{n}\log x_{i}\\
\sum_{i=1}^{n}\log x_{i}-n\log x_{\left(1\right)} & =\frac{n}{\theta}\\
\hat{\theta} & =\frac{n}{\sum_{i=1}^{n}\log x_{i}-n\log x_{\left(1\right)}}\\
\hat{\theta} & =\frac{n}{\log\left(\frac{\prod_{i=1}^{n}x_{i}}{x_{\left(1\right)^{n}}}\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $\Theta_{0}=\left\{ 1\right\} $
\end_inset

 and 
\begin_inset Formula $\Theta_{0}^{c}=\mathbb{R}-\left\{ 1\right\} $
\end_inset

.
 Then
\begin_inset Formula 
\[
H_{0}\,:\,\theta=1,\nu\text{ uknown}\: H_{1}\,:\,\theta\neq1,\nu\text{ uknown}
\]

\end_inset

Then the likelihood ratio test statistic is 
\begin_inset Formula 
\begin{align*}
\lambda\left(\mathbf{x}\right) & =\frac{L\left(1,\nu|\mathbf{x}\right)}{L\left(\hat{\theta},\nu|\mathbf{x}\right)}\\
 & =\frac{\frac{x_{\left(1\right)}^{n}}{\left(\prod_{i=1}^{n}x_{i}\right)^{2}}}{\frac{\left(n/T\right)^{n}x_{\left(1\right)}^{n^{2}/T}}{\left(\prod_{i=1}^{n}x_{i}\right)^{\left(n/T\right)+1}}}\\
 & =\left(x_{\left(1\right)}^{n}e^{T}\right)^{\left(n/T\right)-1}\frac{x_{\left(1\right)}^{n}}{\left(n/T\right)^{n}x_{\left(1\right)}^{n^{2}/T}}\\
 & =e^{n-T}\left(\frac{T}{n}\right)^{n}
\end{align*}

\end_inset

Then 
\begin_inset Formula $e^{n-T}\left(\frac{T}{n}\right)^{n}$
\end_inset

 is an increasing function 
\begin_inset Formula $T$
\end_inset

 for 
\begin_inset Formula $T\leq n$
\end_inset

 since 
\begin_inset Formula 
\begin{align*}
\partial_{T}\log\lambda\left(\mathbf{x}\right) & =\partial_{T}\left(\left(n-T\right)+n\log\left(\frac{T}{n}\right)\right)\\
 & =-1+\frac{n}{T/n}\frac{1}{n}\\
 & =\frac{n}{T}-1
\end{align*}

\end_inset

 and decreasing for 
\begin_inset Formula $T>n$
\end_inset

.
 Therefore 
\begin_inset Formula $T\leq c$
\end_inset

 is equivalent to 
\begin_inset Formula $T\leq c_{1}$
\end_inset

 and 
\begin_inset Formula $T\ge c_{2}$
\end_inset

.
\end_layout

\begin_layout Enumerate
A size 
\begin_inset Formula $\alpha$
\end_inset

 test means 
\begin_inset Formula 
\[
\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)=P\left(T\leq c_{1}\vee\, T\geq c_{1}\right)
\]

\end_inset

Firstly let's construct the transformation 
\begin_inset Formula 
\begin{align*}
Y_{1} & =X_{i\neq\left(1\right)}\\
Y_{2} & =X_{j\neq i\neq\left(1\right)}\\
Y_{3} & =X_{\left(1\right)}
\end{align*}

\end_inset

Cases: if 
\begin_inset Formula $X_{\left(1\right)}=X_{1}=Y_{1}$
\end_inset

 then Then 
\begin_inset Formula $\left(X_{1},\dots,X_{n}|X_{\left(1\right)}\right)=\left(X_{\left(1\right)},X_{\left(2\right)},\dots,X_{\left(n\right)}|X_{\left(1\right)}\right)$
\end_inset

.
 So then 
\begin_inset Formula $\left(X_{\left(2\right)},\dots,X_{\left(n\right)}|X_{\left(1\right)}\right)$
\end_inset

 are still iid and so are 
\begin_inset Formula $\left(\frac{X_{\left(2\right)}}{X_{\left(1\right)},}\dots,\frac{X_{\left(n\right)}}{X_{\left(1\right)}}|X_{\left(1\right)}\right)$
\end_inset

.
 The joint distribution of 
\begin_inset Formula $\left(X_{1},\dots,X_{n}\right)$
\end_inset

 is 
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}\right)=\frac{\theta^{n}\nu^{n\theta}}{\left(\prod_{i=1}^{n}x_{i}\right)^{\theta+1}}I\left(x_{\left(1\right)}\right)_{\left[\nu,\infty\right)}
\]

\end_inset

Because 
\begin_inset Formula $\nu>x_{\left(1\right)}$
\end_inset

 and iid of 
\begin_inset Formula $X_{i}$
\end_inset

 the marginal of 
\begin_inset Formula $X_{\left(1\right)}$
\end_inset

 
\begin_inset Formula 
\begin{align*}
f_{X_{\left(1\right)}}\left(x_{\left(1\right)}\right) & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\underbrace{\int_{x_{\left(1\right)}}^{\infty}\cdots\int_{x_{\left(1\right)}}^{\infty}}_{n-1}\prod_{i=2}^{n}f_{X_{\left(i\right)}}\left(x_{\left(i\right)}\right)d_{x_{\left(i\right)}}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\left(P\left(X_{i}>x_{\left(1\right)}\right)\right)^{n-1}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\left(1-P\left(X_{i}\leq x_{\left(1\right)}\right)\right)^{n-1}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\left[\left(\frac{\nu}{x_{\left(1\right)}}\right)^{\theta}\right]^{n-1}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\left(\frac{\nu}{x_{\left(1\right)}}\right)^{n\theta-\theta}\\
 & =\frac{\theta\nu^{n\theta}}{\left(x_{\left(1\right)}\right)^{n\theta+1}}
\end{align*}

\end_inset

and so 
\begin_inset Formula 
\begin{align*}
f_{\left(X_{\left(2\right)},\dots,X_{\left(n\right)}\bigg|X_{\left(1\right)}\right)}\left(x_{\left(2\right)},\dots,x_{\left(n\right)}\big|x_{\left(1\right)}\right) & =\frac{f_{\mathbf{X}}\left(\mathbf{x}\right)}{f_{X_{\left(1\right)}}\left(x_{\left(1\right)}\right)}\\
 & =\frac{\frac{\theta^{n}\nu^{n\theta}}{\left(\prod_{i=1}^{n}x_{i}\right)^{\theta+1}}}{\frac{\theta\nu^{n\theta}}{\left(x_{\left(1\right)}\right)^{n\theta+1}}}\\
 & \frac{\theta^{n-1}\left(x_{\left(1\right)}\right)^{\theta\left(n-1\right)}}{\left(\prod_{i=2}^{n}x_{\left(i\right)}\right)^{\theta+1}}\\
 & =\prod_{i=2}^{n}\frac{\theta\left(x_{\left(1\right)}\right)^{\theta}}{\left(x_{\left(i\right)}\right)^{\theta+1}}
\end{align*}

\end_inset

Therefore for 
\begin_inset Formula $j\neq1$
\end_inset

 it's the case that 
\begin_inset Formula $X_{\left(j\right)}|X_{\left(1\right)}\sim\text{Pareto}\left(\theta,X_{\left(1\right)}\right)$
\end_inset

.
 Note that 
\begin_inset Formula $cX\sim\text{Pareto}\left(\theta,c\nu\right)$
\end_inset

 and so
\begin_inset Formula 
\[
X_{\left(j\right)}/X_{\left(1\right)}|X_{\left(1\right)}\sim\text{Pareto}\left(\theta,1\right)
\]

\end_inset

and hence for 
\begin_inset Formula $Y_{j}=\frac{X_{\left(j\right)}}{X_{\left(1\right)}}$
\end_inset

 with 
\begin_inset Formula $j\neq1$
\end_inset

 
\begin_inset Formula 
\[
f_{Y_{2},\dots,Y_{n}}\left(y_{2},\dots,y_{n}\right)=\frac{\theta^{n}}{\left(\prod_{j=2}^{n}y_{j}\right)^{\theta+1}}
\]

\end_inset

 is The joint distribution of 
\begin_inset Formula $\left(X_{\left(1\right)},X_{\left(j\right)}\right)$
\end_inset

 
\begin_inset Formula 
\begin{align*}
f_{X_{\left(1\right)},X_{\left(j\right)}}\left(x_{\left(1\right)},x_{\left(j\right)}\right) & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\underbrace{\int_{x_{\left(1\right)}}^{\infty}\cdots\int_{x_{\left(1\right)}}^{\infty}}_{n-2}\prod_{i\neq1,j}^{n}f_{X_{\left(i\right)}}\left(x_{\left(i\right)}\right)d_{x_{\left(i\right)}}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\frac{\theta\nu^{\theta}}{\left(x_{\left(j\right)}\right)^{\theta+1}}\left(P\left(X_{i}>x_{\left(1\right)}\right)\right)^{n-2}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\frac{\theta\nu^{\theta}}{\left(x_{\left(j\right)}\right)^{\theta+1}}\left(1-P\left(X_{i}\leq x_{\left(1\right)}\right)\right)^{n-2}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\frac{\theta\nu^{\theta}}{\left(x_{\left(j\right)}\right)^{\theta+1}}\left[\left(\frac{\nu}{x_{\left(1\right)}}\right)^{\theta}\right]^{\left(n-1\right)-1}\\
 & =\frac{\theta\nu^{\theta}}{\left(x_{\left(1\right)}\right)^{\theta+1}}\frac{\theta\nu^{\theta}}{\left(x_{\left(j\right)}\right)^{\theta+1}}\left(\frac{\nu}{x_{\left(1\right)}}\right)^{\left(n-1\right)\theta-\theta}\\
 & =\frac{\theta^{2}\nu^{n\theta}}{\left(x_{\left(1\right)}\right)^{\left(n-1\right)\theta+1}\left(x_{\left(j\right)}\right)^{\theta+1}}
\end{align*}

\end_inset

Then 
\begin_inset Formula 
\begin{align*}
P\left(\frac{X_{\left(j\right)}}{X_{\left(1\right)}}\leq y\right) & =P\left(X_{\left(j\right)}\leq yX_{\left(1\right)}\right)\\
 & =\int_{\nu}^{\infty}\int_{x_{\left(1\right)}}^{yx_{\left(1\right)}}\frac{\theta^{2}\nu^{n\theta}}{\left(x_{\left(1\right)}\right)^{\left(n-1\right)\theta+1}\left(x_{\left(j\right)}\right)^{\theta+1}}dx_{\left(j\right)}dx_{\left(1\right)}\\
 & =\frac{1-y^{-\theta}}{n}
\end{align*}

\end_inset

and therefore for 
\begin_inset Formula $j\neq1$
\end_inset

 it's the case that 
\begin_inset Formula $Y_{j}=\frac{X_{\left(j\right)}}{X_{\left(1\right)}}$
\end_inset


\begin_inset Formula 
\[
f_{Y_{j}}\left(y_{j}\right)=\frac{\theta}{n\left(y_{j}\right)^{\theta+1}}
\]

\end_inset

Then the joint distribution of 
\begin_inset Formula $\left(Y_{2},\dots,Y_{n}\right)$
\end_inset


\begin_inset Formula 
\[
f_{\mathbf{Y}}\left(\mathbf{y}\right)=\frac{\theta^{n}}{n^{n}\left(\prod_{j\neq1}^{n}y_{j}\right)^{\theta+1}}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[8.13]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}\sim\text{uniform}\left(\theta,\theta+1\right)$
\end_inset

.
 Therefore 
\begin_inset Formula 
\[
f_{X}\left(x\right)=I\left(x\right)_{\left[\theta,\theta+1\right]}
\]

\end_inset

 let 
\begin_inset Formula $\Theta_{0}=0$
\end_inset

 and 
\begin_inset Formula $\Theta_{0}^{c}=\left(0,1\right]$
\end_inset

.
 Therefore the tests
\begin_inset Formula 
\[
H_{0}:\theta=0\quad H_{1}:\theta>0
\]

\end_inset

The two tests are 
\begin_inset Formula 
\begin{align*}
\phi_{1}\left(X_{1}\right) & :\text{ Reject }H_{0}\text{ if }X_{1}>.95\\
\phi_{2}\left(X_{1},X_{2}\right) & :\text{ Reject }H_{0}\text{ if }X_{1}+X_{2}>C
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
The size of 
\begin_inset Formula $\phi_{1}$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
\sup_{\theta\in\Theta_{0}}\beta_{\phi_{1}}\left(\theta\right) & =P\left(X_{1}\in R\right)\\
 & =P\left(X_{1}>.95\right)\\
 & =.05
\end{align*}

\end_inset

The size of 
\begin_inset Formula $\phi_{2}$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
\sup_{\theta\in\Theta_{0}}\beta_{\phi_{2}}\left(\theta\right) & =P\left(X_{1}+X_{2}>c\right)\\
 & =1-P\left(X_{1}+X_{2}<c\right)\\
 & =1-\frac{c^{2}}{2}
\end{align*}

\end_inset

Hence 
\begin_inset Formula $c=\sqrt{1.9}=1.378$
\end_inset

.
\end_layout

\begin_layout Enumerate
The power functions are 
\begin_inset Formula 
\begin{align*}
\beta_{\phi_{1}}\left(\theta\right) & =P\left(X_{1}>.95\right)\\
 & =1-P\left(X_{1}<.95\right)\\
 & =\begin{cases}
0 & \text{if }.95<\theta\\
1-\frac{.95-\theta}{\theta+1-\theta} & \text{if }\theta<.95<\theta+1\\
1 & \text{if }\theta+1<.95
\end{cases}\\
 & =\begin{cases}
0 & \text{if }.95<\theta\\
.05+\theta & \text{if }\theta<.95<\theta+1\\
1 & \text{if }\theta+1<.95
\end{cases}
\end{align*}

\end_inset

and
\begin_inset Formula 
\begin{align*}
\beta_{\phi_{2}}\left(\theta\right) & =P\left(X_{1}+X_{2}>c\right)\\
 & =1-P\left(X_{1}+X_{2}<c\right)\\
 & =\begin{cases}
0 & \text{if }c<\theta\\
1-\frac{1}{2}\left(c-\theta\right)\left(c-\theta\right) & \text{if }\theta<c<\theta+1\\
1 & \text{if }\theta+1<c
\end{cases}\\
 & =\begin{cases}
0 & \text{if }c<\theta\\
1-\frac{1}{2}\left(\theta^{2}+c^{2}-2\theta c\right) & \text{if }\theta<c<\theta+1\\
1 & \text{if }\theta+1<cx`
\end{cases}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[6.25]
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}$
\end_inset

 be distributed such that
\begin_inset Formula 
\begin{align*}
f_{X}\left(x|\theta\right) & =\frac{\theta}{\left(1+x\right)^{\theta+1}}I\left(x\right)_{\left(0,\infty\right)}\\
 & =\theta\left(1+x\right)^{-\left(\theta+1\right)}\\
 & =\theta e^{-\left(\theta+1\right)\log\left(1+x\right)}
\end{align*}

\end_inset

Hence with 
\begin_inset Formula $h\left(x\right)=1,c\left(\theta\right)=\theta,w\left(\theta\right)=-\left(\theta+1\right),t\left(x\right)=\log\left(1+x\right)$
\end_inset

, and since 
\begin_inset Formula $w\left(\theta\right)=-\left(\theta+1\right)$
\end_inset

 is obviously invertible, the complete sufficient statistic is 
\begin_inset Formula $\sum_{i=1}^{n}\log\left(1+x_{i}\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(c)]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}$
\end_inset

 be distributed such that
\begin_inset Formula 
\begin{align*}
f_{X}\left(x|\theta\right) & =\frac{\theta^{x}\log\left(\theta\right)}{\theta-1}I\left(x\right)_{\left(0,1\right)}\\
 & =\frac{\log\left(\theta\right)}{\theta-1}e^{x\log\theta}
\end{align*}

\end_inset

Hence with 
\begin_inset Formula $h\left(x\right)=1,c\left(\theta\right)=\frac{\log\theta}{\theta-1},w\left(\theta\right)=\log\theta,t\left(x\right)=x$
\end_inset

, and since 
\begin_inset Formula $w\left(\theta\right)=\log\theta$
\end_inset

 is obviously invertible, the complete sufficient statistic is 
\begin_inset Formula $\sum_{i=1}^{n}x_{i}$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(e)]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}$
\end_inset

 be distributed such that
\begin_inset Formula 
\begin{align*}
f_{X}\left(x|\theta\right) & =\binom{2}{x}\theta^{x}\left(1-\theta\right)^{2-x}I\left(x\right)_{\left\{ 0,1,2\right\} }\\
 & =\binom{2}{x}\left(1-\theta\right)^{2}\left(\frac{\theta}{1-\theta}\right)^{x}\\
 & =\binom{2}{x}\left(1-\theta\right)^{2}e^{x\log\left(\frac{\theta}{1-\theta}\right)}
\end{align*}

\end_inset

Hence with 
\begin_inset Formula $h\left(x\right)=\binom{2}{x},c\left(\theta\right)=\left(1-\theta\right)^{2},w\left(\theta\right)=\log\left(\frac{\theta}{1-\theta}\right),t\left(x\right)=x$
\end_inset

, and since 
\begin_inset Formula $w\left(\theta\right)=\log\left(\frac{\theta}{1-\theta}\right)$
\end_inset

 is obviously invertible on 
\begin_inset Formula $\left(0,1\right)$
\end_inset

, the complete sufficient statistic is 
\begin_inset Formula $\sum_{i=1}^{n}x_{i}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[7.58]
\end_layout

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 be such that 
\begin_inset Formula 
\[
f_{X}\left(x|\theta\right)=\left(\frac{\theta}{2}\right)^{\left|x\right|}\left(1-\theta\right)^{1-\left|x\right|}I\left(x\right)_{\left\{ -1,0,1\right\} }
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
The MLE 
\begin_inset Formula 
\begin{align*}
L\left(\theta|-1\right) & =\left(\frac{\theta}{2}\right)\\
\hat{\theta} & =1\\
L\left(\theta|0\right) & =\left(1-\theta\right)\\
\hat{\theta} & =0\\
L\left(\theta|1\right) & =\left(\frac{\theta}{2}\right)\\
\hat{\theta} & =1
\end{align*}

\end_inset

Hence 
\begin_inset Formula $\hat{\theta}=\left|x\right|$
\end_inset

.
 The expected value is 
\begin_inset Formula 
\begin{align*}
E_{\theta}\hat{\theta} & =\left|1\right|\left(\frac{\theta}{2}\right)+\left|0\right|\left(1-\theta\right)+\left|-1\right|\left(\frac{\theta}{2}\right)\\
 & =\theta
\end{align*}

\end_inset

So 
\begin_inset Formula $\hat{\theta}=\left|x\right|$
\end_inset

 is unbiased for 
\begin_inset Formula $\theta$
\end_inset

.
 The second moment
\begin_inset Formula 
\[
E_{\theta}\hat{\theta}^{2}=\theta
\]

\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula 
\[
T\left(X\right)=\begin{cases}
2 & \text{if }x=1\\
0 & \text{othetwise}
\end{cases}
\]

\end_inset

Then 
\begin_inset Formula 
\begin{align*}
E_{\theta}T\left(X\right) & =T\left(1\right)\left(\frac{\theta}{2}\right)^{\left|1\right|}\left(1-\theta\right)^{1-\left|1\right|}+T\left(0\right)\left(\frac{\theta}{2}\right)^{\left|0\right|}\left(1-\theta\right)^{1-\left|0\right|}+T\left(-1\right)\left(\frac{\theta}{2}\right)^{\left|-1\right|}\left(1-\theta\right)^{1-\left|-1\right|}\\
 & =2\left(\frac{\theta}{2}\right)+0+0=\theta
\end{align*}

\end_inset

The second moment
\begin_inset Formula 
\begin{align*}
E_{\theta}\left(T\left(X\right)\right)^{2} & =T\left(1\right)^{2}\left(\frac{\theta}{2}\right)^{\left|1\right|}\left(1-\theta\right)^{1-\left|1\right|}+T\left(0\right)^{2}\left(\frac{\theta}{2}\right)^{\left|0\right|}\left(1-\theta\right)^{1-\left|0\right|}+T\left(-1\right)^{2}\left(\frac{\theta}{2}\right)^{\left|-1\right|}\left(1-\theta\right)^{1-\left|-1\right|}\\
 & =4\left(\frac{\theta}{2}\right)+0+0=2\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
It's clear from the computations of the second moments that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a smaller variance estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[7.40]
\end_layout

\end_inset

Let iid 
\begin_inset Formula $X_{i}\sim\text{Bernoulli}\left(p\right)$
\end_inset

.
 The Cramer-Rao lower bound is 
\begin_inset Formula 
\[
\text{Var}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left(\partial_{p}E_{p}W\left(\mathbf{X}\right)\right)^{2}}{E_{p}\left[\left(\partial_{p}\log L\left(\mathbf{X}|\theta\right)\right)^{2}\right]}
\]

\end_inset

Since iid
\begin_inset Formula 
\[
\text{Var}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left(\partial_{p}E_{p}W\left(\mathbf{X}\right)\right)^{2}}{nE_{p}\left[\left(\partial_{p}\log L\left(X|\theta\right)\right)^{2}\right]}
\]

\end_inset

and since exponential family
\begin_inset Formula 
\begin{align*}
\text{Var}\left(W\left(\mathbf{X}\right)\right) & \geq\frac{\left(\partial_{p}E_{p}W\left(\mathbf{X}\right)\right)^{2}}{-nE_{p}\left[\partial_{pp}\log L\left(X|\theta\right)\right]}
\end{align*}

\end_inset

The estimator is 
\begin_inset Formula $\bar{X}$
\end_inset

 so 
\begin_inset Formula 
\begin{align*}
\text{Var}\left(\bar{X}\right) & \geq\frac{\left(\partial_{p}E_{p}\bar{X}\right)^{2}}{-nE_{p}\left[\partial_{pp}\log L\left(X|\theta\right)\right]}
\end{align*}

\end_inset


\begin_inset Formula $\bar{X}$
\end_inset

 is certainly unbiased for 
\begin_inset Formula $p$
\end_inset

 so 
\begin_inset Formula 
\begin{align*}
\text{Var}\left(\bar{X}\right) & \geq\frac{\left(\partial_{p}p\right)^{2}}{-nE_{p}\left[\partial_{pp}\log\left(p^{X}\left(1-p\right)^{1-X}\right)\right]}\\
 & =\frac{1}{-nE_{p}\left[\partial_{pp}\left(X\log p+\left(1-X\right)\log\left(1-p\right)\right)\right]}\\
 & =\frac{1}{-nE_{p}\left[\partial_{p}\left(\frac{X}{p}+\frac{\left(1-X\right)}{1-p}\right)\right]}\\
 & =\frac{1}{-nE_{p}\left[-\frac{X}{p^{2}}+\frac{\left(1-X\right)}{\left(1-p\right)^{2}}\right]}\\
 & =\frac{1}{-n\left(-\frac{p}{p^{2}}+\frac{\left(1-p\right)}{\left(1-p\right)^{2}}\right)}\\
 & =\frac{p\left(p-1\right)}{n\left(2p-1\right)}
\end{align*}

\end_inset

But according to the attainment theorem 
\begin_inset Formula 
\begin{align*}
a\left(\theta\right)\left(W\left(\mathbf{X}\right)-\tau\left(\theta\right)\right) & =\partial_{\theta}\log L\left(\theta|\mathbf{X}\right)\\
 & =\partial_{p}\log\left(\prod_{i=1}^{n}p^{X_{i}}\left(1-p\right)^{1-X_{i}}\right)\\
 & =\partial_{p}\log\left(p^{\sum_{i}X_{i}}\left(1-p\right)^{n-\sum_{i}X_{i}}\right)\\
 & =\partial_{p}\log\left(e^{\left(\sum_{i}X_{i}\right)\log p+\left(n-\left(\sum_{i}X_{i}\right)\right)\log\left(1-p\right)}\right)\\
 & =\frac{\left(\sum_{i}X_{i}\right)}{p}-\frac{n-\left(\sum_{i}X_{i}\right)}{1-p}\\
 & =\frac{\left(\sum_{i}X_{i}\right)\left(1-p\right)-p\left(n-\left(\sum_{i}X_{i}\right)\right)}{p\left(1-p\right)}\\
 & =\frac{\left(\sum_{i}X_{i}\right)-np}{p\left(1-p\right)}\\
 & =\frac{n}{p\left(1-p\right)}\left(\bar{X}-p\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[Ex 8.2.3]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{i}$
\end_inset

 be such that 
\begin_inset Formula 
\[
f_{X}\left(x|\theta\right)=e^{-\left(x-\theta\right)}I\left(x\right)_{\left[\theta,\infty\right)}
\]

\end_inset

and 
\begin_inset Formula $\Theta_{0}=\theta\le\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\Theta_{0}^{c}=\theta>\theta_{0}$
\end_inset

.
 So the hypotheses are 
\begin_inset Formula 
\[
H_{0}:\theta\leq\theta_{0}\quad H_{1}:\theta>\theta_{0}
\]

\end_inset

The likelihood function is 
\begin_inset Formula 
\begin{align*}
L\left(\theta|\mathbf{x}\right) & =e^{-\sum_{i=1}^{n}\left(x_{i}-\theta\right)}I\left(x_{\left(1\right)}\right)_{\left[\theta,\infty\right)}\\
 & =e^{n\theta-\sum_{i=1}^{n}x_{i}}I\left(x_{\left(1\right)}\right)_{\left[\theta,\infty\right)}
\end{align*}

\end_inset

and so the LRT
\begin_inset Formula 
\[
\lambda\left(\mathbf{x}\right)=\frac{\sup_{\theta\in\Theta_{0}}L\left(\theta|\mathbf{x}\right)}{\sup_{\theta\in\Theta}L\left(\theta|\mathbf{x}\right)}
\]

\end_inset

The likelihood is a monotonically increasing function of 
\begin_inset Formula $\theta$
\end_inset

 on 
\begin_inset Formula $-\infty<\theta\leq x_{\left(1\right)}$
\end_inset

 so the unrestricted MLE is 
\begin_inset Formula $x_{\left(1\right)}$
\end_inset

.
 Similarly for the numerator if 
\begin_inset Formula $\theta_{0}<x_{\left(1\right)}$
\end_inset

 then the restricted MLE is 
\begin_inset Formula $x_{\left(1\right)}$
\end_inset

 and if 
\begin_inset Formula $x_{\left(1\right)}<\theta_{0}$
\end_inset

 then the restricted MLE is 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 So 
\begin_inset Formula 
\begin{align*}
\lambda\left(\mathbf{x}\right) & =\begin{cases}
\frac{L\left(x_{\left(1\right)}|\mathbf{x}\right)}{L\left(x_{\left(1\right)}|\mathbf{x}\right)} & \text{if }\theta_{0}<x_{\left(1\right)}\\
\frac{L\left(\theta_{0}|\mathbf{x}\right)}{L\left(x_{\left(1\right)}|\mathbf{x}\right)} & \text{if }\theta_{0}>x_{\left(1\right)}
\end{cases}\\
 & =\begin{cases}
1 & \text{if }\theta_{0}<x_{\left(1\right)}\\
\frac{e^{n\theta_{0}-\sum_{i=1}^{n}x_{i}}}{e^{nx_{\left(1\right)}-\sum_{i=1}^{n}x_{i}}} & \text{if }\theta_{0}>x_{\left(1\right)}
\end{cases}\\
 & =\begin{cases}
1 & \text{if }\theta_{0}<x_{\left(1\right)}\\
e^{n\left(\theta_{0}-x_{\left(1\right)}\right)} & \text{if }\theta_{0}>x_{\left(1\right)}
\end{cases}
\end{align*}

\end_inset

The power function of a test is 
\begin_inset Formula 
\begin{align*}
\beta\left(\theta\right)=P\left(\text{reject }H_{0}\text{ when }\theta\in\Theta_{0}\right)=P\left(\mathbf{X}\in R\right) & =\begin{cases}
P\left(\text{\text{committing a type I error}}\right) & \text{if }\theta\in\Theta_{0}\\
1-P\left(\mathbf{X}\in R^{c}\right) & \text{if }\theta\in\Theta_{0}^{c}
\end{cases}\\
 & =\begin{cases}
P\left(\text{\text{committing a type I error}}\right) & \text{if }\theta\in\Theta_{0}\\
1-P\left(\text{\text{committing a type II error}}\right) & \text{if }\theta\in\Theta_{0}^{c}
\end{cases}
\end{align*}

\end_inset

The size of a test is 
\begin_inset Formula $\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)$
\end_inset

.
 Reject 
\begin_inset Formula $H_{0}$
\end_inset

 is equivalent to 
\begin_inset Formula $\lambda\left(\mathbf{X}\right)<c$
\end_inset

.
 This is equivalent to
\begin_inset Formula 
\begin{align*}
e^{n\left(\theta_{0}-X_{\left(1\right)}\right)} & <c\\
\left(\theta_{0}-X_{\left(1\right)}\right) & <\frac{e^{c}}{n}\\
X_{\left(1\right)} & >\frac{e^{c}}{n}-\theta_{0}
\end{align*}

\end_inset

but 
\begin_inset Formula $X_{1}$
\end_inset

 is distributed something something something.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[Ex 8.3.2]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Binomial}\left(5,\theta\right)$
\end_inset

.
 The null hypothesis space is 
\begin_inset Formula $\Theta_{0}=\theta\leq1/2$
\end_inset

 and 
\begin_inset Formula $\Theta_{0}^{c}=\theta>1/2.$
\end_inset

 The hypotheses are 
\begin_inset Formula 
\[
H_{0}:\theta\leq1/2\quad H_{1}:\theta>1/2
\]

\end_inset

The LRT is 
\begin_inset Formula 
\begin{align*}
\lambda\left(\mathbf{x}\right) & =\frac{\sup_{\theta\in\Theta_{0}}L\left(\theta|x\right)}{\sup_{\theta\in\Theta}L\left(\theta|x\right)}\\
 & =\frac{\sup_{\theta\in\Theta_{0}}\left[\binom{5}{x}\theta^{x}\left(1-\theta\right)^{5-x}\right]}{\sup_{\theta\in\Theta}\left[\binom{5}{x}\theta^{x}\left(1-\theta\right)^{5-x}\right]}
\end{align*}

\end_inset

The unrestricted MLE is 
\begin_inset Formula $x/5$
\end_inset

.
 
\begin_inset Formula $L$
\end_inset

 has a global maximum as a functon of 
\begin_inset Formula $\theta$
\end_inset

 so if 
\begin_inset Formula $\theta_{0}<x/5$
\end_inset

 then the restricted MLE is 
\begin_inset Formula $\theta_{0}$
\end_inset

 and otherwise it's the unrestricted MLE.
 Hence
\begin_inset Formula 
\begin{align*}
\lambda\left(\mathbf{x}\right) & =\begin{cases}
1 & \text{if }\theta_{0}>x/5\\
\frac{\binom{5}{x}\left(\theta_{0}\right)^{x}\left(1-\theta_{0}\right)^{5-x}}{\binom{5}{x}\left(\frac{x}{5}\right)^{x}\left(1-\frac{x}{5}\right)^{5-x}} & \text{otherwise}
\end{cases}\\
 & =\begin{cases}
1 & \text{if }\theta_{0}>x/5\\
\left(\frac{5\theta_{0}}{x}\right)^{x}\left(\frac{1-\theta_{0}}{1-\frac{x}{5}}\right)^{5-x} & \text{otherwise}
\end{cases}
\end{align*}

\end_inset

I can't calculate the power for any of these functions tests! More testing
 and randomized tests and uncorralted estimators of 0.
\end_layout

\end_body
\end_document
