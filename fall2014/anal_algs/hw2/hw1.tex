%% LyX 2.0.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[latin9]{inputenc}
\usepackage{listings}
\lstset{frame=single}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

%
\usepackage{mathdots}
\usepackage{listings}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{tikz-qtree}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{amsfonts}\usepackage{nopageno}%%%  The following few lines affect the margin sizes. 
\addtolength{\topmargin}{-.5in}
\setlength{\textwidth}{6in}       
\setlength{\oddsidemargin}{.25in}              
\setlength{\evensidemargin}{.25in}         
  
\setlength{\textheight}{9in}
\renewcommand{\baselinestretch}{1}
\reversemarginpar   

\lstset{
numbers=left
}


%
%

\makeatother

\begin{document}

\title{COT5405 Homework 2 Solutions}


\author{Maksim Levental}


\date{\today}
\maketitle
\begin{enumerate}
\item [4.2-3]Pad with zeroes until you get matrices of size which is a
power of 2 then truncate all non-zero entries in the resultant matrix.
Let $k=2^{\lceil\lg n\rceil}$ and pad the $n\times n$ matrices out
to $k\times k$. Complexity is $O(k^{\lg7})=O\Bigg(\bigg(2^{\lceil\lg n\rceil}\bigg)^{\lg7}\Bigg)=O\Bigg(\bigg(2^{\lg n}\bigg)^{\lg7}\Bigg)=O\Bigg(\bigg(n^{\lg2}\bigg)^{\lg7}\Bigg)=O\bigg(n{}^{\lg7}\bigg)$. 
\item [4.2-4]If the reduction is from one multiplication to $k$ multiplications
of matrices size $n/3$ then the recurrence relation is $T(n)=kT(n/3)+\Theta(n^{2})$
because each smaller multiplcation, presumably, still takes $\Theta(n^{2})$
time. Where $\log_{3}k$ falls in relation to $2$ determines whether
$T(n)=O(n^{\lg7}).$ If $\log_{3}k>2$ then case 1 of the master theorem
applies and $T(n)=\Theta(n^{\log_{3}k})$ and $\log_{3}k<\lg7$ when
$k<3^{\lg7}\approx21.8$. If $\log_{3}k=2$ and $k=9$ then case 2
of the master theorem applies and $T(n)=\Theta(n^{2}\lg n)=O(n^{\lg7})$.
If $\log_{3}k<2$ and $k<9$ then case 3 of the master theorem applies
and $T(n)=\Theta(n^{2})=O(n^{\lg7})$. So the maximum number is 21
by way of case 1.
\item [4.2-6]Let $A$ be the $kn\times n$ matrix and $B$ be the $n\times kn$
matrix. We can look at $A$ as a $k$-row column vector of $n\times n$
matrices and $B$ as a $k$-column row vector of $n\times n$ matrices.
I.e 
\begin{align*}
A=\left(\begin{array}{c}
A_{1}\\
A_{2}\\
\vdots\\
A_{k}
\end{array}\right) & \text{ }B=\left(B_{1},B_{2},\dots,B_{k}\right)
\end{align*}
Then $A\cdot B$ is the outerproduct of the column and row vector
and therefore
\[
A\cdot B=\left(\begin{array}{cccc}
A_{1}\cdot B_{1} & A_{1}\cdot B_{2} & \cdots & A_{1}\cdot B_{k}\\
A_{2}\cdot B_{1} & A_{2}\cdot B_{2} & \cdots & A_{2}B_{k}\\
\vdots & \vdots & \ddots & \vdots\\
A_{k}\cdot B_{1} & A_{k}\cdot B_{2} & \cdots & A_{k}\cdot B_{k}
\end{array}\right)
\]
where every $A_{i}\cdot B_{j}$ is an $n\times n$ matrix product
that can be computed using Strassen's algorithm. Therefore the total
running time is $k^{2}O(n^{\lg7})=O(k^{2}n^{\lg7}).$ If the input
matrices are reversed then $B\cdot A$ is the ``dot product'' and
therefore 
\[
A\cdot B=A_{1}\cdot B_{1}+A_{2}\cdot B_{2}+\cdots+A_{k}\cdot B_{k}
\]
where every $A_{i}\cdot B_{j}$ is an $n\times n$ matrix product
that can be computed using Strassen's algorithm. Therefore the total
running time is $kO(n^{\lg7})=O(kn^{\lg7}).$
\item [6-3] ~\\


\begin{enumerate}
\item %
\begin{tabular}{|c|c|c|c|}
\hline 
2 & 8 & 12 & 16\tabularnewline
\hline 
\hline 
3 & 9 & 14 & $\infty$\tabularnewline
\hline 
4 & $\infty$ & $\infty$ & $\infty$\tabularnewline
\hline 
5 & $\infty$ & $\infty$ & $\infty$\tabularnewline
\hline 
\end{tabular}\\

\item If $Y[1,1]=\infty$ then the only entries that can appear in $Y[1,j]$
for $j>1$ are $\infty$ by the requirement that $Y[1,j]\leq Y[1,j+1]$
and similarly the only entries that can appear in $Y[i,1]$ for $i>1$
are $\infty$. Hence by induction/recursion all of the entries in
the tableau are $\infty$. If $Y[m,n]<\infty$ then no entries equal
to $\infty$ can appear in $Y[m,j]$ for $j<n$ by the requirement
that $Y[1,j]\leq Y[1,j+1]$ and similarly no entries equal to $\infty$
can appear in $Y[i,n]$ for $i<n$. Hence by induction/recursion $Y$
is full.
\item The minimum element is at $Y[1,1]$. Remove/extract it and replace
it with $\infty$. Consider the sub-tableaux $Y_{ij}$ of $Y$ with
top-left corner at row $i$ and column $j$. After extracting the
minimum $Y[1,1]$ and replacing it with $\infty$ compute the minima
of $Y_{i+1,j}$ and $Y_{i,j+1}$ for $i=j=1$ and swap the $\infty$
in $Y[1,1]$ with the $\min\{Y_{1+1,1},Y_{1,1+1}\}$. This is correct
because it does not violate the Young-tableau property $Y$ since
you're picking the minimum of $\{Y_{1+1,1},Y_{1,1+1}\}$. The algorithm
terminates when 
\[
\infty=\min\{Y_{i+1,j},Y_{i,j+1}\}
\]
 or $i>m$ and $j>n$. The recurrence relation is 
\[
T(m+n)\leq\max\{T(m-1+n),T(m+n-1)\}+\Theta(1)
\]
Let $p=m+n$ then 
\begin{align*}
T(p) & \leq\max\{T(p-1),T(p-1)\}+\Theta(1)\\
 & =T(p-1)+\Theta(1)
\end{align*}
Whose solution is clearly $O(p)=O(m+n)$ since the sub-problem is
1 smaller.
\item By symmetry the maximum element in a Young-tableau is in the bottom-right
entry. To insert replace the maximum element of $Y$ (by part (b)
this entry is $\infty$ if $Y$ isn't full). Then compute the maximum
of $Y[i-1,j]$ and $Y[i,j-1]$ for $i=m$ and $j=n$ and swap the
entry in $Y[m,n]$ with the $\text{max}\{Y[m-1,n],Y[m,n-1]\}$. If
$Y[m-1,n]=Y[m,n-1]$ swap $Y[m-1,n]$ just to be definite. Then repeat.
This is correct because you're picking the maximum of $\{Y[m-1,n],Y[m,n-1]\}$
and so at the end having $Y[i-1,j]\text{ and }Y[i,j-1]$ in the same
column or row does not violate the Young-Tableaux property since $\text{max}\{Y[i-1,j],Y[i,j-1]\}\geq Y[i-1,j]$
and $\text{max}\{Y[i-1,j],Y[i,j-1]\}\geq Y[i,j-1]$. The algorithm
terminates when the entry to be inserted is $\geq\text{max}\{Y[m-1,n],Y[m,n-1]\}$
or $j<1$ and $j<1$. Since at most the algorithm has to make comparisons
with every entry in the last column and every entry in the first row
minus 1 the running time is $O(m+n).$
\item Run Extract-Min for every one of the $n^{2}$ elements. The running
time is $n^{2}O(n+n)=n^{2}O(2n)=O(n^{3})$.
\item Start at the top right corner and inspect $Y[1,1]$ and $Y[m,n]$.
These are the minimum and the maximum entries in the tableau $Y.$
If the entry you're searching for is outside of that range return
false. Otherwise Consider the 2 sub-tableax with with corners (counterclockwise
starting from upper right) $Y_{1}=\{Y[2,n],Y[m,n],Y[m,1],Y[2,1]\}$
and $Y_{2}=\{Y[1,n-1],Y[m,n-1],Y[m,1],Y[1,1]\}$. $Y_{1}$ has range
(maximum and minimum) $ $$Y[m,n],Y[2,1]$ and $Y_{2}$ has range
$ $$Y[m,n-1],Y[1,1]$. If the neither ranges contain the element
you're searching for return false. Else recurse to the sub-tableau
with the smallest range. Per recursion you perform $O(1)$ operations
and the most work would be done were you to ``walk'' along the edges
of $Y$. Hence the running time is $(m+n)O(1)=O(m+n)$.
\end{enumerate}
\item [9.3-5]Just implement the worst case linear time algorithm Select
but replace steps 1-3 with your linear time median algorithm. Another
way to look at it is to implement Randomized-Select but forget the
randomization and change partition to accept a pivot. Then partition
on whatever value Linear-Med returns
\begin{lstlisting}
Pivot-Partition(A,p,r,i)
	//A[i] is the entry you want to partition around
	swap(A[i],A[r])
	return Partition(A,p,r)

Linear-Select(A,p,r,i)
	if p == r
		return A[p]
	m = Linear-Med(A,p,r)
	q = Pivot-Partition(A,p,r,m)
	k = q-p+1
	if i == k
		return A[q]
	else if i < k
		return Linear-Select(A,p,q-1,i)
	else
		return Linear-Select(A,q+1,r,i-k)
\end{lstlisting}
 
\item [9.3-7]Modify Select to Select-Index, which return the index of the
$m$th statistic instead of the value. Duplicate the array in linear
time. Call the first array $A$ and the duplicated array $B$. Use
Select to find the median $m$ of $A$ in $O(n)$ time then in $O(n)$
time subtract $m$ from all elements in $A$ and compute their absolute
value and store them in $A$. The $k$ smallest elements in this adjusted
array then correspond to the closest to the median from the original
array ($B$). Compute the index of the $k$th order statistic of this
new array. Then use a modified version of Partition, Double-Pivot-Partition,
that partitions $A$ but also performs the same swaps in $B$, i.e.
partitions $B$ the same way as $A$. Then the first $k$ elements
in $B$ are elements in the original array that were the $k$ closest
to the median.\\
 
\begin{lstlisting}
Double-Partition(A,B,p,r)
	x = A[r]
	i = p-1
	for j = p to r-1 
		if A[j] <= x
			i = i+1
			swap(A[i],A[j])
			swap(B[i],B[j])
	swap(A[i+1],A[r])
	swap(B[i+1],B[r])
	return i+1

Double-Pivot-Partition(A,B,p,r,i)
	//A[i] is the entry you want to partition around
	swap(A[i],A[r])
	swap(B[i],B[r])
	return Double-Partition(A,B,p,r)

Median-Closest(A,k)
	//the actual median, not just the index
	m = Select-Index(A,0,A.size-1,floor((A.size-1)/2))
	B = A
	for j=1 to A.size-1
		A[j] = A[j]-A[m]
	//
	l = Select-Index(A,0,A.size-1,k)
	//return will be k because l is index of kth OS
	//and so it will be placed in the kth position.
	Double-Pivot-Partition(A,B,0,l)
	return B[0..k]
\end{lstlisting}

\item [9.3-8]Let $a_{m}=A[\lceil A.\text{size}/2\rceil]$ and $b_{m}=B[\lceil B.\text{size}/2\rceil]$
the medians of each array. Let $c_{i}<c_{i+1}$ be the sorted elements
in $A\cup B$ and $m$ be the median of the $\{c_{i}\}$, i.e. $A\cup B$.
\\
\\
\textbf{Lemma:} In $A\cup B$ it's the case that $a_{b}\leq m\leq b_{m}$,
that is in the final arrangement of elements the median of both arrays
is between (or equal to one or the other other both of) the medians
of $A$ and $B$. \\
\\
\textbf{Proof: }If $a_{m}=b_{m}$ then clearly the median of $A\cup B=a_{m}=b_{m}$.
Hence without loss of generality assume $a_{m}\neq b_{m}$ and $a_{m}<b_{m}$.
The quantity of $c_{i}$ such that $c_{i}\leq a_{m}$ is at most $\lceil A.\text{size}/2\rceil-1+\lceil B.\text{size}/2\rceil-1$,
(saturating if $a_{m}=b_{m}$). These are all of the elements in $A\le a_{m}$
unioned with all of the elements in $B\leq a_{m}.$ Similarly there
are at most $\lceil A.\text{size}/2\rceil-1+\lceil B.\text{size}/2\rceil-1$
such that $c_{j}\geq b_{m}$. Furthermore since $a_{m}<b_{m}$ there
are at most $\lceil A.\text{size}/2\rceil-1+\lceil B.\text{size}/2\rceil$
elements in $A\cup B$ such that $a_{m}\leq c_{i}$ ($a_{m}=c_{i}$
and $b_{m}=c_{i+1})$ and similarly at most $\lceil A.\text{size}/2\rceil-1+\lceil B.\text{size}/2\rceil$
elements such that $c_{j}\leq b_{m}$. But the median $m$ is the
element such that there are exactly $\lceil A\cup B.\text{size}/2\rceil-1=\lceil A.\text{size}/2\rceil+\lceil B.\text{size}/2\rceil+1$
elements less than it and $\lceil A\cup B.\text{size}/2\rceil-1=\lceil A.\text{size}/2\rceil+\lceil B.\text{size}/2\rceil+1$
greater than it, which is a contradiction. Therefore $a_{m}\leq m\leq b_{m}$.
$\blacksmiley$\\
\\
To compute the median of $A\cup B$ compute $a_{m}=A[\lceil A.\text{size}/2\rceil]$
and $b_{m}=B[\lceil B.\text{size}/2\rceil]$, the medians of each
array, in constant time. If $a_{m}=b_{m}$ return $a_{m}$. Otherwise
without loss of generality we assume $a_{m}<b_{m}$. Then let $S_{a}$
be all of the elements between $a_{m}$ and $A[n]$ in $A$ and likewise
for $S_{b}$ all of the elements between $b_{m}$ and $B[n]$ in $B$.
Finally by the Lemma above we know that the median $m\in S_{a}\cup S_{b}$
and since each of $S_{a}$and $S_{b}$ are already sorted we can recurse
into them.\\
\\
At each recursion medians of each sub-array are computed in constant
time, and since we're truncating around the medians of the subarray
half of the entries are truncated and so we have the recursion relation
$T(n)\approx T(n/2)+O(1)$ whose solution is $O(\lg n)$.
\item [10-1]The complexities:\\
\\
\begin{tabular}{p{3cm}p{1.25cm}p{1.25cm}p{1.25cm}p{1.25cm}}
\hline 
Operation & unsorted, singly linked & sorted, singly linked & unsorted, doubly linked & sorted, doubly linked \\ 
\hline  
Search$(L,k)$  & $O(n)$ & $O(n)$ & $O(n)$ & $O(n)$\\
Insert$(L,x)$ & $O(1)$ & $O(n)$ & $O(1)$ & $O(n)$\\
Delete$(L,x)$ & $O(1)$ & $O(1)$ & $O(1)$ & $O(1)$\\
Successor$(L,x)$ & $O(n)$ & $O(1)$ & $O(n)$ & $O(1)$\\
Predecessor$(L,x)$ & $O(n)$ & $O(n)$ & $O(n)$ & $O(1)$\\
Minimum$(L)$ & $O(n)$ & $O(1)$ & $O(n)$ & $O(1)$\\
Maximum$(L)$ & $O(n)$ & $O(n)$ & $O(n)$ & $O(1)$\\
\hline
\end{tabular}
\item [12.2-5]Proof by contradiction. Let $X$ be the node in question
and $S$ be the successor of node $X$. Assume $S$ has a left child
$S.left$. But the successor of $X$ must be in the right subtree
of $X$ and hence the $S.left$ is simultaneously less than $S$,
by the binary search tree property (because it's the left child of
$S$), and greater than $X$ (because it's in the right subtree of
$X$). But then $S$ is not the successor. For predecessor let $P$
be the predecessor of $X.$ Assume $P$ has a right child $P.right$.
But the predecessor of $X$ must be in the left subtree of $X$ and
hence the $P.right$ is simultaneously greater than $P$, by the binary
search tree property (because it's the right child of $P$), and less
than $X$ (because it's in the left subtree of $X$). But then $P$
is not the predecessor.
\item [12.3-4] Deletion is not commutative. Counterexample:\\
\\
 \tikzset{every tree node/.style={minimum width=2em,draw,circle},
         blank/.style={draw=none},
         edge from parent/.style=
         {draw, edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}},
         level distance=1.5cm}
\begin{tikzpicture}

\Tree [.a
         [.b ]
	     [.c
		\edge[]; {d}
         \edge[blank]; \node[blank]{};		 
    ]
] 
\end{tikzpicture}~~$\underrightarrow{\text{delete b}}$~~
\tikzset{every tree node/.style={minimum width=2em,draw,circle},
         blank/.style={draw=none},
         edge from parent/.style=
         {draw, edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}},
         level distance=1.5cm}
\begin{tikzpicture}

\Tree [.a
         \edge[blank]; \node[blank]{};
	     [.c
		 \edge[]; {d}
         \edge[blank]; \node[blank]{};		 
    ]
] 
\end{tikzpicture}~~$\underrightarrow{\text{delete a}}$~~
\tikzset{every tree node/.style={minimum width=2em,draw,circle},
         blank/.style={draw=none},
         edge from parent/.style=
         {draw, edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}},
         level distance=1.5cm}
\begin{tikzpicture}

\Tree [.c
		 \edge[]; {d}
         \edge[blank]; \node[blank]{};		 
    ]
] 
\end{tikzpicture}\\
\tikzset{every tree node/.style={minimum width=2em,draw,circle},
         blank/.style={draw=none},
         edge from parent/.style=
         {draw, edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}},
         level distance=1.5cm}
\begin{tikzpicture}

\Tree [.a
         [.b ]
	     [.c
		\edge[]; {d}
         \edge[blank]; \node[blank]{};		 
    ]
] 
\end{tikzpicture}~~$\underrightarrow{\text{delete a}}$~~
\tikzset{every tree node/.style={minimum width=2em,draw,circle},
         blank/.style={draw=none},
         edge from parent/.style=
         {draw, edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}},
         level distance=1.5cm}
\begin{tikzpicture}

\Tree [.d
         [.b ]
	     [.c ]
] 
\end{tikzpicture}~~$\underrightarrow{\text{delete b}}$~~
\tikzset{every tree node/.style={minimum width=2em,draw,circle},
         blank/.style={draw=none},
         edge from parent/.style=
         {draw, edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}},
         level distance=1.5cm}
\begin{tikzpicture}
\Tree [.d
		 \edge[blank]; \node[blank]{};
         [.c ]	     	
] 
\end{tikzpicture}\\

\item [12.3-5]Search is not affected because it doesn't use the parent
pointer in any of the nodes. \\
\\
Insertion isn't affected either except that now instead $z.p=y$ we
need to set $z.succ$ to something appropriate. If $z$ is inserted
as a left child then its successor is its parent $p$, since $p$
is certainly greater and if there were a node $p'<p$ and $z<p'$
then $z$ would have been inserted as the left child of $p'$. If
$z$ is inserted as a right child then its successor is the successor
of its parent $p$ and $p$'s successor is now $z$, since being inserted
as a right child implies $p<z$ therefore $z$ is ``closer'' to
$p$'s successor than $p$ and there are no other elements between
$p$ and $z$. \\
\\
Hence 
\begin{lstlisting}
Tree-Insert(T,z)
	y = NIL
	x = T.root
	while x != NIL
		y = x
		if z.key < x.key
			x = x.left
		else
			x = x.right
	if y == NIL
		T.root = z
		z.succ = NIL
	else if z.key < y.key
		y.left = z
		z.succ = y
	else
		y.right = z
		z.succ = y.succ
		y.succ = z
\end{lstlisting}
Running time of Tree-Insert is still $O(\lg n)$ since we've only
added constant time operations to it. \\
\\
In order to implement Delete, which calls Transpant (which makes heavy
use of parent pointers), we implement another auxilliary routine Parent(T,u).
The easiest way to find $u'$s parent is to search for $u$ while
keeping a trailing pointer 
\begin{lstlisting}
Parent(T,u)
	y = NIL
	x = T.root
	while x != u
		y = x
		if u.key < x.key
			x = x.left
		else 
			x = x.right
	return y
\end{lstlisting}
Parent has the same structure as Iterative-Tree-Search except that
it matches on node rather than key (in case there are several nodes
with the same value) and hence is correct and runs in $O(\lg n)$
time. Then we implment transplant with calls to Parent wherever there's
a parent pointer being referenced
\begin{lstlisting}
Transplant(T,u,v)
	uParent = Parent(T,u)
	if uParent == NIL
		T.root = V
	else if u == uParent.left
		uParent.left = v
	else
		uParent.right = v
	if v != NIL
		vParent = Parent(T,v)
		vParent = uParent
\end{lstlisting}
Because the only thing changed is how parent pointers are found Transplant
is correct for the same reason Transplant from CLRS is correct. Furthermore
Transplant(T,u,v) runs in at most $O(\lg n)$ time since it makes
at most 2 calls to Parent. And similarly for Tree-Delete
\begin{lstlisting}
Tree-Delete(T,z)
	if z.left == NIL
		Transplant(T,z,z.right)
	else if z.right == NIL
		Transplant(T,z,z.left)
	else 
		y = z.succ
		yParent = Parent(T,y)
		if yParent != z
			Transplant(T,y,y.right)
			y.right = z.right
			yRightParent = Parent(T,y.right)
			yRightParent = y
		Transplant(T,z,y)
		y.left = z.left
		yLeftParent = Parent(T,y.left)
		yLeftParent = y
\end{lstlisting}
Tree-Delete has the same structure as Tree-Delete from CLRS and hence
is correct and runs in at most $O(\lg n)$ time since it makes at
most 3 calls to Transplant and 3 calls to Parent, each of which are
run in at most $O(\lg n)$ time themselves.\end{enumerate}

\end{document}
